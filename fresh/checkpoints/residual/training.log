Training started at: 2025-05-25 01:12:14
Device: cuda, Num Epochs: 200, Model: UNet
Batch Size: 8, Learning Rate: 0.001
Loss Function: TverskyLoss, Optimizer: Adam
Weight Decay: None
Dropout: 0.05
Patch Size: 128
Epoch, AvgTrainLoss, 
1, 0.492627, 
2, 0.411692, 
3, 0.404561, 
4, 0.398054, 
5, 0.381878, 
6, 0.373897, 
7, 0.370284, 
8, 0.374883, 
9, 0.365428, 
10, 0.361416, 
11, 0.352933, 
12, 0.350289, 
13, 0.352779, 
14, 0.345500, 
15, 0.338262, 
16, 0.336006, 
17, 0.335350, 
18, 0.330327, 
19, 0.330699, 
20, 0.338166, 
21, 0.328648, 
22, 0.326299, 
23, 0.323741, 
24, 0.321244, 
25, 0.319691, 
26, 0.315461, 
27, 0.313775, 
28, 0.313661, 
29, 0.309858, 
30, 0.311525, 
31, 0.304905, 
32, 0.305023, 
33, 0.297354, 
34, 0.300942, 
35, 0.302322, 
36, 0.301021, 
37, 0.295031, 
38, 0.301524, 
39, 0.297630, 
40, 0.299785, 
41, 0.293096, 
42, 0.298982, 
43, 0.295691, 
44, 0.291153, 
45, 0.286323, 
46, 0.287329, 
47, 0.286123, 
48, 0.285620, 
49, 0.282382, 
50, 0.291823, 
51, 0.282920, 
52, 0.275560, 
53, 0.281101, 
54, 0.280811, 
55, 0.284302, 
56, 0.280544, 
57, 0.278913, 
58, 0.276323, 
59, 0.273793, 
60, 0.277087, 
61, 0.275329, 
62, 0.274930, 
63, 0.271975, 
64, 0.271218, 
65, 0.265240, 
66, 0.263412, 
67, 0.264997, 
68, 0.268448, 
69, 0.266386, 
70, 0.267481, 
71, 0.263022, 
72, 0.258893, 
73, 0.266793, 
74, 0.260235, 
75, 0.265686, 
76, 0.254905, 
77, 0.262203, 
78, 0.261121, 
79, 0.259174, 
80, 0.261663, 
81, 0.257521, 
82, 0.253350, 
83, 0.256342, 
84, 0.252472, 
85, 0.253987, 
86, 0.255160, 
87, 0.255760, 
88, 0.254361, 
89, 0.255649, 
90, 0.252576, 
91, 0.249027, 
92, 0.250910, 
93, 0.248582, 
94, 0.245548, 
95, 0.247379, 
96, 0.248965, 
97, 0.248459, 
98, 0.247334, 
99, 0.244208, 
100, 0.247143, 
101, 0.246873, 
102, 0.243888, 
103, 0.244506, 
104, 0.246121, 
105, 0.244224, 
106, 0.243050, 
107, 0.242321, 
108, 0.244693, 
109, 0.240721, 
110, 0.242997, 
111, 0.241530, 
112, 0.244151, 
113, 0.243678, 
114, 0.242360, 
115, 0.239995, 
116, 0.242376, 
117, 0.239250, 
118, 0.241422, 
119, 0.245813, 
120, 0.246271, 
121, 0.242608, 
122, 0.239583, 
123, 0.238159, 
124, 0.239478, 
125, 0.239432, 
126, 0.239837, 
127, 0.238113, 
128, 0.243593, 
129, 0.235995, 
130, 0.238493, 
131, 0.239377, 
132, 0.239802, 
133, 0.238048, 
134, 0.241627, 
135, 0.238902, 
136, 0.233954, 
137, 0.235127, 
138, 0.236093, 
139, 0.239665, 
140, 0.237092, 
141, 0.232566, 
142, 0.232758, 
143, 0.236465, 
144, 0.235137, 
145, 0.236205, 
146, 0.235339, 
147, 0.239416, 
148, 0.236075, 
149, 0.233448, 
150, 0.230584, 
151, 0.231492, 
152, 0.237970, 
153, 0.233328, 
154, 0.235167, 
155, 0.232591, 
156, 0.235573, 
157, 0.233564, 
158, 0.235399, 
159, 0.233029, 
160, 0.230397, 
161, 0.234503, 
162, 0.227199, 
163, 0.235976, 
164, 0.225787, 
165, 0.229292, 
166, 0.232332, 
167, 0.235430, 
168, 0.229181, 
169, 0.236974, 
170, 0.227285, 
171, 0.235883, 
172, 0.232800, 
173, 0.225951, 
174, 0.233524, 
175, 0.230217, 
176, 0.235718, 
