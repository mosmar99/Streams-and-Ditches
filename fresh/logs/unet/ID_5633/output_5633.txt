Beginning to load data...
 -- Train dataset size: 4134 / 4593 ~ 0.9
 -- Test dataset size: 459 / 4593 ~ 0.1
 -- Created DataLoaders with batch size: 8

Starting UNet Training...
  Epoch [1/200], Batch [47/517], Loss: 0.7168
  Epoch [1/200], Batch [94/517], Loss: 0.6658
  Epoch [1/200], Batch [141/517], Loss: 0.5839
  Epoch [1/200], Batch [188/517], Loss: 0.5261
  Epoch [1/200], Batch [235/517], Loss: 0.5136
  Epoch [1/200], Batch [282/517], Loss: 0.4802
  Epoch [1/200], Batch [329/517], Loss: 0.4414
  Epoch [1/200], Batch [376/517], Loss: 0.4242
  Epoch [1/200], Batch [423/517], Loss: 0.4199
  Epoch [1/200], Batch [470/517], Loss: 0.4332
  Epoch [1/200], Batch [517/517], Loss: 0.5290
--- Epoch [1/200] complete. Average Training Loss: 0.5315 ---
--- Time taken for epoch: 314.29 seconds ---
  Epoch [2/200], Batch [47/517], Loss: 0.3981
  Epoch [2/200], Batch [94/517], Loss: 0.4333
  Epoch [2/200], Batch [141/517], Loss: 0.4525
  Epoch [2/200], Batch [188/517], Loss: 0.5020
  Epoch [2/200], Batch [235/517], Loss: 0.4035
  Epoch [2/200], Batch [282/517], Loss: 0.4610
  Epoch [2/200], Batch [329/517], Loss: 0.3643
  Epoch [2/200], Batch [376/517], Loss: 0.4238
  Epoch [2/200], Batch [423/517], Loss: 0.3865
  Epoch [2/200], Batch [470/517], Loss: 0.3646
  Epoch [2/200], Batch [517/517], Loss: 0.4369
--- Epoch [2/200] complete. Average Training Loss: 0.4126 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [3/200], Batch [47/517], Loss: 0.3011
  Epoch [3/200], Batch [94/517], Loss: 0.4318
  Epoch [3/200], Batch [141/517], Loss: 0.4494
  Epoch [3/200], Batch [188/517], Loss: 0.4208
  Epoch [3/200], Batch [235/517], Loss: 0.3953
  Epoch [3/200], Batch [282/517], Loss: 0.4150
  Epoch [3/200], Batch [329/517], Loss: 0.4105
  Epoch [3/200], Batch [376/517], Loss: 0.4439
  Epoch [3/200], Batch [423/517], Loss: 0.4025
  Epoch [3/200], Batch [470/517], Loss: 0.3904
  Epoch [3/200], Batch [517/517], Loss: 0.4689
--- Epoch [3/200] complete. Average Training Loss: 0.3888 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [4/200], Batch [47/517], Loss: 0.3831
  Epoch [4/200], Batch [94/517], Loss: 0.3437
  Epoch [4/200], Batch [141/517], Loss: 0.4430
  Epoch [4/200], Batch [188/517], Loss: 0.4087
  Epoch [4/200], Batch [235/517], Loss: 0.2950
  Epoch [4/200], Batch [282/517], Loss: 0.3057
  Epoch [4/200], Batch [329/517], Loss: 0.3810
  Epoch [4/200], Batch [376/517], Loss: 0.1764
  Epoch [4/200], Batch [423/517], Loss: 0.4022
  Epoch [4/200], Batch [470/517], Loss: 0.4085
  Epoch [4/200], Batch [517/517], Loss: 0.3475
--- Epoch [4/200] complete. Average Training Loss: 0.3800 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [5/200], Batch [47/517], Loss: 0.4296
  Epoch [5/200], Batch [94/517], Loss: 0.4469
  Epoch [5/200], Batch [141/517], Loss: 0.4097
  Epoch [5/200], Batch [188/517], Loss: 0.3226
  Epoch [5/200], Batch [235/517], Loss: 0.2591
  Epoch [5/200], Batch [282/517], Loss: 0.3716
  Epoch [5/200], Batch [329/517], Loss: 0.3867
  Epoch [5/200], Batch [376/517], Loss: 0.3550
  Epoch [5/200], Batch [423/517], Loss: 0.2856
  Epoch [5/200], Batch [470/517], Loss: 0.1360
  Epoch [5/200], Batch [517/517], Loss: 0.4084
--- Epoch [5/200] complete. Average Training Loss: 0.3657 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [6/200], Batch [47/517], Loss: 0.3741
  Epoch [6/200], Batch [94/517], Loss: 0.4222
  Epoch [6/200], Batch [141/517], Loss: 0.4195
  Epoch [6/200], Batch [188/517], Loss: 0.4145
  Epoch [6/200], Batch [235/517], Loss: 0.4142
  Epoch [6/200], Batch [282/517], Loss: 0.4555
  Epoch [6/200], Batch [329/517], Loss: 0.1805
  Epoch [6/200], Batch [376/517], Loss: 0.3321
  Epoch [6/200], Batch [423/517], Loss: 0.3090
  Epoch [6/200], Batch [470/517], Loss: 0.2804
  Epoch [6/200], Batch [517/517], Loss: 0.3035
--- Epoch [6/200] complete. Average Training Loss: 0.3604 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [7/200], Batch [47/517], Loss: 0.2504
  Epoch [7/200], Batch [94/517], Loss: 0.2860
  Epoch [7/200], Batch [141/517], Loss: 0.3941
  Epoch [7/200], Batch [188/517], Loss: 0.4329
  Epoch [7/200], Batch [235/517], Loss: 0.3245
  Epoch [7/200], Batch [282/517], Loss: 0.3158
  Epoch [7/200], Batch [329/517], Loss: 0.2750
  Epoch [7/200], Batch [376/517], Loss: 0.4241
  Epoch [7/200], Batch [423/517], Loss: 0.3108
  Epoch [7/200], Batch [470/517], Loss: 0.2948
  Epoch [7/200], Batch [517/517], Loss: 0.2903
--- Epoch [7/200] complete. Average Training Loss: 0.3590 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [8/200], Batch [47/517], Loss: 0.3980
  Epoch [8/200], Batch [94/517], Loss: 0.4392
  Epoch [8/200], Batch [141/517], Loss: 0.3620
  Epoch [8/200], Batch [188/517], Loss: 0.3360
  Epoch [8/200], Batch [235/517], Loss: 0.3602
  Epoch [8/200], Batch [282/517], Loss: 0.4334
  Epoch [8/200], Batch [329/517], Loss: 0.4509
  Epoch [8/200], Batch [376/517], Loss: 0.4389
  Epoch [8/200], Batch [423/517], Loss: 0.4230
  Epoch [8/200], Batch [470/517], Loss: 0.2619
  Epoch [8/200], Batch [517/517], Loss: 0.3986
--- Epoch [8/200] complete. Average Training Loss: 0.3480 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [9/200], Batch [47/517], Loss: 0.2347
  Epoch [9/200], Batch [94/517], Loss: 0.3766
  Epoch [9/200], Batch [141/517], Loss: 0.4321
  Epoch [9/200], Batch [188/517], Loss: 0.3626
  Epoch [9/200], Batch [235/517], Loss: 0.2156
  Epoch [9/200], Batch [282/517], Loss: 0.3362
  Epoch [9/200], Batch [329/517], Loss: 0.2216
  Epoch [9/200], Batch [376/517], Loss: 0.2975
  Epoch [9/200], Batch [423/517], Loss: 0.3709
  Epoch [9/200], Batch [470/517], Loss: 0.4143
  Epoch [9/200], Batch [517/517], Loss: 0.4337
--- Epoch [9/200] complete. Average Training Loss: 0.3443 ---
--- Time taken for epoch: 313.84 seconds ---
  Epoch [10/200], Batch [47/517], Loss: 0.3912
  Epoch [10/200], Batch [94/517], Loss: 0.2870
  Epoch [10/200], Batch [141/517], Loss: 0.2629
  Epoch [10/200], Batch [188/517], Loss: 0.3585
  Epoch [10/200], Batch [235/517], Loss: 0.2284
  Epoch [10/200], Batch [282/517], Loss: 0.2643
  Epoch [10/200], Batch [329/517], Loss: 0.3171
  Epoch [10/200], Batch [376/517], Loss: 0.4395
  Epoch [10/200], Batch [423/517], Loss: 0.3260
  Epoch [10/200], Batch [470/517], Loss: 0.3967
  Epoch [10/200], Batch [517/517], Loss: 0.2166
--- Epoch [10/200] complete. Average Training Loss: 0.3437 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [11/200], Batch [47/517], Loss: 0.1821
  Epoch [11/200], Batch [94/517], Loss: 0.3137
  Epoch [11/200], Batch [141/517], Loss: 0.3986
  Epoch [11/200], Batch [188/517], Loss: 0.1452
  Epoch [11/200], Batch [235/517], Loss: 0.3282
  Epoch [11/200], Batch [282/517], Loss: 0.3780
  Epoch [11/200], Batch [329/517], Loss: 0.3704
  Epoch [11/200], Batch [376/517], Loss: 0.3611
  Epoch [11/200], Batch [423/517], Loss: 0.1438
  Epoch [11/200], Batch [470/517], Loss: 0.3496
  Epoch [11/200], Batch [517/517], Loss: 0.4015
--- Epoch [11/200] complete. Average Training Loss: 0.3321 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [12/200], Batch [47/517], Loss: 0.4586
  Epoch [12/200], Batch [94/517], Loss: 0.3351
  Epoch [12/200], Batch [141/517], Loss: 0.3501
  Epoch [12/200], Batch [188/517], Loss: 0.4284
  Epoch [12/200], Batch [235/517], Loss: 0.2933
  Epoch [12/200], Batch [282/517], Loss: 0.4161
  Epoch [12/200], Batch [329/517], Loss: 0.3126
  Epoch [12/200], Batch [376/517], Loss: 0.3720
  Epoch [12/200], Batch [423/517], Loss: 0.3555
  Epoch [12/200], Batch [470/517], Loss: 0.3234
  Epoch [12/200], Batch [517/517], Loss: 0.3530
--- Epoch [12/200] complete. Average Training Loss: 0.3342 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [13/200], Batch [47/517], Loss: 0.3098
  Epoch [13/200], Batch [94/517], Loss: 0.4367
  Epoch [13/200], Batch [141/517], Loss: 0.4170
  Epoch [13/200], Batch [188/517], Loss: 0.3753
  Epoch [13/200], Batch [235/517], Loss: 0.2027
  Epoch [13/200], Batch [282/517], Loss: 0.3323
  Epoch [13/200], Batch [329/517], Loss: 0.3504
  Epoch [13/200], Batch [376/517], Loss: 0.4270
  Epoch [13/200], Batch [423/517], Loss: 0.2650
  Epoch [13/200], Batch [470/517], Loss: 0.2306
  Epoch [13/200], Batch [517/517], Loss: 0.1721
--- Epoch [13/200] complete. Average Training Loss: 0.3333 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [14/200], Batch [47/517], Loss: 0.1709
  Epoch [14/200], Batch [94/517], Loss: 0.3900
  Epoch [14/200], Batch [141/517], Loss: 0.4053
  Epoch [14/200], Batch [188/517], Loss: 0.2706
  Epoch [14/200], Batch [235/517], Loss: 0.2084
  Epoch [14/200], Batch [282/517], Loss: 0.4159
  Epoch [14/200], Batch [329/517], Loss: 0.4178
  Epoch [14/200], Batch [376/517], Loss: 0.3857
  Epoch [14/200], Batch [423/517], Loss: 0.4198
  Epoch [14/200], Batch [470/517], Loss: 0.4370
  Epoch [14/200], Batch [517/517], Loss: 0.4129
--- Epoch [14/200] complete. Average Training Loss: 0.3292 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [15/200], Batch [47/517], Loss: 0.4377
  Epoch [15/200], Batch [94/517], Loss: 0.4027
  Epoch [15/200], Batch [141/517], Loss: 0.3855
  Epoch [15/200], Batch [188/517], Loss: 0.3867
  Epoch [15/200], Batch [235/517], Loss: 0.4536
  Epoch [15/200], Batch [282/517], Loss: 0.2257
  Epoch [15/200], Batch [329/517], Loss: 0.4087
  Epoch [15/200], Batch [376/517], Loss: 0.3674
  Epoch [15/200], Batch [423/517], Loss: 0.3481
  Epoch [15/200], Batch [470/517], Loss: 0.2556
  Epoch [15/200], Batch [517/517], Loss: 0.3575
--- Epoch [15/200] complete. Average Training Loss: 0.3267 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [16/200], Batch [47/517], Loss: 0.2725
  Epoch [16/200], Batch [94/517], Loss: 0.3901
  Epoch [16/200], Batch [141/517], Loss: 0.2388
  Epoch [16/200], Batch [188/517], Loss: 0.2831
  Epoch [16/200], Batch [235/517], Loss: 0.4215
  Epoch [16/200], Batch [282/517], Loss: 0.4079
  Epoch [16/200], Batch [329/517], Loss: 0.4194
  Epoch [16/200], Batch [376/517], Loss: 0.1629
  Epoch [16/200], Batch [423/517], Loss: 0.3721
  Epoch [16/200], Batch [470/517], Loss: 0.3250
  Epoch [16/200], Batch [517/517], Loss: 0.3557
--- Epoch [16/200] complete. Average Training Loss: 0.3366 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [17/200], Batch [47/517], Loss: 0.4464
  Epoch [17/200], Batch [94/517], Loss: 0.2422
  Epoch [17/200], Batch [141/517], Loss: 0.4007
  Epoch [17/200], Batch [188/517], Loss: 0.3040
  Epoch [17/200], Batch [235/517], Loss: 0.2737
  Epoch [17/200], Batch [282/517], Loss: 0.1444
  Epoch [17/200], Batch [329/517], Loss: 0.2353
  Epoch [17/200], Batch [376/517], Loss: 0.2470
  Epoch [17/200], Batch [423/517], Loss: 0.3063
  Epoch [17/200], Batch [470/517], Loss: 0.2982
  Epoch [17/200], Batch [517/517], Loss: 0.2256
--- Epoch [17/200] complete. Average Training Loss: 0.3210 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [18/200], Batch [47/517], Loss: 0.2304
  Epoch [18/200], Batch [94/517], Loss: 0.2665
  Epoch [18/200], Batch [141/517], Loss: 0.2134
  Epoch [18/200], Batch [188/517], Loss: 0.1784
  Epoch [18/200], Batch [235/517], Loss: 0.3498
  Epoch [18/200], Batch [282/517], Loss: 0.2171
  Epoch [18/200], Batch [329/517], Loss: 0.2692
  Epoch [18/200], Batch [376/517], Loss: 0.2240
  Epoch [18/200], Batch [423/517], Loss: 0.2167
  Epoch [18/200], Batch [470/517], Loss: 0.2084
  Epoch [18/200], Batch [517/517], Loss: 0.1586
--- Epoch [18/200] complete. Average Training Loss: 0.3241 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [19/200], Batch [47/517], Loss: 0.3009
  Epoch [19/200], Batch [94/517], Loss: 0.4306
  Epoch [19/200], Batch [141/517], Loss: 0.3177
  Epoch [19/200], Batch [188/517], Loss: 0.3859
  Epoch [19/200], Batch [235/517], Loss: 0.3059
  Epoch [19/200], Batch [282/517], Loss: 0.1594
  Epoch [19/200], Batch [329/517], Loss: 0.2360
  Epoch [19/200], Batch [376/517], Loss: 0.2507
  Epoch [19/200], Batch [423/517], Loss: 0.2840
  Epoch [19/200], Batch [470/517], Loss: 0.2585
  Epoch [19/200], Batch [517/517], Loss: 0.2172
--- Epoch [19/200] complete. Average Training Loss: 0.3131 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [20/200], Batch [47/517], Loss: 0.4143
  Epoch [20/200], Batch [94/517], Loss: 0.2436
  Epoch [20/200], Batch [141/517], Loss: 0.3662
  Epoch [20/200], Batch [188/517], Loss: 0.3012
  Epoch [20/200], Batch [235/517], Loss: 0.1716
  Epoch [20/200], Batch [282/517], Loss: 0.3492
  Epoch [20/200], Batch [329/517], Loss: 0.3289
  Epoch [20/200], Batch [376/517], Loss: 0.4055
  Epoch [20/200], Batch [423/517], Loss: 0.2924
  Epoch [20/200], Batch [470/517], Loss: 0.4288
  Epoch [20/200], Batch [517/517], Loss: 0.3186
--- Epoch [20/200] complete. Average Training Loss: 0.3128 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [21/200], Batch [47/517], Loss: 0.4101
  Epoch [21/200], Batch [94/517], Loss: 0.4372
  Epoch [21/200], Batch [141/517], Loss: 0.2681
  Epoch [21/200], Batch [188/517], Loss: 0.2581
  Epoch [21/200], Batch [235/517], Loss: 0.2523
  Epoch [21/200], Batch [282/517], Loss: 0.2501
  Epoch [21/200], Batch [329/517], Loss: 0.3875
  Epoch [21/200], Batch [376/517], Loss: 0.1886
  Epoch [21/200], Batch [423/517], Loss: 0.4017
  Epoch [21/200], Batch [470/517], Loss: 0.3229
  Epoch [21/200], Batch [517/517], Loss: 0.2078
--- Epoch [21/200] complete. Average Training Loss: 0.3182 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [22/200], Batch [47/517], Loss: 0.2961
  Epoch [22/200], Batch [94/517], Loss: 0.4117
  Epoch [22/200], Batch [141/517], Loss: 0.2316
  Epoch [22/200], Batch [188/517], Loss: 0.4150
  Epoch [22/200], Batch [235/517], Loss: 0.2523
  Epoch [22/200], Batch [282/517], Loss: 0.2377
  Epoch [22/200], Batch [329/517], Loss: 0.1904
  Epoch [22/200], Batch [376/517], Loss: 0.2459
  Epoch [22/200], Batch [423/517], Loss: 0.3419
  Epoch [22/200], Batch [470/517], Loss: 0.1834
  Epoch [22/200], Batch [517/517], Loss: 0.2198
--- Epoch [22/200] complete. Average Training Loss: 0.3107 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [23/200], Batch [47/517], Loss: 0.3723
  Epoch [23/200], Batch [94/517], Loss: 0.4286
  Epoch [23/200], Batch [141/517], Loss: 0.2827
  Epoch [23/200], Batch [188/517], Loss: 0.3933
  Epoch [23/200], Batch [235/517], Loss: 0.2072
  Epoch [23/200], Batch [282/517], Loss: 0.4082
  Epoch [23/200], Batch [329/517], Loss: 0.1734
  Epoch [23/200], Batch [376/517], Loss: 0.2745
  Epoch [23/200], Batch [423/517], Loss: 0.2837
  Epoch [23/200], Batch [470/517], Loss: 0.2768
  Epoch [23/200], Batch [517/517], Loss: 0.1513
--- Epoch [23/200] complete. Average Training Loss: 0.3018 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [24/200], Batch [47/517], Loss: 0.2965
  Epoch [24/200], Batch [94/517], Loss: 0.3962
  Epoch [24/200], Batch [141/517], Loss: 0.4277
  Epoch [24/200], Batch [188/517], Loss: 0.4103
  Epoch [24/200], Batch [235/517], Loss: 0.2801
  Epoch [24/200], Batch [282/517], Loss: 0.2641
  Epoch [24/200], Batch [329/517], Loss: 0.2898
  Epoch [24/200], Batch [376/517], Loss: 0.3079
  Epoch [24/200], Batch [423/517], Loss: 0.3928
  Epoch [24/200], Batch [470/517], Loss: 0.2973
  Epoch [24/200], Batch [517/517], Loss: 0.1890
--- Epoch [24/200] complete. Average Training Loss: 0.2968 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [25/200], Batch [47/517], Loss: 0.3920
  Epoch [25/200], Batch [94/517], Loss: 0.4041
  Epoch [25/200], Batch [141/517], Loss: 0.3095
  Epoch [25/200], Batch [188/517], Loss: 0.3953
  Epoch [25/200], Batch [235/517], Loss: 0.3654
  Epoch [25/200], Batch [282/517], Loss: 0.3221
  Epoch [25/200], Batch [329/517], Loss: 0.3902
  Epoch [25/200], Batch [376/517], Loss: 0.3876
  Epoch [25/200], Batch [423/517], Loss: 0.4133
  Epoch [25/200], Batch [470/517], Loss: 0.4074
  Epoch [25/200], Batch [517/517], Loss: 0.4146
--- Epoch [25/200] complete. Average Training Loss: 0.3068 ---
--- Time taken for epoch: 313.89 seconds ---
 -- Updated Checkpoint: Saved model at 25 epochs.
  Epoch [26/200], Batch [47/517], Loss: 0.3935
  Epoch [26/200], Batch [94/517], Loss: 0.2023
  Epoch [26/200], Batch [141/517], Loss: 0.2140
  Epoch [26/200], Batch [188/517], Loss: 0.3629
  Epoch [26/200], Batch [235/517], Loss: 0.1751
  Epoch [26/200], Batch [282/517], Loss: 0.2075
  Epoch [26/200], Batch [329/517], Loss: 0.1861
  Epoch [26/200], Batch [376/517], Loss: 0.3196
  Epoch [26/200], Batch [423/517], Loss: 0.2762
  Epoch [26/200], Batch [470/517], Loss: 0.2037
  Epoch [26/200], Batch [517/517], Loss: 0.4493
--- Epoch [26/200] complete. Average Training Loss: 0.2959 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [27/200], Batch [47/517], Loss: 0.2568
  Epoch [27/200], Batch [94/517], Loss: 0.4177
  Epoch [27/200], Batch [141/517], Loss: 0.2670
  Epoch [27/200], Batch [188/517], Loss: 0.2495
  Epoch [27/200], Batch [235/517], Loss: 0.3782
  Epoch [27/200], Batch [282/517], Loss: 0.2923
  Epoch [27/200], Batch [329/517], Loss: 0.1823
  Epoch [27/200], Batch [376/517], Loss: 0.2560
  Epoch [27/200], Batch [423/517], Loss: 0.2124
  Epoch [27/200], Batch [470/517], Loss: 0.2699
  Epoch [27/200], Batch [517/517], Loss: 0.2947
--- Epoch [27/200] complete. Average Training Loss: 0.2984 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [28/200], Batch [47/517], Loss: 0.4678
  Epoch [28/200], Batch [94/517], Loss: 0.4271
  Epoch [28/200], Batch [141/517], Loss: 0.1076
  Epoch [28/200], Batch [188/517], Loss: 0.2476
  Epoch [28/200], Batch [235/517], Loss: 0.3518
  Epoch [28/200], Batch [282/517], Loss: 0.1509
  Epoch [28/200], Batch [329/517], Loss: 0.4001
  Epoch [28/200], Batch [376/517], Loss: 0.3735
  Epoch [28/200], Batch [423/517], Loss: 0.2168
  Epoch [28/200], Batch [470/517], Loss: 0.2215
  Epoch [28/200], Batch [517/517], Loss: 0.3145
--- Epoch [28/200] complete. Average Training Loss: 0.2980 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [29/200], Batch [47/517], Loss: 0.4235
  Epoch [29/200], Batch [94/517], Loss: 0.2031
  Epoch [29/200], Batch [141/517], Loss: 0.4035
  Epoch [29/200], Batch [188/517], Loss: 0.4127
  Epoch [29/200], Batch [235/517], Loss: 0.2460
  Epoch [29/200], Batch [282/517], Loss: 0.3105
  Epoch [29/200], Batch [329/517], Loss: 0.2043
  Epoch [29/200], Batch [376/517], Loss: 0.3339
  Epoch [29/200], Batch [423/517], Loss: 0.4010
  Epoch [29/200], Batch [470/517], Loss: 0.1543
  Epoch [29/200], Batch [517/517], Loss: 0.1921
--- Epoch [29/200] complete. Average Training Loss: 0.2896 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [30/200], Batch [47/517], Loss: 0.2983
  Epoch [30/200], Batch [94/517], Loss: 0.2979
  Epoch [30/200], Batch [141/517], Loss: 0.4117
  Epoch [30/200], Batch [188/517], Loss: 0.2560
  Epoch [30/200], Batch [235/517], Loss: 0.4109
  Epoch [30/200], Batch [282/517], Loss: 0.3361
  Epoch [30/200], Batch [329/517], Loss: 0.3855
  Epoch [30/200], Batch [376/517], Loss: 0.1707
  Epoch [30/200], Batch [423/517], Loss: 0.3066
  Epoch [30/200], Batch [470/517], Loss: 0.4093
  Epoch [30/200], Batch [517/517], Loss: 0.3973
--- Epoch [30/200] complete. Average Training Loss: 0.2866 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [31/200], Batch [47/517], Loss: 0.3832
  Epoch [31/200], Batch [94/517], Loss: 0.1641
  Epoch [31/200], Batch [141/517], Loss: 0.4073
  Epoch [31/200], Batch [188/517], Loss: 0.2050
  Epoch [31/200], Batch [235/517], Loss: 0.3832
  Epoch [31/200], Batch [282/517], Loss: 0.2706
  Epoch [31/200], Batch [329/517], Loss: 0.4230
  Epoch [31/200], Batch [376/517], Loss: 0.1907
  Epoch [31/200], Batch [423/517], Loss: 0.2974
  Epoch [31/200], Batch [470/517], Loss: 0.4162
  Epoch [31/200], Batch [517/517], Loss: 0.4044
--- Epoch [31/200] complete. Average Training Loss: 0.2839 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [32/200], Batch [47/517], Loss: 0.4117
  Epoch [32/200], Batch [94/517], Loss: 0.2281
  Epoch [32/200], Batch [141/517], Loss: 0.3874
  Epoch [32/200], Batch [188/517], Loss: 0.3823
  Epoch [32/200], Batch [235/517], Loss: 0.3985
  Epoch [32/200], Batch [282/517], Loss: 0.1679
  Epoch [32/200], Batch [329/517], Loss: 0.2995
  Epoch [32/200], Batch [376/517], Loss: 0.4095
  Epoch [32/200], Batch [423/517], Loss: 0.4374
  Epoch [32/200], Batch [470/517], Loss: 0.2508
  Epoch [32/200], Batch [517/517], Loss: 0.1680
--- Epoch [32/200] complete. Average Training Loss: 0.2930 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [33/200], Batch [47/517], Loss: 0.2040
  Epoch [33/200], Batch [94/517], Loss: 0.2260
  Epoch [33/200], Batch [141/517], Loss: 0.4031
  Epoch [33/200], Batch [188/517], Loss: 0.3168
  Epoch [33/200], Batch [235/517], Loss: 0.2719
  Epoch [33/200], Batch [282/517], Loss: 0.1347
  Epoch [33/200], Batch [329/517], Loss: 0.2296
  Epoch [33/200], Batch [376/517], Loss: 0.1885
  Epoch [33/200], Batch [423/517], Loss: 0.3902
  Epoch [33/200], Batch [470/517], Loss: 0.1342
  Epoch [33/200], Batch [517/517], Loss: 0.4169
--- Epoch [33/200] complete. Average Training Loss: 0.2855 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [34/200], Batch [47/517], Loss: 0.2245
  Epoch [34/200], Batch [94/517], Loss: 0.2203
  Epoch [34/200], Batch [141/517], Loss: 0.1859
  Epoch [34/200], Batch [188/517], Loss: 0.2267
  Epoch [34/200], Batch [235/517], Loss: 0.2185
  Epoch [34/200], Batch [282/517], Loss: 0.2114
  Epoch [34/200], Batch [329/517], Loss: 0.4020
  Epoch [34/200], Batch [376/517], Loss: 0.4267
  Epoch [34/200], Batch [423/517], Loss: 0.4151
  Epoch [34/200], Batch [470/517], Loss: 0.3544
  Epoch [34/200], Batch [517/517], Loss: 0.3781
--- Epoch [34/200] complete. Average Training Loss: 0.2836 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [35/200], Batch [47/517], Loss: 0.4055
  Epoch [35/200], Batch [94/517], Loss: 0.4215
  Epoch [35/200], Batch [141/517], Loss: 0.4221
  Epoch [35/200], Batch [188/517], Loss: 0.2418
  Epoch [35/200], Batch [235/517], Loss: 0.1979
  Epoch [35/200], Batch [282/517], Loss: 0.2092
  Epoch [35/200], Batch [329/517], Loss: 0.2717
  Epoch [35/200], Batch [376/517], Loss: 0.1211
  Epoch [35/200], Batch [423/517], Loss: 0.1684
  Epoch [35/200], Batch [470/517], Loss: 0.2357
  Epoch [35/200], Batch [517/517], Loss: 0.4034
--- Epoch [35/200] complete. Average Training Loss: 0.2768 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [36/200], Batch [47/517], Loss: 0.3268
  Epoch [36/200], Batch [94/517], Loss: 0.1392
  Epoch [36/200], Batch [141/517], Loss: 0.3553
  Epoch [36/200], Batch [188/517], Loss: 0.1899
  Epoch [36/200], Batch [235/517], Loss: 0.4041
  Epoch [36/200], Batch [282/517], Loss: 0.2231
  Epoch [36/200], Batch [329/517], Loss: 0.2934
  Epoch [36/200], Batch [376/517], Loss: 0.2198
  Epoch [36/200], Batch [423/517], Loss: 0.4002
  Epoch [36/200], Batch [470/517], Loss: 0.3757
  Epoch [36/200], Batch [517/517], Loss: 0.2860
--- Epoch [36/200] complete. Average Training Loss: 0.2766 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [37/200], Batch [47/517], Loss: 0.4101
  Epoch [37/200], Batch [94/517], Loss: 0.1929
  Epoch [37/200], Batch [141/517], Loss: 0.4291
  Epoch [37/200], Batch [188/517], Loss: 0.3026
  Epoch [37/200], Batch [235/517], Loss: 0.1831
  Epoch [37/200], Batch [282/517], Loss: 0.2162
  Epoch [37/200], Batch [329/517], Loss: 0.3309
  Epoch [37/200], Batch [376/517], Loss: 0.1667
  Epoch [37/200], Batch [423/517], Loss: 0.3020
  Epoch [37/200], Batch [470/517], Loss: 0.1865
  Epoch [37/200], Batch [517/517], Loss: 0.4057
--- Epoch [37/200] complete. Average Training Loss: 0.2797 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [38/200], Batch [47/517], Loss: 0.2827
  Epoch [38/200], Batch [94/517], Loss: 0.4315
  Epoch [38/200], Batch [141/517], Loss: 0.4182
  Epoch [38/200], Batch [188/517], Loss: 0.1347
  Epoch [38/200], Batch [235/517], Loss: 0.1601
  Epoch [38/200], Batch [282/517], Loss: 0.3846
  Epoch [38/200], Batch [329/517], Loss: 0.2307
  Epoch [38/200], Batch [376/517], Loss: 0.1692
  Epoch [38/200], Batch [423/517], Loss: 0.2252
  Epoch [38/200], Batch [470/517], Loss: 0.3898
  Epoch [38/200], Batch [517/517], Loss: 0.2069
--- Epoch [38/200] complete. Average Training Loss: 0.2729 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [39/200], Batch [47/517], Loss: 0.1851
  Epoch [39/200], Batch [94/517], Loss: 0.2197
  Epoch [39/200], Batch [141/517], Loss: 0.3887
  Epoch [39/200], Batch [188/517], Loss: 0.1668
  Epoch [39/200], Batch [235/517], Loss: 0.3496
  Epoch [39/200], Batch [282/517], Loss: 0.3376
  Epoch [39/200], Batch [329/517], Loss: 0.3605
  Epoch [39/200], Batch [376/517], Loss: 0.2769
  Epoch [39/200], Batch [423/517], Loss: 0.3966
  Epoch [39/200], Batch [470/517], Loss: 0.1989
  Epoch [39/200], Batch [517/517], Loss: 0.4303
--- Epoch [39/200] complete. Average Training Loss: 0.2739 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [40/200], Batch [47/517], Loss: 0.2312
  Epoch [40/200], Batch [94/517], Loss: 0.3073
  Epoch [40/200], Batch [141/517], Loss: 0.1852
  Epoch [40/200], Batch [188/517], Loss: 0.2785
  Epoch [40/200], Batch [235/517], Loss: 0.1778
  Epoch [40/200], Batch [282/517], Loss: 0.1177
  Epoch [40/200], Batch [329/517], Loss: 0.1923
  Epoch [40/200], Batch [376/517], Loss: 0.4190
  Epoch [40/200], Batch [423/517], Loss: 0.2722
  Epoch [40/200], Batch [470/517], Loss: 0.2089
  Epoch [40/200], Batch [517/517], Loss: 0.3852
--- Epoch [40/200] complete. Average Training Loss: 0.2731 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [41/200], Batch [47/517], Loss: 0.2584
  Epoch [41/200], Batch [94/517], Loss: 0.3027
  Epoch [41/200], Batch [141/517], Loss: 0.1985
  Epoch [41/200], Batch [188/517], Loss: 0.1467
  Epoch [41/200], Batch [235/517], Loss: 0.1705
  Epoch [41/200], Batch [282/517], Loss: 0.2887
  Epoch [41/200], Batch [329/517], Loss: 0.2848
  Epoch [41/200], Batch [376/517], Loss: 0.1711
  Epoch [41/200], Batch [423/517], Loss: 0.1393
  Epoch [41/200], Batch [470/517], Loss: 0.2673
  Epoch [41/200], Batch [517/517], Loss: 0.1664
--- Epoch [41/200] complete. Average Training Loss: 0.2700 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [42/200], Batch [47/517], Loss: 0.2839
  Epoch [42/200], Batch [94/517], Loss: 0.1935
  Epoch [42/200], Batch [141/517], Loss: 0.2852
  Epoch [42/200], Batch [188/517], Loss: 0.4556
  Epoch [42/200], Batch [235/517], Loss: 0.1985
  Epoch [42/200], Batch [282/517], Loss: 0.1661
  Epoch [42/200], Batch [329/517], Loss: 0.1714
  Epoch [42/200], Batch [376/517], Loss: 0.4358
  Epoch [42/200], Batch [423/517], Loss: 0.2098
  Epoch [42/200], Batch [470/517], Loss: 0.1681
  Epoch [42/200], Batch [517/517], Loss: 0.2097
--- Epoch [42/200] complete. Average Training Loss: 0.2665 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [43/200], Batch [47/517], Loss: 0.1572
  Epoch [43/200], Batch [94/517], Loss: 0.2353
  Epoch [43/200], Batch [141/517], Loss: 0.0981
  Epoch [43/200], Batch [188/517], Loss: 0.1289
  Epoch [43/200], Batch [235/517], Loss: 0.1993
  Epoch [43/200], Batch [282/517], Loss: 0.3008
  Epoch [43/200], Batch [329/517], Loss: 0.1722
  Epoch [43/200], Batch [376/517], Loss: 0.1709
  Epoch [43/200], Batch [423/517], Loss: 0.1678
  Epoch [43/200], Batch [470/517], Loss: 0.4012
  Epoch [43/200], Batch [517/517], Loss: 0.1431
--- Epoch [43/200] complete. Average Training Loss: 0.2599 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [44/200], Batch [47/517], Loss: 0.2858
  Epoch [44/200], Batch [94/517], Loss: 0.1706
  Epoch [44/200], Batch [141/517], Loss: 0.3069
  Epoch [44/200], Batch [188/517], Loss: 0.2302
  Epoch [44/200], Batch [235/517], Loss: 0.1023
  Epoch [44/200], Batch [282/517], Loss: 0.3156
  Epoch [44/200], Batch [329/517], Loss: 0.3455
  Epoch [44/200], Batch [376/517], Loss: 0.4233
  Epoch [44/200], Batch [423/517], Loss: 0.2126
  Epoch [44/200], Batch [470/517], Loss: 0.1711
  Epoch [44/200], Batch [517/517], Loss: 0.3949
--- Epoch [44/200] complete. Average Training Loss: 0.2590 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [45/200], Batch [47/517], Loss: 0.2761
  Epoch [45/200], Batch [94/517], Loss: 0.3172
  Epoch [45/200], Batch [141/517], Loss: 0.0970
  Epoch [45/200], Batch [188/517], Loss: 0.3882
  Epoch [45/200], Batch [235/517], Loss: 0.2627
  Epoch [45/200], Batch [282/517], Loss: 0.3050
  Epoch [45/200], Batch [329/517], Loss: 0.1644
  Epoch [45/200], Batch [376/517], Loss: 0.1321
  Epoch [45/200], Batch [423/517], Loss: 0.2719
  Epoch [45/200], Batch [470/517], Loss: 0.0878
  Epoch [45/200], Batch [517/517], Loss: 0.4339
--- Epoch [45/200] complete. Average Training Loss: 0.2610 ---
--- Time taken for epoch: 314.11 seconds ---
  Epoch [46/200], Batch [47/517], Loss: 0.1637
  Epoch [46/200], Batch [94/517], Loss: 0.2487
  Epoch [46/200], Batch [141/517], Loss: 0.3229
  Epoch [46/200], Batch [188/517], Loss: 0.1968
  Epoch [46/200], Batch [235/517], Loss: 0.3832
  Epoch [46/200], Batch [282/517], Loss: 0.3643
  Epoch [46/200], Batch [329/517], Loss: 0.2321
  Epoch [46/200], Batch [376/517], Loss: 0.4013
  Epoch [46/200], Batch [423/517], Loss: 0.4241
  Epoch [46/200], Batch [470/517], Loss: 0.2720
  Epoch [46/200], Batch [517/517], Loss: 0.1499
--- Epoch [46/200] complete. Average Training Loss: 0.2635 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [47/200], Batch [47/517], Loss: 0.3842
  Epoch [47/200], Batch [94/517], Loss: 0.1015
  Epoch [47/200], Batch [141/517], Loss: 0.2904
  Epoch [47/200], Batch [188/517], Loss: 0.4227
  Epoch [47/200], Batch [235/517], Loss: 0.2344
  Epoch [47/200], Batch [282/517], Loss: 0.1324
  Epoch [47/200], Batch [329/517], Loss: 0.4117
  Epoch [47/200], Batch [376/517], Loss: 0.1160
  Epoch [47/200], Batch [423/517], Loss: 0.2124
  Epoch [47/200], Batch [470/517], Loss: 0.1806
  Epoch [47/200], Batch [517/517], Loss: 0.1954
--- Epoch [47/200] complete. Average Training Loss: 0.2573 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [48/200], Batch [47/517], Loss: 0.1654
  Epoch [48/200], Batch [94/517], Loss: 0.2660
  Epoch [48/200], Batch [141/517], Loss: 0.3822
  Epoch [48/200], Batch [188/517], Loss: 0.1729
  Epoch [48/200], Batch [235/517], Loss: 0.1521
  Epoch [48/200], Batch [282/517], Loss: 0.3779
  Epoch [48/200], Batch [329/517], Loss: 0.3830
  Epoch [48/200], Batch [376/517], Loss: 0.2815
  Epoch [48/200], Batch [423/517], Loss: 0.2311
  Epoch [48/200], Batch [470/517], Loss: 0.2214
  Epoch [48/200], Batch [517/517], Loss: 0.4405
--- Epoch [48/200] complete. Average Training Loss: 0.2476 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [49/200], Batch [47/517], Loss: 0.2928
  Epoch [49/200], Batch [94/517], Loss: 0.1465
  Epoch [49/200], Batch [141/517], Loss: 0.3978
  Epoch [49/200], Batch [188/517], Loss: 0.2211
  Epoch [49/200], Batch [235/517], Loss: 0.2543
  Epoch [49/200], Batch [282/517], Loss: 0.2939
  Epoch [49/200], Batch [329/517], Loss: 0.2017
  Epoch [49/200], Batch [376/517], Loss: 0.2264
  Epoch [49/200], Batch [423/517], Loss: 0.3899
  Epoch [49/200], Batch [470/517], Loss: 0.4133
  Epoch [49/200], Batch [517/517], Loss: 0.3493
--- Epoch [49/200] complete. Average Training Loss: 0.2593 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [50/200], Batch [47/517], Loss: 0.1073
  Epoch [50/200], Batch [94/517], Loss: 0.1608
  Epoch [50/200], Batch [141/517], Loss: 0.4101
  Epoch [50/200], Batch [188/517], Loss: 0.0842
  Epoch [50/200], Batch [235/517], Loss: 0.4054
  Epoch [50/200], Batch [282/517], Loss: 0.2227
  Epoch [50/200], Batch [329/517], Loss: 0.2078
  Epoch [50/200], Batch [376/517], Loss: 0.2661
  Epoch [50/200], Batch [423/517], Loss: 0.2638
  Epoch [50/200], Batch [470/517], Loss: 0.1946
  Epoch [50/200], Batch [517/517], Loss: 0.4178
--- Epoch [50/200] complete. Average Training Loss: 0.2633 ---
--- Time taken for epoch: 313.98 seconds ---
 -- Updated Checkpoint: Saved model at 50 epochs.
  Epoch [51/200], Batch [47/517], Loss: 0.1852
  Epoch [51/200], Batch [94/517], Loss: 0.1760
  Epoch [51/200], Batch [141/517], Loss: 0.1400
  Epoch [51/200], Batch [188/517], Loss: 0.1767
  Epoch [51/200], Batch [235/517], Loss: 0.1056
  Epoch [51/200], Batch [282/517], Loss: 0.2239
  Epoch [51/200], Batch [329/517], Loss: 0.4131
  Epoch [51/200], Batch [376/517], Loss: 0.1904
  Epoch [51/200], Batch [423/517], Loss: 0.1966
  Epoch [51/200], Batch [470/517], Loss: 0.1818
  Epoch [51/200], Batch [517/517], Loss: 0.2116
--- Epoch [51/200] complete. Average Training Loss: 0.2596 ---
--- Time taken for epoch: 314.06 seconds ---
  Epoch [52/200], Batch [47/517], Loss: 0.1803
  Epoch [52/200], Batch [94/517], Loss: 0.2037
  Epoch [52/200], Batch [141/517], Loss: 0.2507
  Epoch [52/200], Batch [188/517], Loss: 0.2007
  Epoch [52/200], Batch [235/517], Loss: 0.1702
  Epoch [52/200], Batch [282/517], Loss: 0.2154
  Epoch [52/200], Batch [329/517], Loss: 0.2958
  Epoch [52/200], Batch [376/517], Loss: 0.2155
  Epoch [52/200], Batch [423/517], Loss: 0.1820
  Epoch [52/200], Batch [470/517], Loss: 0.1510
  Epoch [52/200], Batch [517/517], Loss: 0.1856
--- Epoch [52/200] complete. Average Training Loss: 0.2511 ---
--- Time taken for epoch: 314.18 seconds ---
  Epoch [53/200], Batch [47/517], Loss: 0.3756
  Epoch [53/200], Batch [94/517], Loss: 0.3449
  Epoch [53/200], Batch [141/517], Loss: 0.3991
  Epoch [53/200], Batch [188/517], Loss: 0.1400
  Epoch [53/200], Batch [235/517], Loss: 0.3906
  Epoch [53/200], Batch [282/517], Loss: 0.1790
  Epoch [53/200], Batch [329/517], Loss: 0.2659
  Epoch [53/200], Batch [376/517], Loss: 0.3895
  Epoch [53/200], Batch [423/517], Loss: 0.1406
  Epoch [53/200], Batch [470/517], Loss: 0.4032
  Epoch [53/200], Batch [517/517], Loss: 0.3863
--- Epoch [53/200] complete. Average Training Loss: 0.2506 ---
--- Time taken for epoch: 314.12 seconds ---
  Epoch [54/200], Batch [47/517], Loss: 0.1707
  Epoch [54/200], Batch [94/517], Loss: 0.3241
  Epoch [54/200], Batch [141/517], Loss: 0.4073
  Epoch [54/200], Batch [188/517], Loss: 0.2446
  Epoch [54/200], Batch [235/517], Loss: 0.2127
  Epoch [54/200], Batch [282/517], Loss: 0.1672
  Epoch [54/200], Batch [329/517], Loss: 0.1740
  Epoch [54/200], Batch [376/517], Loss: 0.1528
  Epoch [54/200], Batch [423/517], Loss: 0.3429
  Epoch [54/200], Batch [470/517], Loss: 0.2317
  Epoch [54/200], Batch [517/517], Loss: 0.2309
--- Epoch [54/200] complete. Average Training Loss: 0.2458 ---
--- Time taken for epoch: 314.20 seconds ---
  Epoch [55/200], Batch [47/517], Loss: 0.4179
  Epoch [55/200], Batch [94/517], Loss: 0.1569
  Epoch [55/200], Batch [141/517], Loss: 0.2611
  Epoch [55/200], Batch [188/517], Loss: 0.2083
  Epoch [55/200], Batch [235/517], Loss: 0.3981
  Epoch [55/200], Batch [282/517], Loss: 0.4194
  Epoch [55/200], Batch [329/517], Loss: 0.1510
  Epoch [55/200], Batch [376/517], Loss: 0.1078
  Epoch [55/200], Batch [423/517], Loss: 0.1795
  Epoch [55/200], Batch [470/517], Loss: 0.3934
  Epoch [55/200], Batch [517/517], Loss: 0.4199
--- Epoch [55/200] complete. Average Training Loss: 0.2441 ---
--- Time taken for epoch: 314.27 seconds ---
  Epoch [56/200], Batch [47/517], Loss: 0.1269
  Epoch [56/200], Batch [94/517], Loss: 0.4056
  Epoch [56/200], Batch [141/517], Loss: 0.3933
  Epoch [56/200], Batch [188/517], Loss: 0.1874
  Epoch [56/200], Batch [235/517], Loss: 0.1725
  Epoch [56/200], Batch [282/517], Loss: 0.1649
  Epoch [56/200], Batch [329/517], Loss: 0.3927
  Epoch [56/200], Batch [376/517], Loss: 0.1827
  Epoch [56/200], Batch [423/517], Loss: 0.1715
  Epoch [56/200], Batch [470/517], Loss: 0.2003
  Epoch [56/200], Batch [517/517], Loss: 0.3843
--- Epoch [56/200] complete. Average Training Loss: 0.2482 ---
--- Time taken for epoch: 314.39 seconds ---
  Epoch [57/200], Batch [47/517], Loss: 0.1550
  Epoch [57/200], Batch [94/517], Loss: 0.3076
  Epoch [57/200], Batch [141/517], Loss: 0.4067
  Epoch [57/200], Batch [188/517], Loss: 0.3053
  Epoch [57/200], Batch [235/517], Loss: 0.1832
  Epoch [57/200], Batch [282/517], Loss: 0.3531
  Epoch [57/200], Batch [329/517], Loss: 0.4040
  Epoch [57/200], Batch [376/517], Loss: 0.2766
  Epoch [57/200], Batch [423/517], Loss: 0.2213
  Epoch [57/200], Batch [470/517], Loss: 0.2749
  Epoch [57/200], Batch [517/517], Loss: 0.2070
--- Epoch [57/200] complete. Average Training Loss: 0.2613 ---
--- Time taken for epoch: 314.26 seconds ---
  Epoch [58/200], Batch [47/517], Loss: 0.4038
  Epoch [58/200], Batch [94/517], Loss: 0.2624
  Epoch [58/200], Batch [141/517], Loss: 0.3983
  Epoch [58/200], Batch [188/517], Loss: 0.1818
  Epoch [58/200], Batch [235/517], Loss: 0.1952
  Epoch [58/200], Batch [282/517], Loss: 0.2924
  Epoch [58/200], Batch [329/517], Loss: 0.2473
  Epoch [58/200], Batch [376/517], Loss: 0.2146
  Epoch [58/200], Batch [423/517], Loss: 0.1162
  Epoch [58/200], Batch [470/517], Loss: 0.1441
  Epoch [58/200], Batch [517/517], Loss: 0.1522
--- Epoch [58/200] complete. Average Training Loss: 0.2542 ---
--- Time taken for epoch: 314.25 seconds ---
  Epoch [59/200], Batch [47/517], Loss: 0.2524
  Epoch [59/200], Batch [94/517], Loss: 0.3990
  Epoch [59/200], Batch [141/517], Loss: 0.1534
  Epoch [59/200], Batch [188/517], Loss: 0.1774
  Epoch [59/200], Batch [235/517], Loss: 0.3042
  Epoch [59/200], Batch [282/517], Loss: 0.2165
  Epoch [59/200], Batch [329/517], Loss: 0.4218
  Epoch [59/200], Batch [376/517], Loss: 0.4412
  Epoch [59/200], Batch [423/517], Loss: 0.1675
  Epoch [59/200], Batch [470/517], Loss: 0.4249
  Epoch [59/200], Batch [517/517], Loss: 0.1687
--- Epoch [59/200] complete. Average Training Loss: 0.2435 ---
--- Time taken for epoch: 314.17 seconds ---
  Epoch [60/200], Batch [47/517], Loss: 0.3914
  Epoch [60/200], Batch [94/517], Loss: 0.2014
  Epoch [60/200], Batch [141/517], Loss: 0.4194
  Epoch [60/200], Batch [188/517], Loss: 0.1449
  Epoch [60/200], Batch [235/517], Loss: 0.2122
  Epoch [60/200], Batch [282/517], Loss: 0.3931
  Epoch [60/200], Batch [329/517], Loss: 0.1241
  Epoch [60/200], Batch [376/517], Loss: 0.1841
  Epoch [60/200], Batch [423/517], Loss: 0.2735
  Epoch [60/200], Batch [470/517], Loss: 0.1554
  Epoch [60/200], Batch [517/517], Loss: 0.2519
--- Epoch [60/200] complete. Average Training Loss: 0.2439 ---
--- Time taken for epoch: 314.16 seconds ---
  Epoch [61/200], Batch [47/517], Loss: 0.2304
  Epoch [61/200], Batch [94/517], Loss: 0.4162
  Epoch [61/200], Batch [141/517], Loss: 0.2729
  Epoch [61/200], Batch [188/517], Loss: 0.1691
  Epoch [61/200], Batch [235/517], Loss: 0.4134
  Epoch [61/200], Batch [282/517], Loss: 0.3908
  Epoch [61/200], Batch [329/517], Loss: 0.1520
  Epoch [61/200], Batch [376/517], Loss: 0.2420
  Epoch [61/200], Batch [423/517], Loss: 0.1770
  Epoch [61/200], Batch [470/517], Loss: 0.1296
  Epoch [61/200], Batch [517/517], Loss: 0.2179
--- Epoch [61/200] complete. Average Training Loss: 0.2418 ---
--- Time taken for epoch: 314.22 seconds ---
  Epoch [62/200], Batch [47/517], Loss: 0.2125
  Epoch [62/200], Batch [94/517], Loss: 0.1776
  Epoch [62/200], Batch [141/517], Loss: 0.1986
  Epoch [62/200], Batch [188/517], Loss: 0.1680
  Epoch [62/200], Batch [235/517], Loss: 0.1874
  Epoch [62/200], Batch [282/517], Loss: 0.1793
  Epoch [62/200], Batch [329/517], Loss: 0.3816
  Epoch [62/200], Batch [376/517], Loss: 0.2395
  Epoch [62/200], Batch [423/517], Loss: 0.1606
  Epoch [62/200], Batch [470/517], Loss: 0.1632
  Epoch [62/200], Batch [517/517], Loss: 0.1450
--- Epoch [62/200] complete. Average Training Loss: 0.2461 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [63/200], Batch [47/517], Loss: 0.1462
  Epoch [63/200], Batch [94/517], Loss: 0.2645
  Epoch [63/200], Batch [141/517], Loss: 0.4144
  Epoch [63/200], Batch [188/517], Loss: 0.3875
  Epoch [63/200], Batch [235/517], Loss: 0.1253
  Epoch [63/200], Batch [282/517], Loss: 0.1708
  Epoch [63/200], Batch [329/517], Loss: 0.1433
  Epoch [63/200], Batch [376/517], Loss: 0.1634
  Epoch [63/200], Batch [423/517], Loss: 0.4013
  Epoch [63/200], Batch [470/517], Loss: 0.1758
  Epoch [63/200], Batch [517/517], Loss: 0.3057
--- Epoch [63/200] complete. Average Training Loss: 0.2433 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [64/200], Batch [47/517], Loss: 0.2752
  Epoch [64/200], Batch [94/517], Loss: 0.4448
  Epoch [64/200], Batch [141/517], Loss: 0.1201
  Epoch [64/200], Batch [188/517], Loss: 0.1203
  Epoch [64/200], Batch [235/517], Loss: 0.2346
  Epoch [64/200], Batch [282/517], Loss: 0.1851
  Epoch [64/200], Batch [329/517], Loss: 0.3804
  Epoch [64/200], Batch [376/517], Loss: 0.1638
  Epoch [64/200], Batch [423/517], Loss: 0.1887
  Epoch [64/200], Batch [470/517], Loss: 0.3054
  Epoch [64/200], Batch [517/517], Loss: 0.3678
--- Epoch [64/200] complete. Average Training Loss: 0.2404 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [65/200], Batch [47/517], Loss: 0.0930
  Epoch [65/200], Batch [94/517], Loss: 0.2534
  Epoch [65/200], Batch [141/517], Loss: 0.3982
  Epoch [65/200], Batch [188/517], Loss: 0.1736
  Epoch [65/200], Batch [235/517], Loss: 0.4153
  Epoch [65/200], Batch [282/517], Loss: 0.2286
  Epoch [65/200], Batch [329/517], Loss: 0.2273
  Epoch [65/200], Batch [376/517], Loss: 0.1573
  Epoch [65/200], Batch [423/517], Loss: 0.1103
  Epoch [65/200], Batch [470/517], Loss: 0.2793
  Epoch [65/200], Batch [517/517], Loss: 0.2461
--- Epoch [65/200] complete. Average Training Loss: 0.2442 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [66/200], Batch [47/517], Loss: 0.4106
  Epoch [66/200], Batch [94/517], Loss: 0.3946
  Epoch [66/200], Batch [141/517], Loss: 0.3991
  Epoch [66/200], Batch [188/517], Loss: 0.4055
  Epoch [66/200], Batch [235/517], Loss: 0.4319
  Epoch [66/200], Batch [282/517], Loss: 0.2234
  Epoch [66/200], Batch [329/517], Loss: 0.3466
  Epoch [66/200], Batch [376/517], Loss: 0.2524
  Epoch [66/200], Batch [423/517], Loss: 0.2061
  Epoch [66/200], Batch [470/517], Loss: 0.3315
  Epoch [66/200], Batch [517/517], Loss: 0.1656
--- Epoch [66/200] complete. Average Training Loss: 0.2488 ---
--- Time taken for epoch: 314.19 seconds ---
  Epoch [67/200], Batch [47/517], Loss: 0.2067
  Epoch [67/200], Batch [94/517], Loss: 0.1894
  Epoch [67/200], Batch [141/517], Loss: 0.1499
  Epoch [67/200], Batch [188/517], Loss: 0.1920
  Epoch [67/200], Batch [235/517], Loss: 0.1471
  Epoch [67/200], Batch [282/517], Loss: 0.1581
  Epoch [67/200], Batch [329/517], Loss: 0.1883
  Epoch [67/200], Batch [376/517], Loss: 0.1037
  Epoch [67/200], Batch [423/517], Loss: 0.1769
  Epoch [67/200], Batch [470/517], Loss: 0.1698
  Epoch [67/200], Batch [517/517], Loss: 0.2441
--- Epoch [67/200] complete. Average Training Loss: 0.2413 ---
--- Time taken for epoch: 314.15 seconds ---
  Epoch [68/200], Batch [47/517], Loss: 0.4103
  Epoch [68/200], Batch [94/517], Loss: 0.3982
  Epoch [68/200], Batch [141/517], Loss: 0.1773
  Epoch [68/200], Batch [188/517], Loss: 0.1314
  Epoch [68/200], Batch [235/517], Loss: 0.2259
  Epoch [68/200], Batch [282/517], Loss: 0.1836
  Epoch [68/200], Batch [329/517], Loss: 0.2667
  Epoch [68/200], Batch [376/517], Loss: 0.1604
  Epoch [68/200], Batch [423/517], Loss: 0.3214
  Epoch [68/200], Batch [470/517], Loss: 0.1644
  Epoch [68/200], Batch [517/517], Loss: 0.2114
--- Epoch [68/200] complete. Average Training Loss: 0.2414 ---
--- Time taken for epoch: 314.14 seconds ---
  Epoch [69/200], Batch [47/517], Loss: 0.1643
  Epoch [69/200], Batch [94/517], Loss: 0.2102
  Epoch [69/200], Batch [141/517], Loss: 0.4042
  Epoch [69/200], Batch [188/517], Loss: 0.1794
  Epoch [69/200], Batch [235/517], Loss: 0.3838
  Epoch [69/200], Batch [282/517], Loss: 0.3808
  Epoch [69/200], Batch [329/517], Loss: 0.1430
  Epoch [69/200], Batch [376/517], Loss: 0.2096
  Epoch [69/200], Batch [423/517], Loss: 0.1759
  Epoch [69/200], Batch [470/517], Loss: 0.4069
  Epoch [69/200], Batch [517/517], Loss: 0.1761
--- Epoch [69/200] complete. Average Training Loss: 0.2316 ---
--- Time taken for epoch: 314.12 seconds ---
  Epoch [70/200], Batch [47/517], Loss: 0.2884
  Epoch [70/200], Batch [94/517], Loss: 0.3845
  Epoch [70/200], Batch [141/517], Loss: 0.2125
  Epoch [70/200], Batch [188/517], Loss: 0.1718
  Epoch [70/200], Batch [235/517], Loss: 0.1740
  Epoch [70/200], Batch [282/517], Loss: 0.1048
  Epoch [70/200], Batch [329/517], Loss: 0.2095
  Epoch [70/200], Batch [376/517], Loss: 0.3885
  Epoch [70/200], Batch [423/517], Loss: 0.2051
  Epoch [70/200], Batch [470/517], Loss: 0.1700
  Epoch [70/200], Batch [517/517], Loss: 0.3859
--- Epoch [70/200] complete. Average Training Loss: 0.2377 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [71/200], Batch [47/517], Loss: 0.3397
  Epoch [71/200], Batch [94/517], Loss: 0.2355
  Epoch [71/200], Batch [141/517], Loss: 0.2529
  Epoch [71/200], Batch [188/517], Loss: 0.2225
  Epoch [71/200], Batch [235/517], Loss: 0.3935
  Epoch [71/200], Batch [282/517], Loss: 0.2055
  Epoch [71/200], Batch [329/517], Loss: 0.1151
  Epoch [71/200], Batch [376/517], Loss: 0.4021
  Epoch [71/200], Batch [423/517], Loss: 0.0986
  Epoch [71/200], Batch [470/517], Loss: 0.2356
  Epoch [71/200], Batch [517/517], Loss: 0.1120
--- Epoch [71/200] complete. Average Training Loss: 0.2357 ---
--- Time taken for epoch: 314.10 seconds ---
  Epoch [72/200], Batch [47/517], Loss: 0.1525
  Epoch [72/200], Batch [94/517], Loss: 0.4118
  Epoch [72/200], Batch [141/517], Loss: 0.1905
  Epoch [72/200], Batch [188/517], Loss: 0.2087
  Epoch [72/200], Batch [235/517], Loss: 0.1216
  Epoch [72/200], Batch [282/517], Loss: 0.3966
  Epoch [72/200], Batch [329/517], Loss: 0.1290
  Epoch [72/200], Batch [376/517], Loss: 0.1811
  Epoch [72/200], Batch [423/517], Loss: 0.1940
  Epoch [72/200], Batch [470/517], Loss: 0.2061
  Epoch [72/200], Batch [517/517], Loss: 0.4414
--- Epoch [72/200] complete. Average Training Loss: 0.2364 ---
--- Time taken for epoch: 314.27 seconds ---
  Epoch [73/200], Batch [47/517], Loss: 0.1187
  Epoch [73/200], Batch [94/517], Loss: 0.1568
  Epoch [73/200], Batch [141/517], Loss: 0.1810
  Epoch [73/200], Batch [188/517], Loss: 0.3791
  Epoch [73/200], Batch [235/517], Loss: 0.1515
  Epoch [73/200], Batch [282/517], Loss: 0.1750
  Epoch [73/200], Batch [329/517], Loss: 0.2176
  Epoch [73/200], Batch [376/517], Loss: 0.2310
  Epoch [73/200], Batch [423/517], Loss: 0.3880
  Epoch [73/200], Batch [470/517], Loss: 0.3918
  Epoch [73/200], Batch [517/517], Loss: 0.1294
--- Epoch [73/200] complete. Average Training Loss: 0.2423 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [74/200], Batch [47/517], Loss: 0.1641
  Epoch [74/200], Batch [94/517], Loss: 0.1594
  Epoch [74/200], Batch [141/517], Loss: 0.1318
  Epoch [74/200], Batch [188/517], Loss: 0.2291
  Epoch [74/200], Batch [235/517], Loss: 0.2417
  Epoch [74/200], Batch [282/517], Loss: 0.1549
  Epoch [74/200], Batch [329/517], Loss: 0.3198
  Epoch [74/200], Batch [376/517], Loss: 0.2151
  Epoch [74/200], Batch [423/517], Loss: 0.2295
  Epoch [74/200], Batch [470/517], Loss: 0.2659
  Epoch [74/200], Batch [517/517], Loss: 0.2624
--- Epoch [74/200] complete. Average Training Loss: 0.2336 ---
--- Time taken for epoch: 314.12 seconds ---
  Epoch [75/200], Batch [47/517], Loss: 0.2228
  Epoch [75/200], Batch [94/517], Loss: 0.3882
  Epoch [75/200], Batch [141/517], Loss: 0.1917
  Epoch [75/200], Batch [188/517], Loss: 0.1772
  Epoch [75/200], Batch [235/517], Loss: 0.1402
  Epoch [75/200], Batch [282/517], Loss: 0.1151
  Epoch [75/200], Batch [329/517], Loss: 0.1737
  Epoch [75/200], Batch [376/517], Loss: 0.1975
  Epoch [75/200], Batch [423/517], Loss: 0.2030
  Epoch [75/200], Batch [470/517], Loss: 0.4036
  Epoch [75/200], Batch [517/517], Loss: 0.3828
--- Epoch [75/200] complete. Average Training Loss: 0.2340 ---
--- Time taken for epoch: 314.03 seconds ---
 -- Updated Checkpoint: Saved model at 75 epochs.
  Epoch [76/200], Batch [47/517], Loss: 0.3913
  Epoch [76/200], Batch [94/517], Loss: 0.1278
  Epoch [76/200], Batch [141/517], Loss: 0.1893
  Epoch [76/200], Batch [188/517], Loss: 0.1484
  Epoch [76/200], Batch [235/517], Loss: 0.1459
  Epoch [76/200], Batch [282/517], Loss: 0.3157
  Epoch [76/200], Batch [329/517], Loss: 0.1915
  Epoch [76/200], Batch [376/517], Loss: 0.1846
  Epoch [76/200], Batch [423/517], Loss: 0.2176
  Epoch [76/200], Batch [470/517], Loss: 0.2227
  Epoch [76/200], Batch [517/517], Loss: 0.1641
--- Epoch [76/200] complete. Average Training Loss: 0.2320 ---
--- Time taken for epoch: 314.16 seconds ---
  Epoch [77/200], Batch [47/517], Loss: 0.1453
  Epoch [77/200], Batch [94/517], Loss: 0.1813
  Epoch [77/200], Batch [141/517], Loss: 0.1990
  Epoch [77/200], Batch [188/517], Loss: 0.1979
  Epoch [77/200], Batch [235/517], Loss: 0.1755
  Epoch [77/200], Batch [282/517], Loss: 0.1147
  Epoch [77/200], Batch [329/517], Loss: 0.4456
  Epoch [77/200], Batch [376/517], Loss: 0.1385
  Epoch [77/200], Batch [423/517], Loss: 0.1460
  Epoch [77/200], Batch [470/517], Loss: 0.1780
  Epoch [77/200], Batch [517/517], Loss: 0.3118
--- Epoch [77/200] complete. Average Training Loss: 0.2221 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [78/200], Batch [47/517], Loss: 0.2507
  Epoch [78/200], Batch [94/517], Loss: 0.3929
  Epoch [78/200], Batch [141/517], Loss: 0.1867
  Epoch [78/200], Batch [188/517], Loss: 0.1396
  Epoch [78/200], Batch [235/517], Loss: 0.1396
  Epoch [78/200], Batch [282/517], Loss: 0.4026
  Epoch [78/200], Batch [329/517], Loss: 0.4171
  Epoch [78/200], Batch [376/517], Loss: 0.3962
  Epoch [78/200], Batch [423/517], Loss: 0.1297
  Epoch [78/200], Batch [470/517], Loss: 0.1297
  Epoch [78/200], Batch [517/517], Loss: 0.1146
--- Epoch [78/200] complete. Average Training Loss: 0.2404 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [79/200], Batch [47/517], Loss: 0.2416
  Epoch [79/200], Batch [94/517], Loss: 0.1352
  Epoch [79/200], Batch [141/517], Loss: 0.1383
  Epoch [79/200], Batch [188/517], Loss: 0.2098
  Epoch [79/200], Batch [235/517], Loss: 0.2201
  Epoch [79/200], Batch [282/517], Loss: 0.2446
  Epoch [79/200], Batch [329/517], Loss: 0.4239
  Epoch [79/200], Batch [376/517], Loss: 0.3075
  Epoch [79/200], Batch [423/517], Loss: 0.1918
  Epoch [79/200], Batch [470/517], Loss: 0.1655
  Epoch [79/200], Batch [517/517], Loss: 0.3103
--- Epoch [79/200] complete. Average Training Loss: 0.2252 ---
--- Time taken for epoch: 314.15 seconds ---
  Epoch [80/200], Batch [47/517], Loss: 0.2585
  Epoch [80/200], Batch [94/517], Loss: 0.1535
  Epoch [80/200], Batch [141/517], Loss: 0.2209
  Epoch [80/200], Batch [188/517], Loss: 0.0876
  Epoch [80/200], Batch [235/517], Loss: 0.1159
  Epoch [80/200], Batch [282/517], Loss: 0.2347
  Epoch [80/200], Batch [329/517], Loss: 0.1969
  Epoch [80/200], Batch [376/517], Loss: 0.1873
  Epoch [80/200], Batch [423/517], Loss: 0.2067
  Epoch [80/200], Batch [470/517], Loss: 0.1751
  Epoch [80/200], Batch [517/517], Loss: 0.3953
--- Epoch [80/200] complete. Average Training Loss: 0.2400 ---
--- Time taken for epoch: 314.21 seconds ---
  Epoch [81/200], Batch [47/517], Loss: 0.1738
  Epoch [81/200], Batch [94/517], Loss: 0.2287
  Epoch [81/200], Batch [141/517], Loss: 0.2649
  Epoch [81/200], Batch [188/517], Loss: 0.2055
  Epoch [81/200], Batch [235/517], Loss: 0.2402
  Epoch [81/200], Batch [282/517], Loss: 0.1707
  Epoch [81/200], Batch [329/517], Loss: 0.1331
  Epoch [81/200], Batch [376/517], Loss: 0.1368
  Epoch [81/200], Batch [423/517], Loss: 0.3811
  Epoch [81/200], Batch [470/517], Loss: 0.3312
  Epoch [81/200], Batch [517/517], Loss: 0.4172
--- Epoch [81/200] complete. Average Training Loss: 0.2253 ---
--- Time taken for epoch: 314.18 seconds ---
  Epoch [82/200], Batch [47/517], Loss: 0.0801
  Epoch [82/200], Batch [94/517], Loss: 0.1506
  Epoch [82/200], Batch [141/517], Loss: 0.2836
  Epoch [82/200], Batch [188/517], Loss: 0.1528
  Epoch [82/200], Batch [235/517], Loss: 0.1464
  Epoch [82/200], Batch [282/517], Loss: 0.4050
  Epoch [82/200], Batch [329/517], Loss: 0.3974
  Epoch [82/200], Batch [376/517], Loss: 0.3893
  Epoch [82/200], Batch [423/517], Loss: 0.2336
  Epoch [82/200], Batch [470/517], Loss: 0.3251
  Epoch [82/200], Batch [517/517], Loss: 0.3440
--- Epoch [82/200] complete. Average Training Loss: 0.2370 ---
--- Time taken for epoch: 314.10 seconds ---
  Epoch [83/200], Batch [47/517], Loss: 0.3189
  Epoch [83/200], Batch [94/517], Loss: 0.1887
  Epoch [83/200], Batch [141/517], Loss: 0.1284
  Epoch [83/200], Batch [188/517], Loss: 0.1588
  Epoch [83/200], Batch [235/517], Loss: 0.1522
  Epoch [83/200], Batch [282/517], Loss: 0.2460
  Epoch [83/200], Batch [329/517], Loss: 0.3623
  Epoch [83/200], Batch [376/517], Loss: 0.1253
  Epoch [83/200], Batch [423/517], Loss: 0.2023
  Epoch [83/200], Batch [470/517], Loss: 0.2221
  Epoch [83/200], Batch [517/517], Loss: 0.0975
--- Epoch [83/200] complete. Average Training Loss: 0.2321 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [84/200], Batch [47/517], Loss: 0.3926
  Epoch [84/200], Batch [94/517], Loss: 0.1609
  Epoch [84/200], Batch [141/517], Loss: 0.1927
  Epoch [84/200], Batch [188/517], Loss: 0.1698
  Epoch [84/200], Batch [235/517], Loss: 0.1691
  Epoch [84/200], Batch [282/517], Loss: 0.1069
  Epoch [84/200], Batch [329/517], Loss: 0.2019
  Epoch [84/200], Batch [376/517], Loss: 0.1818
  Epoch [84/200], Batch [423/517], Loss: 0.2120
  Epoch [84/200], Batch [470/517], Loss: 0.1638
  Epoch [84/200], Batch [517/517], Loss: 0.4120
--- Epoch [84/200] complete. Average Training Loss: 0.2363 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [85/200], Batch [47/517], Loss: 0.1386
  Epoch [85/200], Batch [94/517], Loss: 0.1437
  Epoch [85/200], Batch [141/517], Loss: 0.1582
  Epoch [85/200], Batch [188/517], Loss: 0.1970
  Epoch [85/200], Batch [235/517], Loss: 0.2130
  Epoch [85/200], Batch [282/517], Loss: 0.2584
  Epoch [85/200], Batch [329/517], Loss: 0.1918
  Epoch [85/200], Batch [376/517], Loss: 0.2236
  Epoch [85/200], Batch [423/517], Loss: 0.1594
  Epoch [85/200], Batch [470/517], Loss: 0.1402
  Epoch [85/200], Batch [517/517], Loss: 0.1987
--- Epoch [85/200] complete. Average Training Loss: 0.2232 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [86/200], Batch [47/517], Loss: 0.1892
  Epoch [86/200], Batch [94/517], Loss: 0.1822
  Epoch [86/200], Batch [141/517], Loss: 0.1258
  Epoch [86/200], Batch [188/517], Loss: 0.1563
  Epoch [86/200], Batch [235/517], Loss: 0.1528
  Epoch [86/200], Batch [282/517], Loss: 0.4443
  Epoch [86/200], Batch [329/517], Loss: 0.1941
  Epoch [86/200], Batch [376/517], Loss: 0.2035
  Epoch [86/200], Batch [423/517], Loss: 0.1764
  Epoch [86/200], Batch [470/517], Loss: 0.1181
  Epoch [86/200], Batch [517/517], Loss: 0.3929
--- Epoch [86/200] complete. Average Training Loss: 0.2186 ---
--- Time taken for epoch: 314.31 seconds ---
  Epoch [87/200], Batch [47/517], Loss: 0.1334
  Epoch [87/200], Batch [94/517], Loss: 0.3940
  Epoch [87/200], Batch [141/517], Loss: 0.1627
  Epoch [87/200], Batch [188/517], Loss: 0.2108
  Epoch [87/200], Batch [235/517], Loss: 0.1736
  Epoch [87/200], Batch [282/517], Loss: 0.4039
  Epoch [87/200], Batch [329/517], Loss: 0.3940
  Epoch [87/200], Batch [376/517], Loss: 0.0948
  Epoch [87/200], Batch [423/517], Loss: 0.1737
  Epoch [87/200], Batch [470/517], Loss: 0.1629
  Epoch [87/200], Batch [517/517], Loss: 0.3909
--- Epoch [87/200] complete. Average Training Loss: 0.2231 ---
--- Time taken for epoch: 314.10 seconds ---
  Epoch [88/200], Batch [47/517], Loss: 0.1897
  Epoch [88/200], Batch [94/517], Loss: 0.2478
  Epoch [88/200], Batch [141/517], Loss: 0.1961
  Epoch [88/200], Batch [188/517], Loss: 0.1116
  Epoch [88/200], Batch [235/517], Loss: 0.3144
  Epoch [88/200], Batch [282/517], Loss: 0.1766
  Epoch [88/200], Batch [329/517], Loss: 0.0793
  Epoch [88/200], Batch [376/517], Loss: 0.0966
  Epoch [88/200], Batch [423/517], Loss: 0.2340
  Epoch [88/200], Batch [470/517], Loss: 0.1734
  Epoch [88/200], Batch [517/517], Loss: 0.1679
--- Epoch [88/200] complete. Average Training Loss: 0.2196 ---
--- Time taken for epoch: 314.19 seconds ---
  Epoch [89/200], Batch [47/517], Loss: 0.2699
  Epoch [89/200], Batch [94/517], Loss: 0.1528
  Epoch [89/200], Batch [141/517], Loss: 0.2319
  Epoch [89/200], Batch [188/517], Loss: 0.2313
  Epoch [89/200], Batch [235/517], Loss: 0.1086
  Epoch [89/200], Batch [282/517], Loss: 0.2778
  Epoch [89/200], Batch [329/517], Loss: 0.0934
  Epoch [89/200], Batch [376/517], Loss: 0.1600
  Epoch [89/200], Batch [423/517], Loss: 0.1831
  Epoch [89/200], Batch [470/517], Loss: 0.1389
  Epoch [89/200], Batch [517/517], Loss: 0.2463
--- Epoch [89/200] complete. Average Training Loss: 0.2235 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [90/200], Batch [47/517], Loss: 0.3846
  Epoch [90/200], Batch [94/517], Loss: 0.2137
  Epoch [90/200], Batch [141/517], Loss: 0.3865
  Epoch [90/200], Batch [188/517], Loss: 0.1244
  Epoch [90/200], Batch [235/517], Loss: 0.1071
  Epoch [90/200], Batch [282/517], Loss: 0.4033
  Epoch [90/200], Batch [329/517], Loss: 0.4387
  Epoch [90/200], Batch [376/517], Loss: 0.2713
  Epoch [90/200], Batch [423/517], Loss: 0.2093
  Epoch [90/200], Batch [470/517], Loss: 0.1749
  Epoch [90/200], Batch [517/517], Loss: 0.1184
--- Epoch [90/200] complete. Average Training Loss: 0.2335 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [91/200], Batch [47/517], Loss: 0.1157
  Epoch [91/200], Batch [94/517], Loss: 0.1325
  Epoch [91/200], Batch [141/517], Loss: 0.1131
  Epoch [91/200], Batch [188/517], Loss: 0.1227
  Epoch [91/200], Batch [235/517], Loss: 0.1947
  Epoch [91/200], Batch [282/517], Loss: 0.2069
  Epoch [91/200], Batch [329/517], Loss: 0.1531
  Epoch [91/200], Batch [376/517], Loss: 0.1172
  Epoch [91/200], Batch [423/517], Loss: 0.2088
  Epoch [91/200], Batch [470/517], Loss: 0.2100
  Epoch [91/200], Batch [517/517], Loss: 0.1678
--- Epoch [91/200] complete. Average Training Loss: 0.2276 ---
--- Time taken for epoch: 314.21 seconds ---
  Epoch [92/200], Batch [47/517], Loss: 0.2294
  Epoch [92/200], Batch [94/517], Loss: 0.2355
  Epoch [92/200], Batch [141/517], Loss: 0.1442
  Epoch [92/200], Batch [188/517], Loss: 0.0908
  Epoch [92/200], Batch [235/517], Loss: 0.2474
  Epoch [92/200], Batch [282/517], Loss: 0.3863
  Epoch [92/200], Batch [329/517], Loss: 0.2477
  Epoch [92/200], Batch [376/517], Loss: 0.1097
  Epoch [92/200], Batch [423/517], Loss: 0.2181
  Epoch [92/200], Batch [470/517], Loss: 0.2082
  Epoch [92/200], Batch [517/517], Loss: 0.2154
--- Epoch [92/200] complete. Average Training Loss: 0.2324 ---
--- Time taken for epoch: 314.14 seconds ---
  Epoch [93/200], Batch [47/517], Loss: 0.1234
  Epoch [93/200], Batch [94/517], Loss: 0.2017
  Epoch [93/200], Batch [141/517], Loss: 0.1563
  Epoch [93/200], Batch [188/517], Loss: 0.1661
  Epoch [93/200], Batch [235/517], Loss: 0.3887
  Epoch [93/200], Batch [282/517], Loss: 0.1501
  Epoch [93/200], Batch [329/517], Loss: 0.3990
  Epoch [93/200], Batch [376/517], Loss: 0.1192
  Epoch [93/200], Batch [423/517], Loss: 0.2446
  Epoch [93/200], Batch [470/517], Loss: 0.3643
  Epoch [93/200], Batch [517/517], Loss: 0.4083
--- Epoch [93/200] complete. Average Training Loss: 0.2208 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [94/200], Batch [47/517], Loss: 0.4048
  Epoch [94/200], Batch [94/517], Loss: 0.1501
  Epoch [94/200], Batch [141/517], Loss: 0.2337
  Epoch [94/200], Batch [188/517], Loss: 0.2132
  Epoch [94/200], Batch [235/517], Loss: 0.3942
  Epoch [94/200], Batch [282/517], Loss: 0.3974
  Epoch [94/200], Batch [329/517], Loss: 0.1445
  Epoch [94/200], Batch [376/517], Loss: 0.0979
  Epoch [94/200], Batch [423/517], Loss: 0.2276
  Epoch [94/200], Batch [470/517], Loss: 0.3985
  Epoch [94/200], Batch [517/517], Loss: 0.3959
--- Epoch [94/200] complete. Average Training Loss: 0.2212 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [95/200], Batch [47/517], Loss: 0.1163
  Epoch [95/200], Batch [94/517], Loss: 0.1443
  Epoch [95/200], Batch [141/517], Loss: 0.1383
  Epoch [95/200], Batch [188/517], Loss: 0.1072
  Epoch [95/200], Batch [235/517], Loss: 0.2057
  Epoch [95/200], Batch [282/517], Loss: 0.1056
  Epoch [95/200], Batch [329/517], Loss: 0.4020
  Epoch [95/200], Batch [376/517], Loss: 0.0886
  Epoch [95/200], Batch [423/517], Loss: 0.3863
  Epoch [95/200], Batch [470/517], Loss: 0.2017
  Epoch [95/200], Batch [517/517], Loss: 0.1898
--- Epoch [95/200] complete. Average Training Loss: 0.2201 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [96/200], Batch [47/517], Loss: 0.3907
  Epoch [96/200], Batch [94/517], Loss: 0.1112
  Epoch [96/200], Batch [141/517], Loss: 0.3812
  Epoch [96/200], Batch [188/517], Loss: 0.1953
  Epoch [96/200], Batch [235/517], Loss: 0.4678
  Epoch [96/200], Batch [282/517], Loss: 0.1012
  Epoch [96/200], Batch [329/517], Loss: 0.2687
  Epoch [96/200], Batch [376/517], Loss: 0.1261
  Epoch [96/200], Batch [423/517], Loss: 0.1764
  Epoch [96/200], Batch [470/517], Loss: 0.2603
  Epoch [96/200], Batch [517/517], Loss: 0.2460
--- Epoch [96/200] complete. Average Training Loss: 0.2256 ---
--- Time taken for epoch: 314.08 seconds ---
  Epoch [97/200], Batch [47/517], Loss: 0.2065
  Epoch [97/200], Batch [94/517], Loss: 0.4161
  Epoch [97/200], Batch [141/517], Loss: 0.4104
  Epoch [97/200], Batch [188/517], Loss: 0.3874
  Epoch [97/200], Batch [235/517], Loss: 0.1041
  Epoch [97/200], Batch [282/517], Loss: 0.2785
  Epoch [97/200], Batch [329/517], Loss: 0.2842
  Epoch [97/200], Batch [376/517], Loss: 0.3932
  Epoch [97/200], Batch [423/517], Loss: 0.1750
  Epoch [97/200], Batch [470/517], Loss: 0.1567
  Epoch [97/200], Batch [517/517], Loss: 0.4305
--- Epoch [97/200] complete. Average Training Loss: 0.2210 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [98/200], Batch [47/517], Loss: 0.4128
  Epoch [98/200], Batch [94/517], Loss: 0.1018
  Epoch [98/200], Batch [141/517], Loss: 0.3945
  Epoch [98/200], Batch [188/517], Loss: 0.3935
  Epoch [98/200], Batch [235/517], Loss: 0.2953
  Epoch [98/200], Batch [282/517], Loss: 0.1627
  Epoch [98/200], Batch [329/517], Loss: 0.2449
  Epoch [98/200], Batch [376/517], Loss: 0.2262
  Epoch [98/200], Batch [423/517], Loss: 0.4272
  Epoch [98/200], Batch [470/517], Loss: 0.3903
  Epoch [98/200], Batch [517/517], Loss: 0.1473
--- Epoch [98/200] complete. Average Training Loss: 0.2248 ---
--- Time taken for epoch: 314.15 seconds ---
  Epoch [99/200], Batch [47/517], Loss: 0.2002
  Epoch [99/200], Batch [94/517], Loss: 0.0894
  Epoch [99/200], Batch [141/517], Loss: 0.1613
  Epoch [99/200], Batch [188/517], Loss: 0.1423
  Epoch [99/200], Batch [235/517], Loss: 0.2812
  Epoch [99/200], Batch [282/517], Loss: 0.2563
  Epoch [99/200], Batch [329/517], Loss: 0.3992
  Epoch [99/200], Batch [376/517], Loss: 0.1530
  Epoch [99/200], Batch [423/517], Loss: 0.3957
  Epoch [99/200], Batch [470/517], Loss: 0.1220
  Epoch [99/200], Batch [517/517], Loss: 0.1689
--- Epoch [99/200] complete. Average Training Loss: 0.2172 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [100/200], Batch [47/517], Loss: 0.2232
  Epoch [100/200], Batch [94/517], Loss: 0.1264
  Epoch [100/200], Batch [141/517], Loss: 0.1381
  Epoch [100/200], Batch [188/517], Loss: 0.1450
  Epoch [100/200], Batch [235/517], Loss: 0.2095
  Epoch [100/200], Batch [282/517], Loss: 0.1714
  Epoch [100/200], Batch [329/517], Loss: 0.2351
  Epoch [100/200], Batch [376/517], Loss: 0.1184
  Epoch [100/200], Batch [423/517], Loss: 0.2054
  Epoch [100/200], Batch [470/517], Loss: 0.1559
  Epoch [100/200], Batch [517/517], Loss: 0.1396
--- Epoch [100/200] complete. Average Training Loss: 0.2148 ---
--- Time taken for epoch: 314.07 seconds ---
 -- Updated Checkpoint: Saved model at 100 epochs.
  Epoch [101/200], Batch [47/517], Loss: 0.1021
  Epoch [101/200], Batch [94/517], Loss: 0.3932
  Epoch [101/200], Batch [141/517], Loss: 0.1193
  Epoch [101/200], Batch [188/517], Loss: 0.1763
  Epoch [101/200], Batch [235/517], Loss: 0.2123
  Epoch [101/200], Batch [282/517], Loss: 0.3124
  Epoch [101/200], Batch [329/517], Loss: 0.1809
  Epoch [101/200], Batch [376/517], Loss: 0.1418
  Epoch [101/200], Batch [423/517], Loss: 0.2458
  Epoch [101/200], Batch [470/517], Loss: 0.3863
  Epoch [101/200], Batch [517/517], Loss: 0.1455
--- Epoch [101/200] complete. Average Training Loss: 0.2133 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [102/200], Batch [47/517], Loss: 0.1942
  Epoch [102/200], Batch [94/517], Loss: 0.1225
  Epoch [102/200], Batch [141/517], Loss: 0.2135
  Epoch [102/200], Batch [188/517], Loss: 0.1134
  Epoch [102/200], Batch [235/517], Loss: 0.1665
  Epoch [102/200], Batch [282/517], Loss: 0.1403
  Epoch [102/200], Batch [329/517], Loss: 0.3916
  Epoch [102/200], Batch [376/517], Loss: 0.2533
  Epoch [102/200], Batch [423/517], Loss: 0.1281
  Epoch [102/200], Batch [470/517], Loss: 0.1784
  Epoch [102/200], Batch [517/517], Loss: 0.1981
--- Epoch [102/200] complete. Average Training Loss: 0.2177 ---
--- Time taken for epoch: 314.11 seconds ---
  Epoch [103/200], Batch [47/517], Loss: 0.1864
  Epoch [103/200], Batch [94/517], Loss: 0.2016
  Epoch [103/200], Batch [141/517], Loss: 0.3818
  Epoch [103/200], Batch [188/517], Loss: 0.1784
  Epoch [103/200], Batch [235/517], Loss: 0.0812
  Epoch [103/200], Batch [282/517], Loss: 0.1048
  Epoch [103/200], Batch [329/517], Loss: 0.1847
  Epoch [103/200], Batch [376/517], Loss: 0.1092
  Epoch [103/200], Batch [423/517], Loss: 0.1141
  Epoch [103/200], Batch [470/517], Loss: 0.1686
  Epoch [103/200], Batch [517/517], Loss: 0.3899
--- Epoch [103/200] complete. Average Training Loss: 0.2141 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [104/200], Batch [47/517], Loss: 0.1484
  Epoch [104/200], Batch [94/517], Loss: 0.2489
  Epoch [104/200], Batch [141/517], Loss: 0.1931
  Epoch [104/200], Batch [188/517], Loss: 0.2161
  Epoch [104/200], Batch [235/517], Loss: 0.1041
  Epoch [104/200], Batch [282/517], Loss: 0.1944
  Epoch [104/200], Batch [329/517], Loss: 0.1039
  Epoch [104/200], Batch [376/517], Loss: 0.0988
  Epoch [104/200], Batch [423/517], Loss: 0.1412
  Epoch [104/200], Batch [470/517], Loss: 0.1697
  Epoch [104/200], Batch [517/517], Loss: 0.1873
--- Epoch [104/200] complete. Average Training Loss: 0.2135 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [105/200], Batch [47/517], Loss: 0.3921
  Epoch [105/200], Batch [94/517], Loss: 0.4138
  Epoch [105/200], Batch [141/517], Loss: 0.1657
  Epoch [105/200], Batch [188/517], Loss: 0.1423
  Epoch [105/200], Batch [235/517], Loss: 0.1492
  Epoch [105/200], Batch [282/517], Loss: 0.4171
  Epoch [105/200], Batch [329/517], Loss: 0.1938
  Epoch [105/200], Batch [376/517], Loss: 0.1091
  Epoch [105/200], Batch [423/517], Loss: 0.3810
  Epoch [105/200], Batch [470/517], Loss: 0.3995
  Epoch [105/200], Batch [517/517], Loss: 0.1355
--- Epoch [105/200] complete. Average Training Loss: 0.2325 ---
--- Time taken for epoch: 314.18 seconds ---
  Epoch [106/200], Batch [47/517], Loss: 0.2035
  Epoch [106/200], Batch [94/517], Loss: 0.2126
  Epoch [106/200], Batch [141/517], Loss: 0.2193
  Epoch [106/200], Batch [188/517], Loss: 0.1064
  Epoch [106/200], Batch [235/517], Loss: 0.3855
  Epoch [106/200], Batch [282/517], Loss: 0.1274
  Epoch [106/200], Batch [329/517], Loss: 0.0816
  Epoch [106/200], Batch [376/517], Loss: 0.1634
  Epoch [106/200], Batch [423/517], Loss: 0.2128
  Epoch [106/200], Batch [470/517], Loss: 0.3460
  Epoch [106/200], Batch [517/517], Loss: 0.1309
--- Epoch [106/200] complete. Average Training Loss: 0.2159 ---
--- Time taken for epoch: 314.21 seconds ---
  Epoch [107/200], Batch [47/517], Loss: 0.1478
  Epoch [107/200], Batch [94/517], Loss: 0.3967
  Epoch [107/200], Batch [141/517], Loss: 0.1031
  Epoch [107/200], Batch [188/517], Loss: 0.1359
  Epoch [107/200], Batch [235/517], Loss: 0.1240
  Epoch [107/200], Batch [282/517], Loss: 0.1469
  Epoch [107/200], Batch [329/517], Loss: 0.3915
  Epoch [107/200], Batch [376/517], Loss: 0.1138
  Epoch [107/200], Batch [423/517], Loss: 0.1306
  Epoch [107/200], Batch [470/517], Loss: 0.1980
  Epoch [107/200], Batch [517/517], Loss: 0.2286
--- Epoch [107/200] complete. Average Training Loss: 0.2079 ---
--- Time taken for epoch: 314.10 seconds ---
  Epoch [108/200], Batch [47/517], Loss: 0.1802
  Epoch [108/200], Batch [94/517], Loss: 0.3877
  Epoch [108/200], Batch [141/517], Loss: 0.1094
  Epoch [108/200], Batch [188/517], Loss: 0.3989
  Epoch [108/200], Batch [235/517], Loss: 0.1563
  Epoch [108/200], Batch [282/517], Loss: 0.2666
  Epoch [108/200], Batch [329/517], Loss: 0.1749
  Epoch [108/200], Batch [376/517], Loss: 0.1684
  Epoch [108/200], Batch [423/517], Loss: 0.4054
  Epoch [108/200], Batch [470/517], Loss: 0.3172
  Epoch [108/200], Batch [517/517], Loss: 0.1794
--- Epoch [108/200] complete. Average Training Loss: 0.2099 ---
--- Time taken for epoch: 314.17 seconds ---
  Epoch [109/200], Batch [47/517], Loss: 0.1345
  Epoch [109/200], Batch [94/517], Loss: 0.1288
  Epoch [109/200], Batch [141/517], Loss: 0.1349
  Epoch [109/200], Batch [188/517], Loss: 0.0970
  Epoch [109/200], Batch [235/517], Loss: 0.0939
  Epoch [109/200], Batch [282/517], Loss: 0.4192
  Epoch [109/200], Batch [329/517], Loss: 0.1826
  Epoch [109/200], Batch [376/517], Loss: 0.1021
  Epoch [109/200], Batch [423/517], Loss: 0.3922
  Epoch [109/200], Batch [470/517], Loss: 0.2474
  Epoch [109/200], Batch [517/517], Loss: 0.0866
--- Epoch [109/200] complete. Average Training Loss: 0.2180 ---
--- Time taken for epoch: 314.08 seconds ---
  Epoch [110/200], Batch [47/517], Loss: 0.4173
  Epoch [110/200], Batch [94/517], Loss: 0.3851
  Epoch [110/200], Batch [141/517], Loss: 0.1634
  Epoch [110/200], Batch [188/517], Loss: 0.2425
  Epoch [110/200], Batch [235/517], Loss: 0.1353
  Epoch [110/200], Batch [282/517], Loss: 0.3809
  Epoch [110/200], Batch [329/517], Loss: 0.1011
  Epoch [110/200], Batch [376/517], Loss: 0.1565
  Epoch [110/200], Batch [423/517], Loss: 0.1686
  Epoch [110/200], Batch [470/517], Loss: 0.1392
  Epoch [110/200], Batch [517/517], Loss: 0.4019
--- Epoch [110/200] complete. Average Training Loss: 0.2169 ---
--- Time taken for epoch: 314.22 seconds ---
  Epoch [111/200], Batch [47/517], Loss: 0.2809
  Epoch [111/200], Batch [94/517], Loss: 0.1763
  Epoch [111/200], Batch [141/517], Loss: 0.1599
  Epoch [111/200], Batch [188/517], Loss: 0.2222
  Epoch [111/200], Batch [235/517], Loss: 0.1707
  Epoch [111/200], Batch [282/517], Loss: 0.2407
  Epoch [111/200], Batch [329/517], Loss: 0.2922
  Epoch [111/200], Batch [376/517], Loss: 0.3964
  Epoch [111/200], Batch [423/517], Loss: 0.1148
  Epoch [111/200], Batch [470/517], Loss: 0.1639
  Epoch [111/200], Batch [517/517], Loss: 0.4200
--- Epoch [111/200] complete. Average Training Loss: 0.2070 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [112/200], Batch [47/517], Loss: 0.1868
  Epoch [112/200], Batch [94/517], Loss: 0.3843
  Epoch [112/200], Batch [141/517], Loss: 0.1123
  Epoch [112/200], Batch [188/517], Loss: 0.1537
  Epoch [112/200], Batch [235/517], Loss: 0.2072
  Epoch [112/200], Batch [282/517], Loss: 0.1958
  Epoch [112/200], Batch [329/517], Loss: 0.1455
  Epoch [112/200], Batch [376/517], Loss: 0.2244
  Epoch [112/200], Batch [423/517], Loss: 0.3927
  Epoch [112/200], Batch [470/517], Loss: 0.2012
  Epoch [112/200], Batch [517/517], Loss: 0.3765
--- Epoch [112/200] complete. Average Training Loss: 0.2179 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [113/200], Batch [47/517], Loss: 0.4096
  Epoch [113/200], Batch [94/517], Loss: 0.1111
  Epoch [113/200], Batch [141/517], Loss: 0.3793
  Epoch [113/200], Batch [188/517], Loss: 0.4301
  Epoch [113/200], Batch [235/517], Loss: 0.1991
  Epoch [113/200], Batch [282/517], Loss: 0.1048
  Epoch [113/200], Batch [329/517], Loss: 0.1876
  Epoch [113/200], Batch [376/517], Loss: 0.1694
  Epoch [113/200], Batch [423/517], Loss: 0.3917
  Epoch [113/200], Batch [470/517], Loss: 0.3876
  Epoch [113/200], Batch [517/517], Loss: 0.1218
--- Epoch [113/200] complete. Average Training Loss: 0.2057 ---
--- Time taken for epoch: 314.13 seconds ---
  Epoch [114/200], Batch [47/517], Loss: 0.3901
  Epoch [114/200], Batch [94/517], Loss: 0.2894
  Epoch [114/200], Batch [141/517], Loss: 0.1835
  Epoch [114/200], Batch [188/517], Loss: 0.1606
  Epoch [114/200], Batch [235/517], Loss: 0.1156
  Epoch [114/200], Batch [282/517], Loss: 0.3810
  Epoch [114/200], Batch [329/517], Loss: 0.1770
  Epoch [114/200], Batch [376/517], Loss: 0.3149
  Epoch [114/200], Batch [423/517], Loss: 0.2335
  Epoch [114/200], Batch [470/517], Loss: 0.0958
  Epoch [114/200], Batch [517/517], Loss: 0.3014
--- Epoch [114/200] complete. Average Training Loss: 0.2220 ---
--- Time taken for epoch: 314.16 seconds ---
  Epoch [115/200], Batch [47/517], Loss: 0.0991
  Epoch [115/200], Batch [94/517], Loss: 0.2072
  Epoch [115/200], Batch [141/517], Loss: 0.1282
  Epoch [115/200], Batch [188/517], Loss: 0.0891
  Epoch [115/200], Batch [235/517], Loss: 0.2392
  Epoch [115/200], Batch [282/517], Loss: 0.4157
  Epoch [115/200], Batch [329/517], Loss: 0.1337
  Epoch [115/200], Batch [376/517], Loss: 0.1032
  Epoch [115/200], Batch [423/517], Loss: 0.3010
  Epoch [115/200], Batch [470/517], Loss: 0.3867
  Epoch [115/200], Batch [517/517], Loss: 0.2250
--- Epoch [115/200] complete. Average Training Loss: 0.2109 ---
--- Time taken for epoch: 314.22 seconds ---
  Epoch [116/200], Batch [47/517], Loss: 0.3821
  Epoch [116/200], Batch [94/517], Loss: 0.1896
  Epoch [116/200], Batch [141/517], Loss: 0.1656
  Epoch [116/200], Batch [188/517], Loss: 0.3884
  Epoch [116/200], Batch [235/517], Loss: 0.4084
  Epoch [116/200], Batch [282/517], Loss: 0.1843
  Epoch [116/200], Batch [329/517], Loss: 0.1137
  Epoch [116/200], Batch [376/517], Loss: 0.1521
  Epoch [116/200], Batch [423/517], Loss: 0.1456
  Epoch [116/200], Batch [470/517], Loss: 0.1581
  Epoch [116/200], Batch [517/517], Loss: 0.3848
--- Epoch [116/200] complete. Average Training Loss: 0.2103 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [117/200], Batch [47/517], Loss: 0.1554
  Epoch [117/200], Batch [94/517], Loss: 0.0689
  Epoch [117/200], Batch [141/517], Loss: 0.0997
  Epoch [117/200], Batch [188/517], Loss: 0.3920
  Epoch [117/200], Batch [235/517], Loss: 0.4143
  Epoch [117/200], Batch [282/517], Loss: 0.1930
  Epoch [117/200], Batch [329/517], Loss: 0.1289
  Epoch [117/200], Batch [376/517], Loss: 0.3761
  Epoch [117/200], Batch [423/517], Loss: 0.2467
  Epoch [117/200], Batch [470/517], Loss: 0.1402
  Epoch [117/200], Batch [517/517], Loss: 0.0891
--- Epoch [117/200] complete. Average Training Loss: 0.2121 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [118/200], Batch [47/517], Loss: 0.3839
  Epoch [118/200], Batch [94/517], Loss: 0.1239
  Epoch [118/200], Batch [141/517], Loss: 0.4100
  Epoch [118/200], Batch [188/517], Loss: 0.2059
  Epoch [118/200], Batch [235/517], Loss: 0.1502
  Epoch [118/200], Batch [282/517], Loss: 0.2476
  Epoch [118/200], Batch [329/517], Loss: 0.2485
  Epoch [118/200], Batch [376/517], Loss: 0.1605
  Epoch [118/200], Batch [423/517], Loss: 0.3884
  Epoch [118/200], Batch [470/517], Loss: 0.2217
  Epoch [118/200], Batch [517/517], Loss: 0.4020
--- Epoch [118/200] complete. Average Training Loss: 0.2089 ---
--- Time taken for epoch: 314.03 seconds ---
  Epoch [119/200], Batch [47/517], Loss: 0.1162
  Epoch [119/200], Batch [94/517], Loss: 0.1796
  Epoch [119/200], Batch [141/517], Loss: 0.2399
  Epoch [119/200], Batch [188/517], Loss: 0.1560
  Epoch [119/200], Batch [235/517], Loss: 0.2664
  Epoch [119/200], Batch [282/517], Loss: 0.3082
  Epoch [119/200], Batch [329/517], Loss: 0.2513
  Epoch [119/200], Batch [376/517], Loss: 0.1616
  Epoch [119/200], Batch [423/517], Loss: 0.1705
  Epoch [119/200], Batch [470/517], Loss: 0.2362
  Epoch [119/200], Batch [517/517], Loss: 0.1698
--- Epoch [119/200] complete. Average Training Loss: 0.2037 ---
--- Time taken for epoch: 314.21 seconds ---
  Epoch [120/200], Batch [47/517], Loss: 0.1855
  Epoch [120/200], Batch [94/517], Loss: 0.3932
  Epoch [120/200], Batch [141/517], Loss: 0.1516
  Epoch [120/200], Batch [188/517], Loss: 0.1787
  Epoch [120/200], Batch [235/517], Loss: 0.1136
  Epoch [120/200], Batch [282/517], Loss: 0.4229
  Epoch [120/200], Batch [329/517], Loss: 0.3941
  Epoch [120/200], Batch [376/517], Loss: 0.2350
  Epoch [120/200], Batch [423/517], Loss: 0.1507
  Epoch [120/200], Batch [470/517], Loss: 0.1406
  Epoch [120/200], Batch [517/517], Loss: 0.3728
--- Epoch [120/200] complete. Average Training Loss: 0.2204 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [121/200], Batch [47/517], Loss: 0.1629
  Epoch [121/200], Batch [94/517], Loss: 0.2070
  Epoch [121/200], Batch [141/517], Loss: 0.1518
  Epoch [121/200], Batch [188/517], Loss: 0.2608
  Epoch [121/200], Batch [235/517], Loss: 0.2414
  Epoch [121/200], Batch [282/517], Loss: 0.1143
  Epoch [121/200], Batch [329/517], Loss: 0.3729
  Epoch [121/200], Batch [376/517], Loss: 0.1622
  Epoch [121/200], Batch [423/517], Loss: 0.3905
  Epoch [121/200], Batch [470/517], Loss: 0.3945
  Epoch [121/200], Batch [517/517], Loss: 0.1421
--- Epoch [121/200] complete. Average Training Loss: 0.2197 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [122/200], Batch [47/517], Loss: 0.2272
  Epoch [122/200], Batch [94/517], Loss: 0.4183
  Epoch [122/200], Batch [141/517], Loss: 0.1053
  Epoch [122/200], Batch [188/517], Loss: 0.1109
  Epoch [122/200], Batch [235/517], Loss: 0.0812
  Epoch [122/200], Batch [282/517], Loss: 0.2262
  Epoch [122/200], Batch [329/517], Loss: 0.2025
  Epoch [122/200], Batch [376/517], Loss: 0.1502
  Epoch [122/200], Batch [423/517], Loss: 0.1226
  Epoch [122/200], Batch [470/517], Loss: 0.0871
  Epoch [122/200], Batch [517/517], Loss: 0.1187
--- Epoch [122/200] complete. Average Training Loss: 0.2110 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [123/200], Batch [47/517], Loss: 0.1415
  Epoch [123/200], Batch [94/517], Loss: 0.1388
  Epoch [123/200], Batch [141/517], Loss: 0.2761
  Epoch [123/200], Batch [188/517], Loss: 0.1543
  Epoch [123/200], Batch [235/517], Loss: 0.1189
  Epoch [123/200], Batch [282/517], Loss: 0.1601
  Epoch [123/200], Batch [329/517], Loss: 0.1611
  Epoch [123/200], Batch [376/517], Loss: 0.1026
  Epoch [123/200], Batch [423/517], Loss: 0.3864
  Epoch [123/200], Batch [470/517], Loss: 0.1248
  Epoch [123/200], Batch [517/517], Loss: 0.1201
--- Epoch [123/200] complete. Average Training Loss: 0.2017 ---
--- Time taken for epoch: 314.17 seconds ---
  Epoch [124/200], Batch [47/517], Loss: 0.1495
  Epoch [124/200], Batch [94/517], Loss: 0.1351
  Epoch [124/200], Batch [141/517], Loss: 0.1193
  Epoch [124/200], Batch [188/517], Loss: 0.1470
  Epoch [124/200], Batch [235/517], Loss: 0.1504
  Epoch [124/200], Batch [282/517], Loss: 0.1747
  Epoch [124/200], Batch [329/517], Loss: 0.1206
  Epoch [124/200], Batch [376/517], Loss: 0.1451
  Epoch [124/200], Batch [423/517], Loss: 0.0984
  Epoch [124/200], Batch [470/517], Loss: 0.1167
  Epoch [124/200], Batch [517/517], Loss: 0.2139
--- Epoch [124/200] complete. Average Training Loss: 0.1986 ---
--- Time taken for epoch: 314.08 seconds ---
  Epoch [125/200], Batch [47/517], Loss: 0.3848
  Epoch [125/200], Batch [94/517], Loss: 0.3737
  Epoch [125/200], Batch [141/517], Loss: 0.2150
  Epoch [125/200], Batch [188/517], Loss: 0.1250
  Epoch [125/200], Batch [235/517], Loss: 0.1580
  Epoch [125/200], Batch [282/517], Loss: 0.0979
  Epoch [125/200], Batch [329/517], Loss: 0.4046
  Epoch [125/200], Batch [376/517], Loss: 0.1054
  Epoch [125/200], Batch [423/517], Loss: 0.1173
  Epoch [125/200], Batch [470/517], Loss: 0.1558
  Epoch [125/200], Batch [517/517], Loss: 0.2509
--- Epoch [125/200] complete. Average Training Loss: 0.1993 ---
--- Time taken for epoch: 314.17 seconds ---
 -- Updated Checkpoint: Saved model at 125 epochs.
  Epoch [126/200], Batch [47/517], Loss: 0.3936
  Epoch [126/200], Batch [94/517], Loss: 0.1217
  Epoch [126/200], Batch [141/517], Loss: 0.1449
  Epoch [126/200], Batch [188/517], Loss: 0.1351
  Epoch [126/200], Batch [235/517], Loss: 0.4150
  Epoch [126/200], Batch [282/517], Loss: 0.1152
  Epoch [126/200], Batch [329/517], Loss: 0.4339
  Epoch [126/200], Batch [376/517], Loss: 0.1009
  Epoch [126/200], Batch [423/517], Loss: 0.1556
  Epoch [126/200], Batch [470/517], Loss: 0.3938
  Epoch [126/200], Batch [517/517], Loss: 0.3819
--- Epoch [126/200] complete. Average Training Loss: 0.2043 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [127/200], Batch [47/517], Loss: 0.1482
  Epoch [127/200], Batch [94/517], Loss: 0.2281
  Epoch [127/200], Batch [141/517], Loss: 0.1805
  Epoch [127/200], Batch [188/517], Loss: 0.4027
  Epoch [127/200], Batch [235/517], Loss: 0.2050
  Epoch [127/200], Batch [282/517], Loss: 0.1392
  Epoch [127/200], Batch [329/517], Loss: 0.0885
  Epoch [127/200], Batch [376/517], Loss: 0.3988
  Epoch [127/200], Batch [423/517], Loss: 0.1137
  Epoch [127/200], Batch [470/517], Loss: 0.1648
  Epoch [127/200], Batch [517/517], Loss: 0.1775
--- Epoch [127/200] complete. Average Training Loss: 0.2070 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [128/200], Batch [47/517], Loss: 0.4055
  Epoch [128/200], Batch [94/517], Loss: 0.1015
  Epoch [128/200], Batch [141/517], Loss: 0.1324
  Epoch [128/200], Batch [188/517], Loss: 0.1398
  Epoch [128/200], Batch [235/517], Loss: 0.1389
  Epoch [128/200], Batch [282/517], Loss: 0.0953
  Epoch [128/200], Batch [329/517], Loss: 0.4092
  Epoch [128/200], Batch [376/517], Loss: 0.1602
  Epoch [128/200], Batch [423/517], Loss: 0.1692
  Epoch [128/200], Batch [470/517], Loss: 0.3931
  Epoch [128/200], Batch [517/517], Loss: 0.2324
--- Epoch [128/200] complete. Average Training Loss: 0.2164 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [129/200], Batch [47/517], Loss: 0.1828
  Epoch [129/200], Batch [94/517], Loss: 0.1041
  Epoch [129/200], Batch [141/517], Loss: 0.1463
  Epoch [129/200], Batch [188/517], Loss: 0.1787
  Epoch [129/200], Batch [235/517], Loss: 0.1548
  Epoch [129/200], Batch [282/517], Loss: 0.4054
  Epoch [129/200], Batch [329/517], Loss: 0.4157
  Epoch [129/200], Batch [376/517], Loss: 0.1390
  Epoch [129/200], Batch [423/517], Loss: 0.4056
  Epoch [129/200], Batch [470/517], Loss: 0.1515
  Epoch [129/200], Batch [517/517], Loss: 0.3973
--- Epoch [129/200] complete. Average Training Loss: 0.2331 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [130/200], Batch [47/517], Loss: 0.3065
  Epoch [130/200], Batch [94/517], Loss: 0.2380
  Epoch [130/200], Batch [141/517], Loss: 0.2104
  Epoch [130/200], Batch [188/517], Loss: 0.1303
  Epoch [130/200], Batch [235/517], Loss: 0.3812
  Epoch [130/200], Batch [282/517], Loss: 0.2358
  Epoch [130/200], Batch [329/517], Loss: 0.1402
  Epoch [130/200], Batch [376/517], Loss: 0.1718
  Epoch [130/200], Batch [423/517], Loss: 0.3099
  Epoch [130/200], Batch [470/517], Loss: 0.2520
  Epoch [130/200], Batch [517/517], Loss: 0.1537
--- Epoch [130/200] complete. Average Training Loss: 0.2218 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [131/200], Batch [47/517], Loss: 0.1973
  Epoch [131/200], Batch [94/517], Loss: 0.1778
  Epoch [131/200], Batch [141/517], Loss: 0.3920
  Epoch [131/200], Batch [188/517], Loss: 0.2338
  Epoch [131/200], Batch [235/517], Loss: 0.1775
  Epoch [131/200], Batch [282/517], Loss: 0.2011
  Epoch [131/200], Batch [329/517], Loss: 0.1452
  Epoch [131/200], Batch [376/517], Loss: 0.3743
  Epoch [131/200], Batch [423/517], Loss: 0.2204
  Epoch [131/200], Batch [470/517], Loss: 0.1456
  Epoch [131/200], Batch [517/517], Loss: 0.1905
--- Epoch [131/200] complete. Average Training Loss: 0.2095 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [132/200], Batch [47/517], Loss: 0.1060
  Epoch [132/200], Batch [94/517], Loss: 0.3683
  Epoch [132/200], Batch [141/517], Loss: 0.1150
  Epoch [132/200], Batch [188/517], Loss: 0.1597
  Epoch [132/200], Batch [235/517], Loss: 0.1331
  Epoch [132/200], Batch [282/517], Loss: 0.1440
  Epoch [132/200], Batch [329/517], Loss: 0.1599
  Epoch [132/200], Batch [376/517], Loss: 0.1763
  Epoch [132/200], Batch [423/517], Loss: 0.4048
  Epoch [132/200], Batch [470/517], Loss: 0.2882
  Epoch [132/200], Batch [517/517], Loss: 0.4032
--- Epoch [132/200] complete. Average Training Loss: 0.2018 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [133/200], Batch [47/517], Loss: 0.1888
  Epoch [133/200], Batch [94/517], Loss: 0.3908
  Epoch [133/200], Batch [141/517], Loss: 0.1439
  Epoch [133/200], Batch [188/517], Loss: 0.3794
  Epoch [133/200], Batch [235/517], Loss: 0.2312
  Epoch [133/200], Batch [282/517], Loss: 0.4069
  Epoch [133/200], Batch [329/517], Loss: 0.4023
  Epoch [133/200], Batch [376/517], Loss: 0.1915
