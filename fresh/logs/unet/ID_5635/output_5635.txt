Beginning to load data...
 -- Train dataset size: 4134 / 4593 ~ 0.9
 -- Test dataset size: 459 / 4593 ~ 0.1
 -- Created DataLoaders with batch size: 8

Starting UNet Training...
  Epoch [1/200], Batch [47/517], Loss: 0.6924
  Epoch [1/200], Batch [94/517], Loss: 0.6011
  Epoch [1/200], Batch [141/517], Loss: 0.5588
  Epoch [1/200], Batch [188/517], Loss: 0.5133
  Epoch [1/200], Batch [235/517], Loss: 0.4694
  Epoch [1/200], Batch [282/517], Loss: 0.4485
  Epoch [1/200], Batch [329/517], Loss: 0.4592
  Epoch [1/200], Batch [376/517], Loss: 0.4272
  Epoch [1/200], Batch [423/517], Loss: 0.4190
  Epoch [1/200], Batch [470/517], Loss: 0.4345
  Epoch [1/200], Batch [517/517], Loss: 0.4406
--- Epoch [1/200] complete. Average Training Loss: 0.5191 ---
--- Time taken for epoch: 314.43 seconds ---
  Epoch [2/200], Batch [47/517], Loss: 0.4277
  Epoch [2/200], Batch [94/517], Loss: 0.3553
  Epoch [2/200], Batch [141/517], Loss: 0.4314
  Epoch [2/200], Batch [188/517], Loss: 0.3953
  Epoch [2/200], Batch [235/517], Loss: 0.4275
  Epoch [2/200], Batch [282/517], Loss: 0.4967
  Epoch [2/200], Batch [329/517], Loss: 0.4024
  Epoch [2/200], Batch [376/517], Loss: 0.3827
  Epoch [2/200], Batch [423/517], Loss: 0.4194
  Epoch [2/200], Batch [470/517], Loss: 0.4240
  Epoch [2/200], Batch [517/517], Loss: 0.4366
--- Epoch [2/200] complete. Average Training Loss: 0.4041 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [3/200], Batch [47/517], Loss: 0.3516
  Epoch [3/200], Batch [94/517], Loss: 0.4490
  Epoch [3/200], Batch [141/517], Loss: 0.4207
  Epoch [3/200], Batch [188/517], Loss: 0.4016
  Epoch [3/200], Batch [235/517], Loss: 0.4056
  Epoch [3/200], Batch [282/517], Loss: 0.3971
  Epoch [3/200], Batch [329/517], Loss: 0.4451
  Epoch [3/200], Batch [376/517], Loss: 0.4201
  Epoch [3/200], Batch [423/517], Loss: 0.2676
  Epoch [3/200], Batch [470/517], Loss: 0.4291
  Epoch [3/200], Batch [517/517], Loss: 0.3474
--- Epoch [3/200] complete. Average Training Loss: 0.3776 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [4/200], Batch [47/517], Loss: 0.4392
  Epoch [4/200], Batch [94/517], Loss: 0.4100
  Epoch [4/200], Batch [141/517], Loss: 0.2568
  Epoch [4/200], Batch [188/517], Loss: 0.3955
  Epoch [4/200], Batch [235/517], Loss: 0.4397
  Epoch [4/200], Batch [282/517], Loss: 0.4252
  Epoch [4/200], Batch [329/517], Loss: 0.3253
  Epoch [4/200], Batch [376/517], Loss: 0.2518
  Epoch [4/200], Batch [423/517], Loss: 0.3915
  Epoch [4/200], Batch [470/517], Loss: 0.4011
  Epoch [4/200], Batch [517/517], Loss: 0.4356
--- Epoch [4/200] complete. Average Training Loss: 0.3715 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [5/200], Batch [47/517], Loss: 0.4141
  Epoch [5/200], Batch [94/517], Loss: 0.2656
  Epoch [5/200], Batch [141/517], Loss: 0.3980
  Epoch [5/200], Batch [188/517], Loss: 0.3227
  Epoch [5/200], Batch [235/517], Loss: 0.4133
  Epoch [5/200], Batch [282/517], Loss: 0.4257
  Epoch [5/200], Batch [329/517], Loss: 0.4309
  Epoch [5/200], Batch [376/517], Loss: 0.4747
  Epoch [5/200], Batch [423/517], Loss: 0.4070
  Epoch [5/200], Batch [470/517], Loss: 0.3142
  Epoch [5/200], Batch [517/517], Loss: 0.4778
--- Epoch [5/200] complete. Average Training Loss: 0.3610 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [6/200], Batch [47/517], Loss: 0.4263
  Epoch [6/200], Batch [94/517], Loss: 0.4265
  Epoch [6/200], Batch [141/517], Loss: 0.4273
  Epoch [6/200], Batch [188/517], Loss: 0.2679
  Epoch [6/200], Batch [235/517], Loss: 0.4147
  Epoch [6/200], Batch [282/517], Loss: 0.2844
  Epoch [6/200], Batch [329/517], Loss: 0.4067
  Epoch [6/200], Batch [376/517], Loss: 0.4085
  Epoch [6/200], Batch [423/517], Loss: 0.3919
  Epoch [6/200], Batch [470/517], Loss: 0.3678
  Epoch [6/200], Batch [517/517], Loss: 0.4708
--- Epoch [6/200] complete. Average Training Loss: 0.3534 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [7/200], Batch [47/517], Loss: 0.2470
  Epoch [7/200], Batch [94/517], Loss: 0.4149
  Epoch [7/200], Batch [141/517], Loss: 0.4161
  Epoch [7/200], Batch [188/517], Loss: 0.2512
  Epoch [7/200], Batch [235/517], Loss: 0.3049
  Epoch [7/200], Batch [282/517], Loss: 0.2032
  Epoch [7/200], Batch [329/517], Loss: 0.2657
  Epoch [7/200], Batch [376/517], Loss: 0.2694
  Epoch [7/200], Batch [423/517], Loss: 0.3283
  Epoch [7/200], Batch [470/517], Loss: 0.1851
  Epoch [7/200], Batch [517/517], Loss: 0.3700
--- Epoch [7/200] complete. Average Training Loss: 0.3465 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [8/200], Batch [47/517], Loss: 0.3297
  Epoch [8/200], Batch [94/517], Loss: 0.3172
  Epoch [8/200], Batch [141/517], Loss: 0.3867
  Epoch [8/200], Batch [188/517], Loss: 0.4051
  Epoch [8/200], Batch [235/517], Loss: 0.3171
  Epoch [8/200], Batch [282/517], Loss: 0.2520
  Epoch [8/200], Batch [329/517], Loss: 0.4111
  Epoch [8/200], Batch [376/517], Loss: 0.2308
  Epoch [8/200], Batch [423/517], Loss: 0.3290
  Epoch [8/200], Batch [470/517], Loss: 0.3489
  Epoch [8/200], Batch [517/517], Loss: 0.4161
--- Epoch [8/200] complete. Average Training Loss: 0.3396 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [9/200], Batch [47/517], Loss: 0.2483
  Epoch [9/200], Batch [94/517], Loss: 0.2741
  Epoch [9/200], Batch [141/517], Loss: 0.3685
  Epoch [9/200], Batch [188/517], Loss: 0.3179
  Epoch [9/200], Batch [235/517], Loss: 0.3089
  Epoch [9/200], Batch [282/517], Loss: 0.3637
  Epoch [9/200], Batch [329/517], Loss: 0.3720
  Epoch [9/200], Batch [376/517], Loss: 0.4298
  Epoch [9/200], Batch [423/517], Loss: 0.4018
  Epoch [9/200], Batch [470/517], Loss: 0.3004
  Epoch [9/200], Batch [517/517], Loss: 0.4579
--- Epoch [9/200] complete. Average Training Loss: 0.3348 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [10/200], Batch [47/517], Loss: 0.2840
  Epoch [10/200], Batch [94/517], Loss: 0.2563
  Epoch [10/200], Batch [141/517], Loss: 0.4093
  Epoch [10/200], Batch [188/517], Loss: 0.2722
  Epoch [10/200], Batch [235/517], Loss: 0.3464
  Epoch [10/200], Batch [282/517], Loss: 0.4251
  Epoch [10/200], Batch [329/517], Loss: 0.4353
  Epoch [10/200], Batch [376/517], Loss: 0.4862
  Epoch [10/200], Batch [423/517], Loss: 0.4127
  Epoch [10/200], Batch [470/517], Loss: 0.4242
  Epoch [10/200], Batch [517/517], Loss: 0.5003
--- Epoch [10/200] complete. Average Training Loss: 0.3380 ---
--- Time taken for epoch: 314.11 seconds ---
  Epoch [11/200], Batch [47/517], Loss: 0.4093
  Epoch [11/200], Batch [94/517], Loss: 0.3067
  Epoch [11/200], Batch [141/517], Loss: 0.1920
  Epoch [11/200], Batch [188/517], Loss: 0.3623
  Epoch [11/200], Batch [235/517], Loss: 0.4597
  Epoch [11/200], Batch [282/517], Loss: 0.3683
  Epoch [11/200], Batch [329/517], Loss: 0.3250
  Epoch [11/200], Batch [376/517], Loss: 0.3697
  Epoch [11/200], Batch [423/517], Loss: 0.2141
  Epoch [11/200], Batch [470/517], Loss: 0.3800
  Epoch [11/200], Batch [517/517], Loss: 0.4455
--- Epoch [11/200] complete. Average Training Loss: 0.3331 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [12/200], Batch [47/517], Loss: 0.3409
  Epoch [12/200], Batch [94/517], Loss: 0.3086
  Epoch [12/200], Batch [141/517], Loss: 0.1650
  Epoch [12/200], Batch [188/517], Loss: 0.3197
  Epoch [12/200], Batch [235/517], Loss: 0.4034
  Epoch [12/200], Batch [282/517], Loss: 0.3388
  Epoch [12/200], Batch [329/517], Loss: 0.2367
  Epoch [12/200], Batch [376/517], Loss: 0.4070
  Epoch [12/200], Batch [423/517], Loss: 0.1505
  Epoch [12/200], Batch [470/517], Loss: 0.4206
  Epoch [12/200], Batch [517/517], Loss: 0.1751
--- Epoch [12/200] complete. Average Training Loss: 0.3213 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [13/200], Batch [47/517], Loss: 0.4414
  Epoch [13/200], Batch [94/517], Loss: 0.3449
  Epoch [13/200], Batch [141/517], Loss: 0.1479
  Epoch [13/200], Batch [188/517], Loss: 0.1776
  Epoch [13/200], Batch [235/517], Loss: 0.2268
  Epoch [13/200], Batch [282/517], Loss: 0.2510
  Epoch [13/200], Batch [329/517], Loss: 0.2381
  Epoch [13/200], Batch [376/517], Loss: 0.2607
  Epoch [13/200], Batch [423/517], Loss: 0.4155
  Epoch [13/200], Batch [470/517], Loss: 0.4204
  Epoch [13/200], Batch [517/517], Loss: 0.4072
--- Epoch [13/200] complete. Average Training Loss: 0.3228 ---
--- Time taken for epoch: 314.03 seconds ---
  Epoch [14/200], Batch [47/517], Loss: 0.2818
  Epoch [14/200], Batch [94/517], Loss: 0.1941
  Epoch [14/200], Batch [141/517], Loss: 0.3263
  Epoch [14/200], Batch [188/517], Loss: 0.3474
  Epoch [14/200], Batch [235/517], Loss: 0.3696
  Epoch [14/200], Batch [282/517], Loss: 0.3314
  Epoch [14/200], Batch [329/517], Loss: 0.3901
  Epoch [14/200], Batch [376/517], Loss: 0.4183
  Epoch [14/200], Batch [423/517], Loss: 0.3272
  Epoch [14/200], Batch [470/517], Loss: 0.4231
  Epoch [14/200], Batch [517/517], Loss: 0.3940
--- Epoch [14/200] complete. Average Training Loss: 0.3159 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [15/200], Batch [47/517], Loss: 0.4237
  Epoch [15/200], Batch [94/517], Loss: 0.2976
  Epoch [15/200], Batch [141/517], Loss: 0.2446
  Epoch [15/200], Batch [188/517], Loss: 0.2229
  Epoch [15/200], Batch [235/517], Loss: 0.4176
  Epoch [15/200], Batch [282/517], Loss: 0.2054
  Epoch [15/200], Batch [329/517], Loss: 0.2197
  Epoch [15/200], Batch [376/517], Loss: 0.2506
  Epoch [15/200], Batch [423/517], Loss: 0.2431
  Epoch [15/200], Batch [470/517], Loss: 0.2216
  Epoch [15/200], Batch [517/517], Loss: 0.2922
--- Epoch [15/200] complete. Average Training Loss: 0.3118 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [16/200], Batch [47/517], Loss: 0.3490
  Epoch [16/200], Batch [94/517], Loss: 0.4161
  Epoch [16/200], Batch [141/517], Loss: 0.2738
  Epoch [16/200], Batch [188/517], Loss: 0.3453
  Epoch [16/200], Batch [235/517], Loss: 0.3130
  Epoch [16/200], Batch [282/517], Loss: 0.3890
  Epoch [16/200], Batch [329/517], Loss: 0.1974
  Epoch [16/200], Batch [376/517], Loss: 0.4419
  Epoch [16/200], Batch [423/517], Loss: 0.3087
  Epoch [16/200], Batch [470/517], Loss: 0.3761
  Epoch [16/200], Batch [517/517], Loss: 0.4186
--- Epoch [16/200] complete. Average Training Loss: 0.3045 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [17/200], Batch [47/517], Loss: 0.3310
  Epoch [17/200], Batch [94/517], Loss: 0.1904
  Epoch [17/200], Batch [141/517], Loss: 0.1900
  Epoch [17/200], Batch [188/517], Loss: 0.2505
  Epoch [17/200], Batch [235/517], Loss: 0.4037
  Epoch [17/200], Batch [282/517], Loss: 0.3471
  Epoch [17/200], Batch [329/517], Loss: 0.2942
  Epoch [17/200], Batch [376/517], Loss: 0.3748
  Epoch [17/200], Batch [423/517], Loss: 0.3136
  Epoch [17/200], Batch [470/517], Loss: 0.2274
  Epoch [17/200], Batch [517/517], Loss: 0.1832
--- Epoch [17/200] complete. Average Training Loss: 0.3029 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [18/200], Batch [47/517], Loss: 0.2125
  Epoch [18/200], Batch [94/517], Loss: 0.4243
  Epoch [18/200], Batch [141/517], Loss: 0.2119
  Epoch [18/200], Batch [188/517], Loss: 0.2955
  Epoch [18/200], Batch [235/517], Loss: 0.4058
  Epoch [18/200], Batch [282/517], Loss: 0.2307
  Epoch [18/200], Batch [329/517], Loss: 0.3978
  Epoch [18/200], Batch [376/517], Loss: 0.4225
  Epoch [18/200], Batch [423/517], Loss: 0.4152
  Epoch [18/200], Batch [470/517], Loss: 0.2366
  Epoch [18/200], Batch [517/517], Loss: 0.3875
--- Epoch [18/200] complete. Average Training Loss: 0.3045 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [19/200], Batch [47/517], Loss: 0.3967
  Epoch [19/200], Batch [94/517], Loss: 0.4087
  Epoch [19/200], Batch [141/517], Loss: 0.3865
  Epoch [19/200], Batch [188/517], Loss: 0.1431
  Epoch [19/200], Batch [235/517], Loss: 0.3724
  Epoch [19/200], Batch [282/517], Loss: 0.2228
  Epoch [19/200], Batch [329/517], Loss: 0.4517
  Epoch [19/200], Batch [376/517], Loss: 0.4160
  Epoch [19/200], Batch [423/517], Loss: 0.2570
  Epoch [19/200], Batch [470/517], Loss: 0.3556
  Epoch [19/200], Batch [517/517], Loss: 0.4246
--- Epoch [19/200] complete. Average Training Loss: 0.3039 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [20/200], Batch [47/517], Loss: 0.2424
  Epoch [20/200], Batch [94/517], Loss: 0.2266
  Epoch [20/200], Batch [141/517], Loss: 0.3620
  Epoch [20/200], Batch [188/517], Loss: 0.2289
  Epoch [20/200], Batch [235/517], Loss: 0.2688
  Epoch [20/200], Batch [282/517], Loss: 0.2726
  Epoch [20/200], Batch [329/517], Loss: 0.2581
  Epoch [20/200], Batch [376/517], Loss: 0.2581
  Epoch [20/200], Batch [423/517], Loss: 0.3881
  Epoch [20/200], Batch [470/517], Loss: 0.2481
  Epoch [20/200], Batch [517/517], Loss: 0.3182
--- Epoch [20/200] complete. Average Training Loss: 0.2998 ---
--- Time taken for epoch: 314.20 seconds ---
  Epoch [21/200], Batch [47/517], Loss: 0.3654
  Epoch [21/200], Batch [94/517], Loss: 0.2854
  Epoch [21/200], Batch [141/517], Loss: 0.2749
  Epoch [21/200], Batch [188/517], Loss: 0.3950
  Epoch [21/200], Batch [235/517], Loss: 0.1353
  Epoch [21/200], Batch [282/517], Loss: 0.2957
  Epoch [21/200], Batch [329/517], Loss: 0.2667
  Epoch [21/200], Batch [376/517], Loss: 0.2649
  Epoch [21/200], Batch [423/517], Loss: 0.4104
  Epoch [21/200], Batch [470/517], Loss: 0.4257
  Epoch [21/200], Batch [517/517], Loss: 0.2976
--- Epoch [21/200] complete. Average Training Loss: 0.2971 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [22/200], Batch [47/517], Loss: 0.2886
  Epoch [22/200], Batch [94/517], Loss: 0.2814
  Epoch [22/200], Batch [141/517], Loss: 0.1717
  Epoch [22/200], Batch [188/517], Loss: 0.4046
  Epoch [22/200], Batch [235/517], Loss: 0.4468
  Epoch [22/200], Batch [282/517], Loss: 0.2841
  Epoch [22/200], Batch [329/517], Loss: 0.3927
  Epoch [22/200], Batch [376/517], Loss: 0.4377
  Epoch [22/200], Batch [423/517], Loss: 0.1688
  Epoch [22/200], Batch [470/517], Loss: 0.2659
  Epoch [22/200], Batch [517/517], Loss: 0.1468
--- Epoch [22/200] complete. Average Training Loss: 0.2885 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [23/200], Batch [47/517], Loss: 0.3222
  Epoch [23/200], Batch [94/517], Loss: 0.3971
  Epoch [23/200], Batch [141/517], Loss: 0.3929
  Epoch [23/200], Batch [188/517], Loss: 0.4043
  Epoch [23/200], Batch [235/517], Loss: 0.2556
  Epoch [23/200], Batch [282/517], Loss: 0.2728
  Epoch [23/200], Batch [329/517], Loss: 0.2634
  Epoch [23/200], Batch [376/517], Loss: 0.1253
  Epoch [23/200], Batch [423/517], Loss: 0.3854
  Epoch [23/200], Batch [470/517], Loss: 0.1710
  Epoch [23/200], Batch [517/517], Loss: 0.4013
--- Epoch [23/200] complete. Average Training Loss: 0.2872 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [24/200], Batch [47/517], Loss: 0.4061
  Epoch [24/200], Batch [94/517], Loss: 0.3413
  Epoch [24/200], Batch [141/517], Loss: 0.4038
  Epoch [24/200], Batch [188/517], Loss: 0.1782
  Epoch [24/200], Batch [235/517], Loss: 0.3935
  Epoch [24/200], Batch [282/517], Loss: 0.3857
  Epoch [24/200], Batch [329/517], Loss: 0.2818
  Epoch [24/200], Batch [376/517], Loss: 0.3277
  Epoch [24/200], Batch [423/517], Loss: 0.2732
  Epoch [24/200], Batch [470/517], Loss: 0.1424
  Epoch [24/200], Batch [517/517], Loss: 0.3522
--- Epoch [24/200] complete. Average Training Loss: 0.2863 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [25/200], Batch [47/517], Loss: 0.1950
  Epoch [25/200], Batch [94/517], Loss: 0.1644
  Epoch [25/200], Batch [141/517], Loss: 0.2982
  Epoch [25/200], Batch [188/517], Loss: 0.2963
  Epoch [25/200], Batch [235/517], Loss: 0.3555
  Epoch [25/200], Batch [282/517], Loss: 0.3590
  Epoch [25/200], Batch [329/517], Loss: 0.4120
  Epoch [25/200], Batch [376/517], Loss: 0.4141
  Epoch [25/200], Batch [423/517], Loss: 0.1069
  Epoch [25/200], Batch [470/517], Loss: 0.2269
  Epoch [25/200], Batch [517/517], Loss: 0.2759
--- Epoch [25/200] complete. Average Training Loss: 0.2873 ---
--- Time taken for epoch: 314.10 seconds ---
 -- Updated Checkpoint: Saved model at 25 epochs.
  Epoch [26/200], Batch [47/517], Loss: 0.1498
  Epoch [26/200], Batch [94/517], Loss: 0.3997
  Epoch [26/200], Batch [141/517], Loss: 0.2919
  Epoch [26/200], Batch [188/517], Loss: 0.3310
  Epoch [26/200], Batch [235/517], Loss: 0.2324
  Epoch [26/200], Batch [282/517], Loss: 0.3885
  Epoch [26/200], Batch [329/517], Loss: 0.2848
  Epoch [26/200], Batch [376/517], Loss: 0.2181
  Epoch [26/200], Batch [423/517], Loss: 0.3051
  Epoch [26/200], Batch [470/517], Loss: 0.3911
  Epoch [26/200], Batch [517/517], Loss: 0.4211
--- Epoch [26/200] complete. Average Training Loss: 0.2839 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [27/200], Batch [47/517], Loss: 0.3338
  Epoch [27/200], Batch [94/517], Loss: 0.3475
  Epoch [27/200], Batch [141/517], Loss: 0.4000
  Epoch [27/200], Batch [188/517], Loss: 0.2877
  Epoch [27/200], Batch [235/517], Loss: 0.3520
  Epoch [27/200], Batch [282/517], Loss: 0.1799
  Epoch [27/200], Batch [329/517], Loss: 0.3968
  Epoch [27/200], Batch [376/517], Loss: 0.2422
  Epoch [27/200], Batch [423/517], Loss: 0.2615
  Epoch [27/200], Batch [470/517], Loss: 0.3042
  Epoch [27/200], Batch [517/517], Loss: 0.4129
--- Epoch [27/200] complete. Average Training Loss: 0.2734 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [28/200], Batch [47/517], Loss: 0.3752
  Epoch [28/200], Batch [94/517], Loss: 0.3078
  Epoch [28/200], Batch [141/517], Loss: 0.4397
  Epoch [28/200], Batch [188/517], Loss: 0.2416
  Epoch [28/200], Batch [235/517], Loss: 0.2797
  Epoch [28/200], Batch [282/517], Loss: 0.2311
  Epoch [28/200], Batch [329/517], Loss: 0.1454
  Epoch [28/200], Batch [376/517], Loss: 0.3309
  Epoch [28/200], Batch [423/517], Loss: 0.4023
  Epoch [28/200], Batch [470/517], Loss: 0.2581
  Epoch [28/200], Batch [517/517], Loss: 0.4239
--- Epoch [28/200] complete. Average Training Loss: 0.2812 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [29/200], Batch [47/517], Loss: 0.1807
  Epoch [29/200], Batch [94/517], Loss: 0.2720
  Epoch [29/200], Batch [141/517], Loss: 0.1762
  Epoch [29/200], Batch [188/517], Loss: 0.2159
  Epoch [29/200], Batch [235/517], Loss: 0.1663
  Epoch [29/200], Batch [282/517], Loss: 0.2645
  Epoch [29/200], Batch [329/517], Loss: 0.1644
  Epoch [29/200], Batch [376/517], Loss: 0.2306
  Epoch [29/200], Batch [423/517], Loss: 0.2606
  Epoch [29/200], Batch [470/517], Loss: 0.4234
  Epoch [29/200], Batch [517/517], Loss: 0.4206
--- Epoch [29/200] complete. Average Training Loss: 0.2749 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [30/200], Batch [47/517], Loss: 0.3731
  Epoch [30/200], Batch [94/517], Loss: 0.4308
  Epoch [30/200], Batch [141/517], Loss: 0.2711
  Epoch [30/200], Batch [188/517], Loss: 0.4159
  Epoch [30/200], Batch [235/517], Loss: 0.4169
  Epoch [30/200], Batch [282/517], Loss: 0.1775
  Epoch [30/200], Batch [329/517], Loss: 0.2160
  Epoch [30/200], Batch [376/517], Loss: 0.2911
  Epoch [30/200], Batch [423/517], Loss: 0.1648
  Epoch [30/200], Batch [470/517], Loss: 0.1771
  Epoch [30/200], Batch [517/517], Loss: 0.3050
--- Epoch [30/200] complete. Average Training Loss: 0.2724 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [31/200], Batch [47/517], Loss: 0.3592
  Epoch [31/200], Batch [94/517], Loss: 0.3548
  Epoch [31/200], Batch [141/517], Loss: 0.2539
  Epoch [31/200], Batch [188/517], Loss: 0.1992
  Epoch [31/200], Batch [235/517], Loss: 0.3398
  Epoch [31/200], Batch [282/517], Loss: 0.1745
  Epoch [31/200], Batch [329/517], Loss: 0.3773
  Epoch [31/200], Batch [376/517], Loss: 0.4019
  Epoch [31/200], Batch [423/517], Loss: 0.2449
  Epoch [31/200], Batch [470/517], Loss: 0.3299
  Epoch [31/200], Batch [517/517], Loss: 0.2424
--- Epoch [31/200] complete. Average Training Loss: 0.2702 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [32/200], Batch [47/517], Loss: 0.3929
  Epoch [32/200], Batch [94/517], Loss: 0.3002
  Epoch [32/200], Batch [141/517], Loss: 0.4131
  Epoch [32/200], Batch [188/517], Loss: 0.2869
  Epoch [32/200], Batch [235/517], Loss: 0.2864
  Epoch [32/200], Batch [282/517], Loss: 0.2444
  Epoch [32/200], Batch [329/517], Loss: 0.3128
  Epoch [32/200], Batch [376/517], Loss: 0.1737
  Epoch [32/200], Batch [423/517], Loss: 0.2610
  Epoch [32/200], Batch [470/517], Loss: 0.3129
  Epoch [32/200], Batch [517/517], Loss: 0.3267
--- Epoch [32/200] complete. Average Training Loss: 0.2688 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [33/200], Batch [47/517], Loss: 0.1830
  Epoch [33/200], Batch [94/517], Loss: 0.2535
  Epoch [33/200], Batch [141/517], Loss: 0.1590
  Epoch [33/200], Batch [188/517], Loss: 0.2293
  Epoch [33/200], Batch [235/517], Loss: 0.3914
  Epoch [33/200], Batch [282/517], Loss: 0.1982
  Epoch [33/200], Batch [329/517], Loss: 0.3346
  Epoch [33/200], Batch [376/517], Loss: 0.3992
  Epoch [33/200], Batch [423/517], Loss: 0.1734
  Epoch [33/200], Batch [470/517], Loss: 0.2018
  Epoch [33/200], Batch [517/517], Loss: 0.1768
--- Epoch [33/200] complete. Average Training Loss: 0.2648 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [34/200], Batch [47/517], Loss: 0.2941
  Epoch [34/200], Batch [94/517], Loss: 0.4081
  Epoch [34/200], Batch [141/517], Loss: 0.3276
  Epoch [34/200], Batch [188/517], Loss: 0.3515
  Epoch [34/200], Batch [235/517], Loss: 0.2603
  Epoch [34/200], Batch [282/517], Loss: 0.2226
  Epoch [34/200], Batch [329/517], Loss: 0.1361
  Epoch [34/200], Batch [376/517], Loss: 0.3876
  Epoch [34/200], Batch [423/517], Loss: 0.4057
  Epoch [34/200], Batch [470/517], Loss: 0.2547
  Epoch [34/200], Batch [517/517], Loss: 0.1882
--- Epoch [34/200] complete. Average Training Loss: 0.2669 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [35/200], Batch [47/517], Loss: 0.4167
  Epoch [35/200], Batch [94/517], Loss: 0.1465
  Epoch [35/200], Batch [141/517], Loss: 0.4041
  Epoch [35/200], Batch [188/517], Loss: 0.4109
  Epoch [35/200], Batch [235/517], Loss: 0.1715
  Epoch [35/200], Batch [282/517], Loss: 0.3749
  Epoch [35/200], Batch [329/517], Loss: 0.3492
  Epoch [35/200], Batch [376/517], Loss: 0.3852
  Epoch [35/200], Batch [423/517], Loss: 0.4273
  Epoch [35/200], Batch [470/517], Loss: 0.1801
  Epoch [35/200], Batch [517/517], Loss: 0.1217
--- Epoch [35/200] complete. Average Training Loss: 0.2696 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [36/200], Batch [47/517], Loss: 0.4041
  Epoch [36/200], Batch [94/517], Loss: 0.3162
  Epoch [36/200], Batch [141/517], Loss: 0.4096
  Epoch [36/200], Batch [188/517], Loss: 0.2399
  Epoch [36/200], Batch [235/517], Loss: 0.3392
  Epoch [36/200], Batch [282/517], Loss: 0.2126
  Epoch [36/200], Batch [329/517], Loss: 0.3142
  Epoch [36/200], Batch [376/517], Loss: 0.2612
  Epoch [36/200], Batch [423/517], Loss: 0.3866
  Epoch [36/200], Batch [470/517], Loss: 0.1535
  Epoch [36/200], Batch [517/517], Loss: 0.1934
--- Epoch [36/200] complete. Average Training Loss: 0.2622 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [37/200], Batch [47/517], Loss: 0.1803
  Epoch [37/200], Batch [94/517], Loss: 0.3850
  Epoch [37/200], Batch [141/517], Loss: 0.1852
  Epoch [37/200], Batch [188/517], Loss: 0.1773
  Epoch [37/200], Batch [235/517], Loss: 0.2955
  Epoch [37/200], Batch [282/517], Loss: 0.4153
  Epoch [37/200], Batch [329/517], Loss: 0.2122
  Epoch [37/200], Batch [376/517], Loss: 0.3521
  Epoch [37/200], Batch [423/517], Loss: 0.2397
  Epoch [37/200], Batch [470/517], Loss: 0.2553
  Epoch [37/200], Batch [517/517], Loss: 0.1836
--- Epoch [37/200] complete. Average Training Loss: 0.2618 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [38/200], Batch [47/517], Loss: 0.1760
  Epoch [38/200], Batch [94/517], Loss: 0.2704
  Epoch [38/200], Batch [141/517], Loss: 0.2583
  Epoch [38/200], Batch [188/517], Loss: 0.1964
  Epoch [38/200], Batch [235/517], Loss: 0.3849
  Epoch [38/200], Batch [282/517], Loss: 0.3126
  Epoch [38/200], Batch [329/517], Loss: 0.1539
  Epoch [38/200], Batch [376/517], Loss: 0.4161
  Epoch [38/200], Batch [423/517], Loss: 0.3086
  Epoch [38/200], Batch [470/517], Loss: 0.1368
  Epoch [38/200], Batch [517/517], Loss: 0.1443
--- Epoch [38/200] complete. Average Training Loss: 0.2555 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [39/200], Batch [47/517], Loss: 0.2444
  Epoch [39/200], Batch [94/517], Loss: 0.1508
  Epoch [39/200], Batch [141/517], Loss: 0.1694
  Epoch [39/200], Batch [188/517], Loss: 0.3516
  Epoch [39/200], Batch [235/517], Loss: 0.1478
  Epoch [39/200], Batch [282/517], Loss: 0.0940
  Epoch [39/200], Batch [329/517], Loss: 0.2459
  Epoch [39/200], Batch [376/517], Loss: 0.2037
  Epoch [39/200], Batch [423/517], Loss: 0.4595
  Epoch [39/200], Batch [470/517], Loss: 0.1824
  Epoch [39/200], Batch [517/517], Loss: 0.2157
--- Epoch [39/200] complete. Average Training Loss: 0.2573 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [40/200], Batch [47/517], Loss: 0.2175
  Epoch [40/200], Batch [94/517], Loss: 0.3764
  Epoch [40/200], Batch [141/517], Loss: 0.1445
  Epoch [40/200], Batch [188/517], Loss: 0.2057
  Epoch [40/200], Batch [235/517], Loss: 0.4073
  Epoch [40/200], Batch [282/517], Loss: 0.4078
  Epoch [40/200], Batch [329/517], Loss: 0.1367
  Epoch [40/200], Batch [376/517], Loss: 0.1546
  Epoch [40/200], Batch [423/517], Loss: 0.1600
  Epoch [40/200], Batch [470/517], Loss: 0.2474
  Epoch [40/200], Batch [517/517], Loss: 0.1893
--- Epoch [40/200] complete. Average Training Loss: 0.2546 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [41/200], Batch [47/517], Loss: 0.1813
  Epoch [41/200], Batch [94/517], Loss: 0.2887
  Epoch [41/200], Batch [141/517], Loss: 0.2594
  Epoch [41/200], Batch [188/517], Loss: 0.1668
  Epoch [41/200], Batch [235/517], Loss: 0.1929
  Epoch [41/200], Batch [282/517], Loss: 0.1755
  Epoch [41/200], Batch [329/517], Loss: 0.2904
  Epoch [41/200], Batch [376/517], Loss: 0.2696
  Epoch [41/200], Batch [423/517], Loss: 0.1599
  Epoch [41/200], Batch [470/517], Loss: 0.1343
  Epoch [41/200], Batch [517/517], Loss: 0.3176
--- Epoch [41/200] complete. Average Training Loss: 0.2527 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [42/200], Batch [47/517], Loss: 0.1324
  Epoch [42/200], Batch [94/517], Loss: 0.2669
  Epoch [42/200], Batch [141/517], Loss: 0.2056
  Epoch [42/200], Batch [188/517], Loss: 0.4129
  Epoch [42/200], Batch [235/517], Loss: 0.3891
  Epoch [42/200], Batch [282/517], Loss: 0.1781
  Epoch [42/200], Batch [329/517], Loss: 0.2988
  Epoch [42/200], Batch [376/517], Loss: 0.1187
  Epoch [42/200], Batch [423/517], Loss: 0.1640
  Epoch [42/200], Batch [470/517], Loss: 0.2180
  Epoch [42/200], Batch [517/517], Loss: 0.3720
--- Epoch [42/200] complete. Average Training Loss: 0.2506 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [43/200], Batch [47/517], Loss: 0.1799
  Epoch [43/200], Batch [94/517], Loss: 0.1903
  Epoch [43/200], Batch [141/517], Loss: 0.4016
  Epoch [43/200], Batch [188/517], Loss: 0.1202
  Epoch [43/200], Batch [235/517], Loss: 0.1524
  Epoch [43/200], Batch [282/517], Loss: 0.2342
  Epoch [43/200], Batch [329/517], Loss: 0.2193
  Epoch [43/200], Batch [376/517], Loss: 0.1582
  Epoch [43/200], Batch [423/517], Loss: 0.1577
  Epoch [43/200], Batch [470/517], Loss: 0.2581
  Epoch [43/200], Batch [517/517], Loss: 0.1882
--- Epoch [43/200] complete. Average Training Loss: 0.2473 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [44/200], Batch [47/517], Loss: 0.2020
  Epoch [44/200], Batch [94/517], Loss: 0.0874
  Epoch [44/200], Batch [141/517], Loss: 0.1160
  Epoch [44/200], Batch [188/517], Loss: 0.3779
  Epoch [44/200], Batch [235/517], Loss: 0.4004
  Epoch [44/200], Batch [282/517], Loss: 0.2302
  Epoch [44/200], Batch [329/517], Loss: 0.4390
  Epoch [44/200], Batch [376/517], Loss: 0.2088
  Epoch [44/200], Batch [423/517], Loss: 0.2386
  Epoch [44/200], Batch [470/517], Loss: 0.2023
  Epoch [44/200], Batch [517/517], Loss: 0.1664
--- Epoch [44/200] complete. Average Training Loss: 0.2512 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [45/200], Batch [47/517], Loss: 0.1530
  Epoch [45/200], Batch [94/517], Loss: 0.1704
  Epoch [45/200], Batch [141/517], Loss: 0.3884
  Epoch [45/200], Batch [188/517], Loss: 0.2346
  Epoch [45/200], Batch [235/517], Loss: 0.2407
  Epoch [45/200], Batch [282/517], Loss: 0.2175
  Epoch [45/200], Batch [329/517], Loss: 0.2056
  Epoch [45/200], Batch [376/517], Loss: 0.3161
  Epoch [45/200], Batch [423/517], Loss: 0.1431
  Epoch [45/200], Batch [470/517], Loss: 0.2089
  Epoch [45/200], Batch [517/517], Loss: 0.1105
--- Epoch [45/200] complete. Average Training Loss: 0.2512 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [46/200], Batch [47/517], Loss: 0.3747
  Epoch [46/200], Batch [94/517], Loss: 0.1867
  Epoch [46/200], Batch [141/517], Loss: 0.2021
  Epoch [46/200], Batch [188/517], Loss: 0.1327
  Epoch [46/200], Batch [235/517], Loss: 0.4034
  Epoch [46/200], Batch [282/517], Loss: 0.1400
  Epoch [46/200], Batch [329/517], Loss: 0.2889
  Epoch [46/200], Batch [376/517], Loss: 0.4521
  Epoch [46/200], Batch [423/517], Loss: 0.0975
  Epoch [46/200], Batch [470/517], Loss: 0.1840
  Epoch [46/200], Batch [517/517], Loss: 0.4014
--- Epoch [46/200] complete. Average Training Loss: 0.2460 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [47/200], Batch [47/517], Loss: 0.3115
  Epoch [47/200], Batch [94/517], Loss: 0.2379
  Epoch [47/200], Batch [141/517], Loss: 0.2535
  Epoch [47/200], Batch [188/517], Loss: 0.1345
  Epoch [47/200], Batch [235/517], Loss: 0.1845
  Epoch [47/200], Batch [282/517], Loss: 0.1991
  Epoch [47/200], Batch [329/517], Loss: 0.3944
  Epoch [47/200], Batch [376/517], Loss: 0.4046
  Epoch [47/200], Batch [423/517], Loss: 0.1249
  Epoch [47/200], Batch [470/517], Loss: 0.2239
  Epoch [47/200], Batch [517/517], Loss: 0.1939
--- Epoch [47/200] complete. Average Training Loss: 0.2522 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [48/200], Batch [47/517], Loss: 0.4497
  Epoch [48/200], Batch [94/517], Loss: 0.2749
  Epoch [48/200], Batch [141/517], Loss: 0.3088
  Epoch [48/200], Batch [188/517], Loss: 0.4048
  Epoch [48/200], Batch [235/517], Loss: 0.1654
  Epoch [48/200], Batch [282/517], Loss: 0.1623
  Epoch [48/200], Batch [329/517], Loss: 0.2299
  Epoch [48/200], Batch [376/517], Loss: 0.3446
  Epoch [48/200], Batch [423/517], Loss: 0.1441
  Epoch [48/200], Batch [470/517], Loss: 0.3080
  Epoch [48/200], Batch [517/517], Loss: 0.4444
--- Epoch [48/200] complete. Average Training Loss: 0.2517 ---
--- Time taken for epoch: 314.07 seconds ---
  Epoch [49/200], Batch [47/517], Loss: 0.4146
  Epoch [49/200], Batch [94/517], Loss: 0.2108
  Epoch [49/200], Batch [141/517], Loss: 0.2175
  Epoch [49/200], Batch [188/517], Loss: 0.2042
  Epoch [49/200], Batch [235/517], Loss: 0.1539
  Epoch [49/200], Batch [282/517], Loss: 0.2370
  Epoch [49/200], Batch [329/517], Loss: 0.2937
  Epoch [49/200], Batch [376/517], Loss: 0.1367
  Epoch [49/200], Batch [423/517], Loss: 0.2278
  Epoch [49/200], Batch [470/517], Loss: 0.2072
  Epoch [49/200], Batch [517/517], Loss: 0.2286
--- Epoch [49/200] complete. Average Training Loss: 0.2473 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [50/200], Batch [47/517], Loss: 0.4131
  Epoch [50/200], Batch [94/517], Loss: 0.1949
  Epoch [50/200], Batch [141/517], Loss: 0.1501
  Epoch [50/200], Batch [188/517], Loss: 0.1775
  Epoch [50/200], Batch [235/517], Loss: 0.2186
  Epoch [50/200], Batch [282/517], Loss: 0.4240
  Epoch [50/200], Batch [329/517], Loss: 0.1730
  Epoch [50/200], Batch [376/517], Loss: 0.1665
  Epoch [50/200], Batch [423/517], Loss: 0.2283
  Epoch [50/200], Batch [470/517], Loss: 0.1880
  Epoch [50/200], Batch [517/517], Loss: 0.4028
--- Epoch [50/200] complete. Average Training Loss: 0.2382 ---
--- Time taken for epoch: 313.80 seconds ---
 -- Updated Checkpoint: Saved model at 50 epochs.
  Epoch [51/200], Batch [47/517], Loss: 0.1338
  Epoch [51/200], Batch [94/517], Loss: 0.1644
  Epoch [51/200], Batch [141/517], Loss: 0.2735
  Epoch [51/200], Batch [188/517], Loss: 0.1789
  Epoch [51/200], Batch [235/517], Loss: 0.2075
  Epoch [51/200], Batch [282/517], Loss: 0.1411
  Epoch [51/200], Batch [329/517], Loss: 0.2343
  Epoch [51/200], Batch [376/517], Loss: 0.1401
  Epoch [51/200], Batch [423/517], Loss: 0.0913
  Epoch [51/200], Batch [470/517], Loss: 0.1623
  Epoch [51/200], Batch [517/517], Loss: 0.3889
--- Epoch [51/200] complete. Average Training Loss: 0.2371 ---
--- Time taken for epoch: 313.76 seconds ---
  Epoch [52/200], Batch [47/517], Loss: 0.1294
  Epoch [52/200], Batch [94/517], Loss: 0.4020
  Epoch [52/200], Batch [141/517], Loss: 0.2421
  Epoch [52/200], Batch [188/517], Loss: 0.1499
  Epoch [52/200], Batch [235/517], Loss: 0.1983
  Epoch [52/200], Batch [282/517], Loss: 0.3035
  Epoch [52/200], Batch [329/517], Loss: 0.3404
  Epoch [52/200], Batch [376/517], Loss: 0.1666
  Epoch [52/200], Batch [423/517], Loss: 0.2784
  Epoch [52/200], Batch [470/517], Loss: 0.2136
  Epoch [52/200], Batch [517/517], Loss: 0.1568
--- Epoch [52/200] complete. Average Training Loss: 0.2409 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [53/200], Batch [47/517], Loss: 0.1518
  Epoch [53/200], Batch [94/517], Loss: 0.1883
  Epoch [53/200], Batch [141/517], Loss: 0.1819
  Epoch [53/200], Batch [188/517], Loss: 0.1297
  Epoch [53/200], Batch [235/517], Loss: 0.3451
  Epoch [53/200], Batch [282/517], Loss: 0.1786
  Epoch [53/200], Batch [329/517], Loss: 0.2546
  Epoch [53/200], Batch [376/517], Loss: 0.1334
  Epoch [53/200], Batch [423/517], Loss: 0.2704
  Epoch [53/200], Batch [470/517], Loss: 0.1272
  Epoch [53/200], Batch [517/517], Loss: 0.4386
--- Epoch [53/200] complete. Average Training Loss: 0.2491 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [54/200], Batch [47/517], Loss: 0.4290
  Epoch [54/200], Batch [94/517], Loss: 0.1636
  Epoch [54/200], Batch [141/517], Loss: 0.2067
  Epoch [54/200], Batch [188/517], Loss: 0.2960
  Epoch [54/200], Batch [235/517], Loss: 0.2848
  Epoch [54/200], Batch [282/517], Loss: 0.1417
  Epoch [54/200], Batch [329/517], Loss: 0.1953
  Epoch [54/200], Batch [376/517], Loss: 0.3943
  Epoch [54/200], Batch [423/517], Loss: 0.4048
  Epoch [54/200], Batch [470/517], Loss: 0.2594
  Epoch [54/200], Batch [517/517], Loss: 0.3873
--- Epoch [54/200] complete. Average Training Loss: 0.2364 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [55/200], Batch [47/517], Loss: 0.3966
  Epoch [55/200], Batch [94/517], Loss: 0.1491
  Epoch [55/200], Batch [141/517], Loss: 0.1540
  Epoch [55/200], Batch [188/517], Loss: 0.1517
  Epoch [55/200], Batch [235/517], Loss: 0.2712
  Epoch [55/200], Batch [282/517], Loss: 0.1000
  Epoch [55/200], Batch [329/517], Loss: 0.3962
  Epoch [55/200], Batch [376/517], Loss: 0.1692
  Epoch [55/200], Batch [423/517], Loss: 0.1814
  Epoch [55/200], Batch [470/517], Loss: 0.1782
  Epoch [55/200], Batch [517/517], Loss: 0.1341
--- Epoch [55/200] complete. Average Training Loss: 0.2361 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [56/200], Batch [47/517], Loss: 0.3898
  Epoch [56/200], Batch [94/517], Loss: 0.2222
  Epoch [56/200], Batch [141/517], Loss: 0.1383
  Epoch [56/200], Batch [188/517], Loss: 0.2876
  Epoch [56/200], Batch [235/517], Loss: 0.1040
  Epoch [56/200], Batch [282/517], Loss: 0.1651
  Epoch [56/200], Batch [329/517], Loss: 0.3264
  Epoch [56/200], Batch [376/517], Loss: 0.4350
  Epoch [56/200], Batch [423/517], Loss: 0.2732
  Epoch [56/200], Batch [470/517], Loss: 0.0994
  Epoch [56/200], Batch [517/517], Loss: 0.1469
--- Epoch [56/200] complete. Average Training Loss: 0.2317 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [57/200], Batch [47/517], Loss: 0.2070
  Epoch [57/200], Batch [94/517], Loss: 0.2158
  Epoch [57/200], Batch [141/517], Loss: 0.1492
  Epoch [57/200], Batch [188/517], Loss: 0.1245
  Epoch [57/200], Batch [235/517], Loss: 0.1872
  Epoch [57/200], Batch [282/517], Loss: 0.2229
  Epoch [57/200], Batch [329/517], Loss: 0.3632
  Epoch [57/200], Batch [376/517], Loss: 0.1192
  Epoch [57/200], Batch [423/517], Loss: 0.2593
  Epoch [57/200], Batch [470/517], Loss: 0.1801
  Epoch [57/200], Batch [517/517], Loss: 0.1124
--- Epoch [57/200] complete. Average Training Loss: 0.2332 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [58/200], Batch [47/517], Loss: 0.3857
  Epoch [58/200], Batch [94/517], Loss: 0.3937
  Epoch [58/200], Batch [141/517], Loss: 0.3888
  Epoch [58/200], Batch [188/517], Loss: 0.4192
  Epoch [58/200], Batch [235/517], Loss: 0.1064
  Epoch [58/200], Batch [282/517], Loss: 0.1723
  Epoch [58/200], Batch [329/517], Loss: 0.1925
  Epoch [58/200], Batch [376/517], Loss: 0.3831
  Epoch [58/200], Batch [423/517], Loss: 0.1627
  Epoch [58/200], Batch [470/517], Loss: 0.2694
  Epoch [58/200], Batch [517/517], Loss: 0.3874
--- Epoch [58/200] complete. Average Training Loss: 0.2339 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [59/200], Batch [47/517], Loss: 0.1835
  Epoch [59/200], Batch [94/517], Loss: 0.1652
  Epoch [59/200], Batch [141/517], Loss: 0.2528
  Epoch [59/200], Batch [188/517], Loss: 0.1233
  Epoch [59/200], Batch [235/517], Loss: 0.1734
  Epoch [59/200], Batch [282/517], Loss: 0.1790
  Epoch [59/200], Batch [329/517], Loss: 0.2307
  Epoch [59/200], Batch [376/517], Loss: 0.2901
  Epoch [59/200], Batch [423/517], Loss: 0.1147
  Epoch [59/200], Batch [470/517], Loss: 0.1083
  Epoch [59/200], Batch [517/517], Loss: 0.2277
--- Epoch [59/200] complete. Average Training Loss: 0.2435 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [60/200], Batch [47/517], Loss: 0.1907
  Epoch [60/200], Batch [94/517], Loss: 0.1320
  Epoch [60/200], Batch [141/517], Loss: 0.2152
  Epoch [60/200], Batch [188/517], Loss: 0.1598
  Epoch [60/200], Batch [235/517], Loss: 0.2225
  Epoch [60/200], Batch [282/517], Loss: 0.2764
  Epoch [60/200], Batch [329/517], Loss: 0.2525
  Epoch [60/200], Batch [376/517], Loss: 0.2279
  Epoch [60/200], Batch [423/517], Loss: 0.1297
  Epoch [60/200], Batch [470/517], Loss: 0.1242
  Epoch [60/200], Batch [517/517], Loss: 0.1235
--- Epoch [60/200] complete. Average Training Loss: 0.2390 ---
--- Time taken for epoch: 313.77 seconds ---
  Epoch [61/200], Batch [47/517], Loss: 0.1933
  Epoch [61/200], Batch [94/517], Loss: 0.4069
  Epoch [61/200], Batch [141/517], Loss: 0.2019
  Epoch [61/200], Batch [188/517], Loss: 0.1965
  Epoch [61/200], Batch [235/517], Loss: 0.1891
  Epoch [61/200], Batch [282/517], Loss: 0.1907
  Epoch [61/200], Batch [329/517], Loss: 0.1160
  Epoch [61/200], Batch [376/517], Loss: 0.0910
  Epoch [61/200], Batch [423/517], Loss: 0.0861
  Epoch [61/200], Batch [470/517], Loss: 0.0981
  Epoch [61/200], Batch [517/517], Loss: 0.2073
--- Epoch [61/200] complete. Average Training Loss: 0.2331 ---
--- Time taken for epoch: 313.73 seconds ---
  Epoch [62/200], Batch [47/517], Loss: 0.1878
  Epoch [62/200], Batch [94/517], Loss: 0.2900
  Epoch [62/200], Batch [141/517], Loss: 0.2514
  Epoch [62/200], Batch [188/517], Loss: 0.1508
  Epoch [62/200], Batch [235/517], Loss: 0.1726
  Epoch [62/200], Batch [282/517], Loss: 0.2142
  Epoch [62/200], Batch [329/517], Loss: 0.2428
  Epoch [62/200], Batch [376/517], Loss: 0.3959
  Epoch [62/200], Batch [423/517], Loss: 0.3336
  Epoch [62/200], Batch [470/517], Loss: 0.1851
  Epoch [62/200], Batch [517/517], Loss: 0.1269
--- Epoch [62/200] complete. Average Training Loss: 0.2302 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [63/200], Batch [47/517], Loss: 0.2133
  Epoch [63/200], Batch [94/517], Loss: 0.1628
  Epoch [63/200], Batch [141/517], Loss: 0.1543
  Epoch [63/200], Batch [188/517], Loss: 0.1728
  Epoch [63/200], Batch [235/517], Loss: 0.1612
  Epoch [63/200], Batch [282/517], Loss: 0.2291
  Epoch [63/200], Batch [329/517], Loss: 0.1536
  Epoch [63/200], Batch [376/517], Loss: 0.1103
  Epoch [63/200], Batch [423/517], Loss: 0.3711
  Epoch [63/200], Batch [470/517], Loss: 0.3724
  Epoch [63/200], Batch [517/517], Loss: 0.1591
--- Epoch [63/200] complete. Average Training Loss: 0.2316 ---
--- Time taken for epoch: 313.79 seconds ---
  Epoch [64/200], Batch [47/517], Loss: 0.4211
  Epoch [64/200], Batch [94/517], Loss: 0.1404
  Epoch [64/200], Batch [141/517], Loss: 0.1778
  Epoch [64/200], Batch [188/517], Loss: 0.3883
  Epoch [64/200], Batch [235/517], Loss: 0.2141
  Epoch [64/200], Batch [282/517], Loss: 0.1329
  Epoch [64/200], Batch [329/517], Loss: 0.2090
  Epoch [64/200], Batch [376/517], Loss: 0.4032
  Epoch [64/200], Batch [423/517], Loss: 0.1453
  Epoch [64/200], Batch [470/517], Loss: 0.1606
  Epoch [64/200], Batch [517/517], Loss: 0.4298
--- Epoch [64/200] complete. Average Training Loss: 0.2363 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [65/200], Batch [47/517], Loss: 0.2528
  Epoch [65/200], Batch [94/517], Loss: 0.1469
  Epoch [65/200], Batch [141/517], Loss: 0.1358
  Epoch [65/200], Batch [188/517], Loss: 0.2353
  Epoch [65/200], Batch [235/517], Loss: 0.2079
  Epoch [65/200], Batch [282/517], Loss: 0.3911
  Epoch [65/200], Batch [329/517], Loss: 0.1814
  Epoch [65/200], Batch [376/517], Loss: 0.1585
  Epoch [65/200], Batch [423/517], Loss: 0.1774
  Epoch [65/200], Batch [470/517], Loss: 0.1954
  Epoch [65/200], Batch [517/517], Loss: 0.3967
--- Epoch [65/200] complete. Average Training Loss: 0.2308 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [66/200], Batch [47/517], Loss: 0.1412
  Epoch [66/200], Batch [94/517], Loss: 0.2707
  Epoch [66/200], Batch [141/517], Loss: 0.1730
  Epoch [66/200], Batch [188/517], Loss: 0.1709
  Epoch [66/200], Batch [235/517], Loss: 0.1574
  Epoch [66/200], Batch [282/517], Loss: 0.1490
  Epoch [66/200], Batch [329/517], Loss: 0.1089
  Epoch [66/200], Batch [376/517], Loss: 0.1806
  Epoch [66/200], Batch [423/517], Loss: 0.2916
  Epoch [66/200], Batch [470/517], Loss: 0.2088
  Epoch [66/200], Batch [517/517], Loss: 0.1598
--- Epoch [66/200] complete. Average Training Loss: 0.2224 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [67/200], Batch [47/517], Loss: 0.2649
  Epoch [67/200], Batch [94/517], Loss: 0.4082
  Epoch [67/200], Batch [141/517], Loss: 0.2071
  Epoch [67/200], Batch [188/517], Loss: 0.2462
  Epoch [67/200], Batch [235/517], Loss: 0.0999
  Epoch [67/200], Batch [282/517], Loss: 0.3969
  Epoch [67/200], Batch [329/517], Loss: 0.3312
  Epoch [67/200], Batch [376/517], Loss: 0.1736
  Epoch [67/200], Batch [423/517], Loss: 0.2066
  Epoch [67/200], Batch [470/517], Loss: 0.4028
  Epoch [67/200], Batch [517/517], Loss: 0.1567
--- Epoch [67/200] complete. Average Training Loss: 0.2282 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [68/200], Batch [47/517], Loss: 0.3983
  Epoch [68/200], Batch [94/517], Loss: 0.1680
  Epoch [68/200], Batch [141/517], Loss: 0.3801
  Epoch [68/200], Batch [188/517], Loss: 0.3272
  Epoch [68/200], Batch [235/517], Loss: 0.4025
  Epoch [68/200], Batch [282/517], Loss: 0.1907
  Epoch [68/200], Batch [329/517], Loss: 0.1187
  Epoch [68/200], Batch [376/517], Loss: 0.1295
  Epoch [68/200], Batch [423/517], Loss: 0.2415
  Epoch [68/200], Batch [470/517], Loss: 0.3889
  Epoch [68/200], Batch [517/517], Loss: 0.4571
--- Epoch [68/200] complete. Average Training Loss: 0.2256 ---
--- Time taken for epoch: 313.75 seconds ---
  Epoch [69/200], Batch [47/517], Loss: 0.3979
  Epoch [69/200], Batch [94/517], Loss: 0.1930
  Epoch [69/200], Batch [141/517], Loss: 0.1932
  Epoch [69/200], Batch [188/517], Loss: 0.1800
  Epoch [69/200], Batch [235/517], Loss: 0.1296
  Epoch [69/200], Batch [282/517], Loss: 0.2673
  Epoch [69/200], Batch [329/517], Loss: 0.1869
  Epoch [69/200], Batch [376/517], Loss: 0.2692
  Epoch [69/200], Batch [423/517], Loss: 0.3325
  Epoch [69/200], Batch [470/517], Loss: 0.2384
  Epoch [69/200], Batch [517/517], Loss: 0.2073
--- Epoch [69/200] complete. Average Training Loss: 0.2361 ---
--- Time taken for epoch: 313.78 seconds ---
  Epoch [70/200], Batch [47/517], Loss: 0.2168
  Epoch [70/200], Batch [94/517], Loss: 0.3757
  Epoch [70/200], Batch [141/517], Loss: 0.2221
  Epoch [70/200], Batch [188/517], Loss: 0.3163
  Epoch [70/200], Batch [235/517], Loss: 0.1195
  Epoch [70/200], Batch [282/517], Loss: 0.1623
  Epoch [70/200], Batch [329/517], Loss: 0.2230
  Epoch [70/200], Batch [376/517], Loss: 0.2172
  Epoch [70/200], Batch [423/517], Loss: 0.1991
  Epoch [70/200], Batch [470/517], Loss: 0.2338
  Epoch [70/200], Batch [517/517], Loss: 0.3975
--- Epoch [70/200] complete. Average Training Loss: 0.2277 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [71/200], Batch [47/517], Loss: 0.1943
  Epoch [71/200], Batch [94/517], Loss: 0.3923
  Epoch [71/200], Batch [141/517], Loss: 0.1950
  Epoch [71/200], Batch [188/517], Loss: 0.1344
  Epoch [71/200], Batch [235/517], Loss: 0.3079
  Epoch [71/200], Batch [282/517], Loss: 0.0792
  Epoch [71/200], Batch [329/517], Loss: 0.1535
  Epoch [71/200], Batch [376/517], Loss: 0.1481
  Epoch [71/200], Batch [423/517], Loss: 0.3953
  Epoch [71/200], Batch [470/517], Loss: 0.1208
  Epoch [71/200], Batch [517/517], Loss: 0.1089
--- Epoch [71/200] complete. Average Training Loss: 0.2182 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [72/200], Batch [47/517], Loss: 0.1742
  Epoch [72/200], Batch [94/517], Loss: 0.2698
  Epoch [72/200], Batch [141/517], Loss: 0.4052
  Epoch [72/200], Batch [188/517], Loss: 0.1698
  Epoch [72/200], Batch [235/517], Loss: 0.1688
  Epoch [72/200], Batch [282/517], Loss: 0.1335
  Epoch [72/200], Batch [329/517], Loss: 0.1758
