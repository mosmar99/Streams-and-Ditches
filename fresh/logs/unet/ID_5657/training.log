Training started at: 2025-05-24 15:57:42
Device: cuda, Num Epochs: 200, Model: UNet
Batch Size: 128, Learning Rate: 0.0006
Loss Function: TverskyLoss, Optimizer: Adam
Weight Decay: None
Dropout: 0.05
Patch Size: 128
Epoch, AvgTrainLoss, 
1, 0.499136, 
2, 0.394227, 
3, 0.374897, 
4, 0.358895, 
5, 0.351534, 
6, 0.351476, 
7, 0.340169, 
8, 0.329577, 
9, 0.330074, 
10, 0.320990, 
11, 0.319852, 
12, 0.316026, 
13, 0.311827, 
14, 0.306048, 
15, 0.304652, 
16, 0.301863, 
17, 0.297555, 
18, 0.297386, 
19, 0.291453, 
20, 0.293234, 
21, 0.286527, 
22, 0.288842, 
23, 0.277581, 
24, 0.271238, 
25, 0.277485, 
26, 0.274691, 
27, 0.266163, 
28, 0.265313, 
29, 0.269132, 
30, 0.260554, 
31, 0.255438, 
32, 0.259574, 
33, 0.265985, 
34, 0.259713, 
35, 0.249326, 
36, 0.248768, 
37, 0.247650, 
38, 0.248972, 
39, 0.244850, 
40, 0.243005, 
41, 0.245043, 
42, 0.244865, 
43, 0.233629, 
44, 0.234590, 
45, 0.229135, 
46, 0.230200, 
47, 0.228829, 
48, 0.227063, 
49, 0.228655, 
50, 0.226811, 
51, 0.225708, 
52, 0.222925, 
53, 0.224348, 
54, 0.216303, 
55, 0.220648, 
56, 0.213094, 
57, 0.214360, 
58, 0.217697, 
59, 0.215299, 
60, 0.217619, 
61, 0.214290, 
62, 0.209807, 
63, 0.213773, 
64, 0.208441, 
65, 0.208028, 
66, 0.203438, 
67, 0.207224, 
68, 0.212330, 
69, 0.206805, 
70, 0.209994, 
71, 0.203489, 
72, 0.201087, 
73, 0.205826, 
74, 0.199374, 
75, 0.195576, 
76, 0.201081, 
77, 0.201111, 
78, 0.209450, 
79, 0.192857, 
80, 0.197738, 
81, 0.202371, 
82, 0.192009, 
83, 0.192284, 
84, 0.194960, 
85, 0.198767, 
86, 0.188023, 
87, 0.190640, 
88, 0.194167, 
89, 0.190986, 
90, 0.191867, 
91, 0.198285, 
92, 0.190438, 
93, 0.191729, 
94, 0.190148, 
95, 0.187415, 
96, 0.189622, 
97, 0.186697, 
98, 0.193422, 
99, 0.188840, 
100, 0.182050, 
101, 0.181118, 
102, 0.182967, 
103, 0.184520, 
104, 0.182505, 
105, 0.181793, 
106, 0.182018, 
107, 0.183771, 
108, 0.182051, 
109, 0.187113, 
110, 0.178667, 
111, 0.180941, 
112, 0.184094, 
113, 0.182375, 
114, 0.180334, 
115, 0.185154, 
116, 0.176825, 
117, 0.178395, 
118, 0.177883, 
119, 0.175565, 
120, 0.173073, 
121, 0.174174, 
122, 0.173375, 
123, 0.193353, 
124, 0.174523, 
125, 0.174091, 
126, 0.179922, 
127, 0.170887, 
128, 0.171579, 
129, 0.170080, 
130, 0.167409, 
131, 0.171753, 
132, 0.170973, 
133, 0.170369, 
134, 0.178920, 
135, 0.169773, 
136, 0.170503, 
137, 0.166662, 
138, 0.167573, 
139, 0.170978, 
140, 0.166221, 
141, 0.177649, 
142, 0.168371, 
143, 0.167946, 
144, 0.162360, 
145, 0.167710, 
146, 0.163887, 
147, 0.166068, 
148, 0.166065, 
149, 0.163319, 
150, 0.161348, 
151, 0.168473, 
152, 0.167242, 
153, 0.162751, 
154, 0.162120, 
155, 0.165180, 
156, 0.160516, 
157, 0.172177, 
158, 0.165426, 
159, 0.161593, 
160, 0.165368, 
161, 0.164775, 
162, 0.159805, 
163, 0.161555, 
164, 0.161632, 
165, 0.158210, 
166, 0.161811, 
167, 0.162367, 
168, 0.160488, 
169, 0.161395, 
170, 0.164040, 
171, 0.161419, 
172, 0.157943, 
173, 0.159643, 
174, 0.163047, 
175, 0.164191, 
176, 0.158845, 
177, 0.157967, 
178, 0.155855, 
179, 0.160412, 
180, 0.155376, 
181, 0.154640, 
182, 0.167758, 
183, 0.157867, 
184, 0.157077, 
185, 0.154490, 
186, 0.153471, 
187, 0.152914, 
188, 0.152750, 
189, 0.156840, 
190, 0.160858, 
191, 0.154656, 
192, 0.152836, 
193, 0.151966, 
194, 0.156227, 
195, 0.153957, 
196, 0.153849, 
197, 0.151963, 
198, 0.154488, 
199, 0.153732, 
200, 0.166178, 
Training finished at: 2025-05-25 09:25:51
