Training started at: 2025-05-23 20:53:16
Device: cuda, Num Epochs: 200, Model: UNet
Batch Size: 8, Learning Rate: 0.0005
Loss Function: TverskyLoss, Optimizer: Adam
Weight Decay: None
Dropout: 0.05
Epoch, AvgTrainLoss, 
1, 0.531501, 
2, 0.412563, 
3, 0.388841, 
4, 0.380049, 
5, 0.365724, 
6, 0.360404, 
7, 0.358973, 
8, 0.348002, 
9, 0.344316, 
10, 0.343654, 
11, 0.332075, 
12, 0.334166, 
13, 0.333255, 
14, 0.329203, 
15, 0.326687, 
16, 0.336554, 
17, 0.320980, 
18, 0.324094, 
19, 0.313109, 
20, 0.312802, 
21, 0.318249, 
22, 0.310697, 
23, 0.301840, 
24, 0.296787, 
25, 0.306844, 
26, 0.295925, 
27, 0.298448, 
28, 0.298037, 
29, 0.289550, 
30, 0.286598, 
31, 0.283941, 
32, 0.292955, 
33, 0.285456, 
34, 0.283558, 
35, 0.276806, 
36, 0.276634, 
37, 0.279725, 
38, 0.272861, 
39, 0.273928, 
40, 0.273145, 
41, 0.269966, 
42, 0.266507, 
43, 0.259921, 
44, 0.259037, 
45, 0.260977, 
46, 0.263467, 
47, 0.257306, 
48, 0.247559, 
49, 0.259278, 
50, 0.263305, 
51, 0.259628, 
52, 0.251124, 
53, 0.250567, 
54, 0.245790, 
55, 0.244070, 
56, 0.248184, 
57, 0.261284, 
58, 0.254225, 
59, 0.243525, 
60, 0.243856, 
61, 0.241829, 
62, 0.246134, 
63, 0.243334, 
64, 0.240449, 
65, 0.244187, 
66, 0.248842, 
67, 0.241344, 
68, 0.241381, 
69, 0.231556, 
70, 0.237721, 
71, 0.235697, 
72, 0.236375, 
73, 0.242320, 
74, 0.233615, 
75, 0.234009, 
76, 0.232020, 
77, 0.222109, 
78, 0.240401, 
79, 0.225221, 
80, 0.239951, 
81, 0.225338, 
82, 0.236956, 
83, 0.232089, 
84, 0.236251, 
85, 0.223223, 
86, 0.218551, 
87, 0.223061, 
88, 0.219555, 
89, 0.223476, 
90, 0.233477, 
91, 0.227589, 
92, 0.232449, 
93, 0.220831, 
94, 0.221151, 
95, 0.220121, 
96, 0.225627, 
97, 0.220979, 
98, 0.224816, 
99, 0.217152, 
100, 0.214800, 
101, 0.213293, 
102, 0.217689, 
103, 0.214141, 
104, 0.213530, 
105, 0.232475, 
106, 0.215919, 
107, 0.207910, 
108, 0.209938, 
109, 0.217969, 
110, 0.216854, 
111, 0.206992, 
112, 0.217891, 
113, 0.205683, 
114, 0.222029, 
115, 0.210880, 
116, 0.210298, 
117, 0.212085, 
118, 0.208894, 
119, 0.203735, 
120, 0.220405, 
121, 0.219682, 
122, 0.210993, 
123, 0.201722, 
124, 0.198551, 
125, 0.199317, 
126, 0.204291, 
127, 0.207025, 
128, 0.216449, 
129, 0.233086, 
130, 0.221837, 
131, 0.209504, 
132, 0.201848, 
