Training started at: 2025-05-24 10:51:25
Device: cuda, Num Epochs: 200, Model: UNet
Batch Size: 8, Learning Rate: 0.0005
Loss Function: TverskyLoss, Optimizer: Adam
Weight Decay: None
Dropout: 0.05
Epoch, AvgTrainLoss, 
1, 0.519093, 
2, 0.404124, 
3, 0.377627, 
4, 0.371498, 
5, 0.360964, 
6, 0.353372, 
7, 0.346492, 
8, 0.339580, 
9, 0.334778, 
10, 0.337957, 
11, 0.333142, 
12, 0.321335, 
13, 0.322815, 
14, 0.315918, 
15, 0.311802, 
16, 0.304474, 
17, 0.302867, 
18, 0.304516, 
19, 0.303895, 
20, 0.299807, 
21, 0.297064, 
22, 0.288540, 
23, 0.287192, 
24, 0.286255, 
25, 0.287285, 
26, 0.283940, 
27, 0.273386, 
28, 0.281180, 
29, 0.274912, 
30, 0.272404, 
31, 0.270228, 
32, 0.268787, 
33, 0.264848, 
34, 0.266852, 
35, 0.269581, 
36, 0.262247, 
37, 0.261804, 
38, 0.255487, 
39, 0.257264, 
40, 0.254566, 
41, 0.252739, 
42, 0.250634, 
43, 0.247327, 
44, 0.251214, 
45, 0.251228, 
46, 0.245980, 
47, 0.252181, 
48, 0.251720, 
49, 0.247266, 
50, 0.238249, 
51, 0.237100, 
52, 0.240920, 
53, 0.249125, 
54, 0.236383, 
55, 0.236122, 
56, 0.231729, 
57, 0.233243, 
58, 0.233918, 
59, 0.243454, 
60, 0.238999, 
61, 0.233097, 
62, 0.230204, 
63, 0.231579, 
64, 0.236316, 
65, 0.230772, 
66, 0.222437, 
67, 0.228208, 
68, 0.225614, 
69, 0.236054, 
70, 0.227725, 
71, 0.218248, 
72, 0.228866, 
73, 0.224472, 
74, 0.229931, 
75, 0.223202, 
76, 0.224709, 
77, 0.219087, 
78, 0.220577, 
79, 0.227471, 
80, 0.216072, 
81, 0.219388, 
82, 0.223775, 
83, 0.223714, 
84, 0.226509, 
85, 0.210618, 
86, 0.220511, 
87, 0.207668, 
88, 0.218111, 
89, 0.218190, 
90, 0.210863, 
91, 0.213152, 
92, 0.217007, 
93, 0.214710, 
94, 0.206801, 
95, 0.209849, 
96, 0.206634, 
97, 0.222691, 
98, 0.226016, 
99, 0.209743, 
100, 0.207243, 
101, 0.196889, 
102, 0.202467, 
103, 0.204739, 
104, 0.209214, 
105, 0.211346, 
106, 0.208026, 
107, 0.202173, 
108, 0.207764, 
109, 0.212223, 
110, 0.210083, 
111, 0.212543, 
112, 0.212278, 
113, 0.205320, 
114, 0.218930, 
115, 0.294422, 
116, 0.257798, 
117, 0.338169, 
118, 0.384142, 
119, 0.380822, 
120, 0.377451, 
121, 0.313835, 
122, 0.240635, 
123, 0.286393, 
124, 0.320267, 
125, 0.317729, 
126, 0.342823, 
127, 0.283990, 
128, 0.270085, 
129, 0.318786, 
130, 0.275155, 
131, 0.251817, 
132, 0.250685, 
133, 0.256654, 
134, 0.245146, 
135, 0.260353, 
136, 0.241782, 
137, 0.250624, 
138, 0.246214, 
139, 0.258903, 
140, 0.241057, 
141, 0.278161, 
142, 0.266952, 
143, 0.290184, 
144, 0.264958, 
145, 0.321064, 
146, 0.271615, 
147, 0.243172, 
148, 0.238481, 
149, 0.227436, 
150, 0.374602, 
151, 0.370411, 
152, 0.371916, 
153, 0.371710, 
154, 0.372202, 
155, 0.373240, 
156, 0.369028, 
157, 0.362561, 
158, 0.358115, 
159, 0.358961, 
160, 0.364451, 
161, 0.362450, 
162, 0.355963, 
163, 0.359137, 
164, 0.353653, 
165, 0.276723, 
166, 0.252030, 
167, 0.255439, 
168, 0.234482, 
169, 0.252269, 
170, 0.253837, 
171, 0.269529, 
172, 0.265773, 
173, 0.276967, 
174, 0.302311, 
175, 0.379688, 
176, 0.375741, 
177, 0.371478, 
178, 0.367327, 
179, 0.362646, 
180, 0.372023, 
181, 0.359656, 
182, 0.370825, 
183, 0.365662, 
184, 0.365138, 
185, 0.360934, 
186, 0.359542, 
187, 0.296128, 
188, 0.278105, 
189, 0.249314, 
190, 0.242484, 
191, 0.223021, 
192, 0.225342, 
193, 0.256077, 
194, 0.270768, 
195, 0.314309, 
196, 0.294854, 
197, 0.257872, 
198, 0.279398, 
199, 0.254689, 
200, 0.310155, 
Training finished at: 2025-05-25 04:17:54
