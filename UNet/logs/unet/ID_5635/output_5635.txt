Beginning to load data...
 -- Train dataset size: 4134 / 4593 ~ 0.9
 -- Test dataset size: 459 / 4593 ~ 0.1
 -- Created DataLoaders with batch size: 8

Starting UNet Training...
  Epoch [1/200], Batch [47/517], Loss: 0.6924
  Epoch [1/200], Batch [94/517], Loss: 0.6011
  Epoch [1/200], Batch [141/517], Loss: 0.5588
  Epoch [1/200], Batch [188/517], Loss: 0.5133
  Epoch [1/200], Batch [235/517], Loss: 0.4694
  Epoch [1/200], Batch [282/517], Loss: 0.4485
  Epoch [1/200], Batch [329/517], Loss: 0.4592
  Epoch [1/200], Batch [376/517], Loss: 0.4272
  Epoch [1/200], Batch [423/517], Loss: 0.4190
  Epoch [1/200], Batch [470/517], Loss: 0.4345
  Epoch [1/200], Batch [517/517], Loss: 0.4406
--- Epoch [1/200] complete. Average Training Loss: 0.5191 ---
--- Time taken for epoch: 314.43 seconds ---
  Epoch [2/200], Batch [47/517], Loss: 0.4277
  Epoch [2/200], Batch [94/517], Loss: 0.3553
  Epoch [2/200], Batch [141/517], Loss: 0.4314
  Epoch [2/200], Batch [188/517], Loss: 0.3953
  Epoch [2/200], Batch [235/517], Loss: 0.4275
  Epoch [2/200], Batch [282/517], Loss: 0.4967
  Epoch [2/200], Batch [329/517], Loss: 0.4024
  Epoch [2/200], Batch [376/517], Loss: 0.3827
  Epoch [2/200], Batch [423/517], Loss: 0.4194
  Epoch [2/200], Batch [470/517], Loss: 0.4240
  Epoch [2/200], Batch [517/517], Loss: 0.4366
--- Epoch [2/200] complete. Average Training Loss: 0.4041 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [3/200], Batch [47/517], Loss: 0.3516
  Epoch [3/200], Batch [94/517], Loss: 0.4490
  Epoch [3/200], Batch [141/517], Loss: 0.4207
  Epoch [3/200], Batch [188/517], Loss: 0.4016
  Epoch [3/200], Batch [235/517], Loss: 0.4056
  Epoch [3/200], Batch [282/517], Loss: 0.3971
  Epoch [3/200], Batch [329/517], Loss: 0.4451
  Epoch [3/200], Batch [376/517], Loss: 0.4201
  Epoch [3/200], Batch [423/517], Loss: 0.2676
  Epoch [3/200], Batch [470/517], Loss: 0.4291
  Epoch [3/200], Batch [517/517], Loss: 0.3474
--- Epoch [3/200] complete. Average Training Loss: 0.3776 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [4/200], Batch [47/517], Loss: 0.4392
  Epoch [4/200], Batch [94/517], Loss: 0.4100
  Epoch [4/200], Batch [141/517], Loss: 0.2568
  Epoch [4/200], Batch [188/517], Loss: 0.3955
  Epoch [4/200], Batch [235/517], Loss: 0.4397
  Epoch [4/200], Batch [282/517], Loss: 0.4252
  Epoch [4/200], Batch [329/517], Loss: 0.3253
  Epoch [4/200], Batch [376/517], Loss: 0.2518
  Epoch [4/200], Batch [423/517], Loss: 0.3915
  Epoch [4/200], Batch [470/517], Loss: 0.4011
  Epoch [4/200], Batch [517/517], Loss: 0.4356
--- Epoch [4/200] complete. Average Training Loss: 0.3715 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [5/200], Batch [47/517], Loss: 0.4141
  Epoch [5/200], Batch [94/517], Loss: 0.2656
  Epoch [5/200], Batch [141/517], Loss: 0.3980
  Epoch [5/200], Batch [188/517], Loss: 0.3227
  Epoch [5/200], Batch [235/517], Loss: 0.4133
  Epoch [5/200], Batch [282/517], Loss: 0.4257
  Epoch [5/200], Batch [329/517], Loss: 0.4309
  Epoch [5/200], Batch [376/517], Loss: 0.4747
  Epoch [5/200], Batch [423/517], Loss: 0.4070
  Epoch [5/200], Batch [470/517], Loss: 0.3142
  Epoch [5/200], Batch [517/517], Loss: 0.4778
--- Epoch [5/200] complete. Average Training Loss: 0.3610 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [6/200], Batch [47/517], Loss: 0.4263
  Epoch [6/200], Batch [94/517], Loss: 0.4265
  Epoch [6/200], Batch [141/517], Loss: 0.4273
  Epoch [6/200], Batch [188/517], Loss: 0.2679
  Epoch [6/200], Batch [235/517], Loss: 0.4147
  Epoch [6/200], Batch [282/517], Loss: 0.2844
  Epoch [6/200], Batch [329/517], Loss: 0.4067
  Epoch [6/200], Batch [376/517], Loss: 0.4085
  Epoch [6/200], Batch [423/517], Loss: 0.3919
  Epoch [6/200], Batch [470/517], Loss: 0.3678
  Epoch [6/200], Batch [517/517], Loss: 0.4708
--- Epoch [6/200] complete. Average Training Loss: 0.3534 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [7/200], Batch [47/517], Loss: 0.2470
  Epoch [7/200], Batch [94/517], Loss: 0.4149
  Epoch [7/200], Batch [141/517], Loss: 0.4161
  Epoch [7/200], Batch [188/517], Loss: 0.2512
  Epoch [7/200], Batch [235/517], Loss: 0.3049
  Epoch [7/200], Batch [282/517], Loss: 0.2032
  Epoch [7/200], Batch [329/517], Loss: 0.2657
  Epoch [7/200], Batch [376/517], Loss: 0.2694
  Epoch [7/200], Batch [423/517], Loss: 0.3283
  Epoch [7/200], Batch [470/517], Loss: 0.1851
  Epoch [7/200], Batch [517/517], Loss: 0.3700
--- Epoch [7/200] complete. Average Training Loss: 0.3465 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [8/200], Batch [47/517], Loss: 0.3297
  Epoch [8/200], Batch [94/517], Loss: 0.3172
  Epoch [8/200], Batch [141/517], Loss: 0.3867
  Epoch [8/200], Batch [188/517], Loss: 0.4051
  Epoch [8/200], Batch [235/517], Loss: 0.3171
  Epoch [8/200], Batch [282/517], Loss: 0.2520
  Epoch [8/200], Batch [329/517], Loss: 0.4111
  Epoch [8/200], Batch [376/517], Loss: 0.2308
  Epoch [8/200], Batch [423/517], Loss: 0.3290
  Epoch [8/200], Batch [470/517], Loss: 0.3489
  Epoch [8/200], Batch [517/517], Loss: 0.4161
--- Epoch [8/200] complete. Average Training Loss: 0.3396 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [9/200], Batch [47/517], Loss: 0.2483
  Epoch [9/200], Batch [94/517], Loss: 0.2741
  Epoch [9/200], Batch [141/517], Loss: 0.3685
  Epoch [9/200], Batch [188/517], Loss: 0.3179
  Epoch [9/200], Batch [235/517], Loss: 0.3089
  Epoch [9/200], Batch [282/517], Loss: 0.3637
  Epoch [9/200], Batch [329/517], Loss: 0.3720
  Epoch [9/200], Batch [376/517], Loss: 0.4298
  Epoch [9/200], Batch [423/517], Loss: 0.4018
  Epoch [9/200], Batch [470/517], Loss: 0.3004
  Epoch [9/200], Batch [517/517], Loss: 0.4579
--- Epoch [9/200] complete. Average Training Loss: 0.3348 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [10/200], Batch [47/517], Loss: 0.2840
  Epoch [10/200], Batch [94/517], Loss: 0.2563
  Epoch [10/200], Batch [141/517], Loss: 0.4093
  Epoch [10/200], Batch [188/517], Loss: 0.2722
  Epoch [10/200], Batch [235/517], Loss: 0.3464
  Epoch [10/200], Batch [282/517], Loss: 0.4251
  Epoch [10/200], Batch [329/517], Loss: 0.4353
  Epoch [10/200], Batch [376/517], Loss: 0.4862
  Epoch [10/200], Batch [423/517], Loss: 0.4127
  Epoch [10/200], Batch [470/517], Loss: 0.4242
  Epoch [10/200], Batch [517/517], Loss: 0.5003
--- Epoch [10/200] complete. Average Training Loss: 0.3380 ---
--- Time taken for epoch: 314.11 seconds ---
  Epoch [11/200], Batch [47/517], Loss: 0.4093
  Epoch [11/200], Batch [94/517], Loss: 0.3067
  Epoch [11/200], Batch [141/517], Loss: 0.1920
  Epoch [11/200], Batch [188/517], Loss: 0.3623
  Epoch [11/200], Batch [235/517], Loss: 0.4597
  Epoch [11/200], Batch [282/517], Loss: 0.3683
  Epoch [11/200], Batch [329/517], Loss: 0.3250
  Epoch [11/200], Batch [376/517], Loss: 0.3697
  Epoch [11/200], Batch [423/517], Loss: 0.2141
  Epoch [11/200], Batch [470/517], Loss: 0.3800
  Epoch [11/200], Batch [517/517], Loss: 0.4455
--- Epoch [11/200] complete. Average Training Loss: 0.3331 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [12/200], Batch [47/517], Loss: 0.3409
  Epoch [12/200], Batch [94/517], Loss: 0.3086
  Epoch [12/200], Batch [141/517], Loss: 0.1650
  Epoch [12/200], Batch [188/517], Loss: 0.3197
  Epoch [12/200], Batch [235/517], Loss: 0.4034
  Epoch [12/200], Batch [282/517], Loss: 0.3388
  Epoch [12/200], Batch [329/517], Loss: 0.2367
  Epoch [12/200], Batch [376/517], Loss: 0.4070
  Epoch [12/200], Batch [423/517], Loss: 0.1505
  Epoch [12/200], Batch [470/517], Loss: 0.4206
  Epoch [12/200], Batch [517/517], Loss: 0.1751
--- Epoch [12/200] complete. Average Training Loss: 0.3213 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [13/200], Batch [47/517], Loss: 0.4414
  Epoch [13/200], Batch [94/517], Loss: 0.3449
  Epoch [13/200], Batch [141/517], Loss: 0.1479
  Epoch [13/200], Batch [188/517], Loss: 0.1776
  Epoch [13/200], Batch [235/517], Loss: 0.2268
  Epoch [13/200], Batch [282/517], Loss: 0.2510
  Epoch [13/200], Batch [329/517], Loss: 0.2381
  Epoch [13/200], Batch [376/517], Loss: 0.2607
  Epoch [13/200], Batch [423/517], Loss: 0.4155
  Epoch [13/200], Batch [470/517], Loss: 0.4204
  Epoch [13/200], Batch [517/517], Loss: 0.4072
--- Epoch [13/200] complete. Average Training Loss: 0.3228 ---
--- Time taken for epoch: 314.03 seconds ---
  Epoch [14/200], Batch [47/517], Loss: 0.2818
  Epoch [14/200], Batch [94/517], Loss: 0.1941
  Epoch [14/200], Batch [141/517], Loss: 0.3263
  Epoch [14/200], Batch [188/517], Loss: 0.3474
  Epoch [14/200], Batch [235/517], Loss: 0.3696
  Epoch [14/200], Batch [282/517], Loss: 0.3314
  Epoch [14/200], Batch [329/517], Loss: 0.3901
  Epoch [14/200], Batch [376/517], Loss: 0.4183
  Epoch [14/200], Batch [423/517], Loss: 0.3272
  Epoch [14/200], Batch [470/517], Loss: 0.4231
  Epoch [14/200], Batch [517/517], Loss: 0.3940
--- Epoch [14/200] complete. Average Training Loss: 0.3159 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [15/200], Batch [47/517], Loss: 0.4237
  Epoch [15/200], Batch [94/517], Loss: 0.2976
  Epoch [15/200], Batch [141/517], Loss: 0.2446
  Epoch [15/200], Batch [188/517], Loss: 0.2229
  Epoch [15/200], Batch [235/517], Loss: 0.4176
  Epoch [15/200], Batch [282/517], Loss: 0.2054
  Epoch [15/200], Batch [329/517], Loss: 0.2197
  Epoch [15/200], Batch [376/517], Loss: 0.2506
  Epoch [15/200], Batch [423/517], Loss: 0.2431
  Epoch [15/200], Batch [470/517], Loss: 0.2216
  Epoch [15/200], Batch [517/517], Loss: 0.2922
--- Epoch [15/200] complete. Average Training Loss: 0.3118 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [16/200], Batch [47/517], Loss: 0.3490
  Epoch [16/200], Batch [94/517], Loss: 0.4161
  Epoch [16/200], Batch [141/517], Loss: 0.2738
  Epoch [16/200], Batch [188/517], Loss: 0.3453
  Epoch [16/200], Batch [235/517], Loss: 0.3130
  Epoch [16/200], Batch [282/517], Loss: 0.3890
  Epoch [16/200], Batch [329/517], Loss: 0.1974
  Epoch [16/200], Batch [376/517], Loss: 0.4419
  Epoch [16/200], Batch [423/517], Loss: 0.3087
  Epoch [16/200], Batch [470/517], Loss: 0.3761
  Epoch [16/200], Batch [517/517], Loss: 0.4186
--- Epoch [16/200] complete. Average Training Loss: 0.3045 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [17/200], Batch [47/517], Loss: 0.3310
  Epoch [17/200], Batch [94/517], Loss: 0.1904
  Epoch [17/200], Batch [141/517], Loss: 0.1900
  Epoch [17/200], Batch [188/517], Loss: 0.2505
  Epoch [17/200], Batch [235/517], Loss: 0.4037
  Epoch [17/200], Batch [282/517], Loss: 0.3471
  Epoch [17/200], Batch [329/517], Loss: 0.2942
  Epoch [17/200], Batch [376/517], Loss: 0.3748
  Epoch [17/200], Batch [423/517], Loss: 0.3136
  Epoch [17/200], Batch [470/517], Loss: 0.2274
  Epoch [17/200], Batch [517/517], Loss: 0.1832
--- Epoch [17/200] complete. Average Training Loss: 0.3029 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [18/200], Batch [47/517], Loss: 0.2125
  Epoch [18/200], Batch [94/517], Loss: 0.4243
  Epoch [18/200], Batch [141/517], Loss: 0.2119
  Epoch [18/200], Batch [188/517], Loss: 0.2955
  Epoch [18/200], Batch [235/517], Loss: 0.4058
  Epoch [18/200], Batch [282/517], Loss: 0.2307
  Epoch [18/200], Batch [329/517], Loss: 0.3978
  Epoch [18/200], Batch [376/517], Loss: 0.4225
  Epoch [18/200], Batch [423/517], Loss: 0.4152
  Epoch [18/200], Batch [470/517], Loss: 0.2366
  Epoch [18/200], Batch [517/517], Loss: 0.3875
--- Epoch [18/200] complete. Average Training Loss: 0.3045 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [19/200], Batch [47/517], Loss: 0.3967
  Epoch [19/200], Batch [94/517], Loss: 0.4087
  Epoch [19/200], Batch [141/517], Loss: 0.3865
  Epoch [19/200], Batch [188/517], Loss: 0.1431
  Epoch [19/200], Batch [235/517], Loss: 0.3724
  Epoch [19/200], Batch [282/517], Loss: 0.2228
  Epoch [19/200], Batch [329/517], Loss: 0.4517
  Epoch [19/200], Batch [376/517], Loss: 0.4160
  Epoch [19/200], Batch [423/517], Loss: 0.2570
  Epoch [19/200], Batch [470/517], Loss: 0.3556
  Epoch [19/200], Batch [517/517], Loss: 0.4246
--- Epoch [19/200] complete. Average Training Loss: 0.3039 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [20/200], Batch [47/517], Loss: 0.2424
  Epoch [20/200], Batch [94/517], Loss: 0.2266
  Epoch [20/200], Batch [141/517], Loss: 0.3620
  Epoch [20/200], Batch [188/517], Loss: 0.2289
  Epoch [20/200], Batch [235/517], Loss: 0.2688
  Epoch [20/200], Batch [282/517], Loss: 0.2726
  Epoch [20/200], Batch [329/517], Loss: 0.2581
  Epoch [20/200], Batch [376/517], Loss: 0.2581
  Epoch [20/200], Batch [423/517], Loss: 0.3881
  Epoch [20/200], Batch [470/517], Loss: 0.2481
  Epoch [20/200], Batch [517/517], Loss: 0.3182
--- Epoch [20/200] complete. Average Training Loss: 0.2998 ---
--- Time taken for epoch: 314.20 seconds ---
  Epoch [21/200], Batch [47/517], Loss: 0.3654
  Epoch [21/200], Batch [94/517], Loss: 0.2854
  Epoch [21/200], Batch [141/517], Loss: 0.2749
  Epoch [21/200], Batch [188/517], Loss: 0.3950
  Epoch [21/200], Batch [235/517], Loss: 0.1353
  Epoch [21/200], Batch [282/517], Loss: 0.2957
  Epoch [21/200], Batch [329/517], Loss: 0.2667
  Epoch [21/200], Batch [376/517], Loss: 0.2649
  Epoch [21/200], Batch [423/517], Loss: 0.4104
  Epoch [21/200], Batch [470/517], Loss: 0.4257
  Epoch [21/200], Batch [517/517], Loss: 0.2976
--- Epoch [21/200] complete. Average Training Loss: 0.2971 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [22/200], Batch [47/517], Loss: 0.2886
  Epoch [22/200], Batch [94/517], Loss: 0.2814
  Epoch [22/200], Batch [141/517], Loss: 0.1717
  Epoch [22/200], Batch [188/517], Loss: 0.4046
  Epoch [22/200], Batch [235/517], Loss: 0.4468
  Epoch [22/200], Batch [282/517], Loss: 0.2841
  Epoch [22/200], Batch [329/517], Loss: 0.3927
  Epoch [22/200], Batch [376/517], Loss: 0.4377
  Epoch [22/200], Batch [423/517], Loss: 0.1688
  Epoch [22/200], Batch [470/517], Loss: 0.2659
  Epoch [22/200], Batch [517/517], Loss: 0.1468
--- Epoch [22/200] complete. Average Training Loss: 0.2885 ---
--- Time taken for epoch: 313.99 seconds ---
  Epoch [23/200], Batch [47/517], Loss: 0.3222
  Epoch [23/200], Batch [94/517], Loss: 0.3971
  Epoch [23/200], Batch [141/517], Loss: 0.3929
  Epoch [23/200], Batch [188/517], Loss: 0.4043
  Epoch [23/200], Batch [235/517], Loss: 0.2556
  Epoch [23/200], Batch [282/517], Loss: 0.2728
  Epoch [23/200], Batch [329/517], Loss: 0.2634
  Epoch [23/200], Batch [376/517], Loss: 0.1253
  Epoch [23/200], Batch [423/517], Loss: 0.3854
  Epoch [23/200], Batch [470/517], Loss: 0.1710
  Epoch [23/200], Batch [517/517], Loss: 0.4013
--- Epoch [23/200] complete. Average Training Loss: 0.2872 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [24/200], Batch [47/517], Loss: 0.4061
  Epoch [24/200], Batch [94/517], Loss: 0.3413
  Epoch [24/200], Batch [141/517], Loss: 0.4038
  Epoch [24/200], Batch [188/517], Loss: 0.1782
  Epoch [24/200], Batch [235/517], Loss: 0.3935
  Epoch [24/200], Batch [282/517], Loss: 0.3857
  Epoch [24/200], Batch [329/517], Loss: 0.2818
  Epoch [24/200], Batch [376/517], Loss: 0.3277
  Epoch [24/200], Batch [423/517], Loss: 0.2732
  Epoch [24/200], Batch [470/517], Loss: 0.1424
  Epoch [24/200], Batch [517/517], Loss: 0.3522
--- Epoch [24/200] complete. Average Training Loss: 0.2863 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [25/200], Batch [47/517], Loss: 0.1950
  Epoch [25/200], Batch [94/517], Loss: 0.1644
  Epoch [25/200], Batch [141/517], Loss: 0.2982
  Epoch [25/200], Batch [188/517], Loss: 0.2963
  Epoch [25/200], Batch [235/517], Loss: 0.3555
  Epoch [25/200], Batch [282/517], Loss: 0.3590
  Epoch [25/200], Batch [329/517], Loss: 0.4120
  Epoch [25/200], Batch [376/517], Loss: 0.4141
  Epoch [25/200], Batch [423/517], Loss: 0.1069
  Epoch [25/200], Batch [470/517], Loss: 0.2269
  Epoch [25/200], Batch [517/517], Loss: 0.2759
--- Epoch [25/200] complete. Average Training Loss: 0.2873 ---
--- Time taken for epoch: 314.10 seconds ---
 -- Updated Checkpoint: Saved model at 25 epochs.
  Epoch [26/200], Batch [47/517], Loss: 0.1498
  Epoch [26/200], Batch [94/517], Loss: 0.3997
  Epoch [26/200], Batch [141/517], Loss: 0.2919
  Epoch [26/200], Batch [188/517], Loss: 0.3310
  Epoch [26/200], Batch [235/517], Loss: 0.2324
  Epoch [26/200], Batch [282/517], Loss: 0.3885
  Epoch [26/200], Batch [329/517], Loss: 0.2848
  Epoch [26/200], Batch [376/517], Loss: 0.2181
  Epoch [26/200], Batch [423/517], Loss: 0.3051
  Epoch [26/200], Batch [470/517], Loss: 0.3911
  Epoch [26/200], Batch [517/517], Loss: 0.4211
--- Epoch [26/200] complete. Average Training Loss: 0.2839 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [27/200], Batch [47/517], Loss: 0.3338
  Epoch [27/200], Batch [94/517], Loss: 0.3475
  Epoch [27/200], Batch [141/517], Loss: 0.4000
  Epoch [27/200], Batch [188/517], Loss: 0.2877
  Epoch [27/200], Batch [235/517], Loss: 0.3520
  Epoch [27/200], Batch [282/517], Loss: 0.1799
  Epoch [27/200], Batch [329/517], Loss: 0.3968
  Epoch [27/200], Batch [376/517], Loss: 0.2422
  Epoch [27/200], Batch [423/517], Loss: 0.2615
  Epoch [27/200], Batch [470/517], Loss: 0.3042
  Epoch [27/200], Batch [517/517], Loss: 0.4129
--- Epoch [27/200] complete. Average Training Loss: 0.2734 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [28/200], Batch [47/517], Loss: 0.3752
  Epoch [28/200], Batch [94/517], Loss: 0.3078
  Epoch [28/200], Batch [141/517], Loss: 0.4397
  Epoch [28/200], Batch [188/517], Loss: 0.2416
  Epoch [28/200], Batch [235/517], Loss: 0.2797
  Epoch [28/200], Batch [282/517], Loss: 0.2311
  Epoch [28/200], Batch [329/517], Loss: 0.1454
  Epoch [28/200], Batch [376/517], Loss: 0.3309
  Epoch [28/200], Batch [423/517], Loss: 0.4023
  Epoch [28/200], Batch [470/517], Loss: 0.2581
  Epoch [28/200], Batch [517/517], Loss: 0.4239
--- Epoch [28/200] complete. Average Training Loss: 0.2812 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [29/200], Batch [47/517], Loss: 0.1807
  Epoch [29/200], Batch [94/517], Loss: 0.2720
  Epoch [29/200], Batch [141/517], Loss: 0.1762
  Epoch [29/200], Batch [188/517], Loss: 0.2159
  Epoch [29/200], Batch [235/517], Loss: 0.1663
  Epoch [29/200], Batch [282/517], Loss: 0.2645
  Epoch [29/200], Batch [329/517], Loss: 0.1644
  Epoch [29/200], Batch [376/517], Loss: 0.2306
  Epoch [29/200], Batch [423/517], Loss: 0.2606
  Epoch [29/200], Batch [470/517], Loss: 0.4234
  Epoch [29/200], Batch [517/517], Loss: 0.4206
--- Epoch [29/200] complete. Average Training Loss: 0.2749 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [30/200], Batch [47/517], Loss: 0.3731
  Epoch [30/200], Batch [94/517], Loss: 0.4308
  Epoch [30/200], Batch [141/517], Loss: 0.2711
  Epoch [30/200], Batch [188/517], Loss: 0.4159
  Epoch [30/200], Batch [235/517], Loss: 0.4169
  Epoch [30/200], Batch [282/517], Loss: 0.1775
  Epoch [30/200], Batch [329/517], Loss: 0.2160
  Epoch [30/200], Batch [376/517], Loss: 0.2911
  Epoch [30/200], Batch [423/517], Loss: 0.1648
  Epoch [30/200], Batch [470/517], Loss: 0.1771
  Epoch [30/200], Batch [517/517], Loss: 0.3050
--- Epoch [30/200] complete. Average Training Loss: 0.2724 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [31/200], Batch [47/517], Loss: 0.3592
  Epoch [31/200], Batch [94/517], Loss: 0.3548
  Epoch [31/200], Batch [141/517], Loss: 0.2539
  Epoch [31/200], Batch [188/517], Loss: 0.1992
  Epoch [31/200], Batch [235/517], Loss: 0.3398
  Epoch [31/200], Batch [282/517], Loss: 0.1745
  Epoch [31/200], Batch [329/517], Loss: 0.3773
  Epoch [31/200], Batch [376/517], Loss: 0.4019
  Epoch [31/200], Batch [423/517], Loss: 0.2449
  Epoch [31/200], Batch [470/517], Loss: 0.3299
  Epoch [31/200], Batch [517/517], Loss: 0.2424
--- Epoch [31/200] complete. Average Training Loss: 0.2702 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [32/200], Batch [47/517], Loss: 0.3929
  Epoch [32/200], Batch [94/517], Loss: 0.3002
  Epoch [32/200], Batch [141/517], Loss: 0.4131
  Epoch [32/200], Batch [188/517], Loss: 0.2869
  Epoch [32/200], Batch [235/517], Loss: 0.2864
  Epoch [32/200], Batch [282/517], Loss: 0.2444
  Epoch [32/200], Batch [329/517], Loss: 0.3128
  Epoch [32/200], Batch [376/517], Loss: 0.1737
  Epoch [32/200], Batch [423/517], Loss: 0.2610
  Epoch [32/200], Batch [470/517], Loss: 0.3129
  Epoch [32/200], Batch [517/517], Loss: 0.3267
--- Epoch [32/200] complete. Average Training Loss: 0.2688 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [33/200], Batch [47/517], Loss: 0.1830
  Epoch [33/200], Batch [94/517], Loss: 0.2535
  Epoch [33/200], Batch [141/517], Loss: 0.1590
  Epoch [33/200], Batch [188/517], Loss: 0.2293
  Epoch [33/200], Batch [235/517], Loss: 0.3914
  Epoch [33/200], Batch [282/517], Loss: 0.1982
  Epoch [33/200], Batch [329/517], Loss: 0.3346
  Epoch [33/200], Batch [376/517], Loss: 0.3992
  Epoch [33/200], Batch [423/517], Loss: 0.1734
  Epoch [33/200], Batch [470/517], Loss: 0.2018
  Epoch [33/200], Batch [517/517], Loss: 0.1768
--- Epoch [33/200] complete. Average Training Loss: 0.2648 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [34/200], Batch [47/517], Loss: 0.2941
  Epoch [34/200], Batch [94/517], Loss: 0.4081
  Epoch [34/200], Batch [141/517], Loss: 0.3276
  Epoch [34/200], Batch [188/517], Loss: 0.3515
  Epoch [34/200], Batch [235/517], Loss: 0.2603
  Epoch [34/200], Batch [282/517], Loss: 0.2226
  Epoch [34/200], Batch [329/517], Loss: 0.1361
  Epoch [34/200], Batch [376/517], Loss: 0.3876
  Epoch [34/200], Batch [423/517], Loss: 0.4057
  Epoch [34/200], Batch [470/517], Loss: 0.2547
  Epoch [34/200], Batch [517/517], Loss: 0.1882
--- Epoch [34/200] complete. Average Training Loss: 0.2669 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [35/200], Batch [47/517], Loss: 0.4167
  Epoch [35/200], Batch [94/517], Loss: 0.1465
  Epoch [35/200], Batch [141/517], Loss: 0.4041
  Epoch [35/200], Batch [188/517], Loss: 0.4109
  Epoch [35/200], Batch [235/517], Loss: 0.1715
  Epoch [35/200], Batch [282/517], Loss: 0.3749
  Epoch [35/200], Batch [329/517], Loss: 0.3492
  Epoch [35/200], Batch [376/517], Loss: 0.3852
  Epoch [35/200], Batch [423/517], Loss: 0.4273
  Epoch [35/200], Batch [470/517], Loss: 0.1801
  Epoch [35/200], Batch [517/517], Loss: 0.1217
--- Epoch [35/200] complete. Average Training Loss: 0.2696 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [36/200], Batch [47/517], Loss: 0.4041
  Epoch [36/200], Batch [94/517], Loss: 0.3162
  Epoch [36/200], Batch [141/517], Loss: 0.4096
  Epoch [36/200], Batch [188/517], Loss: 0.2399
  Epoch [36/200], Batch [235/517], Loss: 0.3392
  Epoch [36/200], Batch [282/517], Loss: 0.2126
  Epoch [36/200], Batch [329/517], Loss: 0.3142
  Epoch [36/200], Batch [376/517], Loss: 0.2612
  Epoch [36/200], Batch [423/517], Loss: 0.3866
  Epoch [36/200], Batch [470/517], Loss: 0.1535
  Epoch [36/200], Batch [517/517], Loss: 0.1934
--- Epoch [36/200] complete. Average Training Loss: 0.2622 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [37/200], Batch [47/517], Loss: 0.1803
  Epoch [37/200], Batch [94/517], Loss: 0.3850
  Epoch [37/200], Batch [141/517], Loss: 0.1852
  Epoch [37/200], Batch [188/517], Loss: 0.1773
  Epoch [37/200], Batch [235/517], Loss: 0.2955
  Epoch [37/200], Batch [282/517], Loss: 0.4153
  Epoch [37/200], Batch [329/517], Loss: 0.2122
  Epoch [37/200], Batch [376/517], Loss: 0.3521
  Epoch [37/200], Batch [423/517], Loss: 0.2397
  Epoch [37/200], Batch [470/517], Loss: 0.2553
  Epoch [37/200], Batch [517/517], Loss: 0.1836
--- Epoch [37/200] complete. Average Training Loss: 0.2618 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [38/200], Batch [47/517], Loss: 0.1760
  Epoch [38/200], Batch [94/517], Loss: 0.2704
  Epoch [38/200], Batch [141/517], Loss: 0.2583
  Epoch [38/200], Batch [188/517], Loss: 0.1964
  Epoch [38/200], Batch [235/517], Loss: 0.3849
  Epoch [38/200], Batch [282/517], Loss: 0.3126
  Epoch [38/200], Batch [329/517], Loss: 0.1539
  Epoch [38/200], Batch [376/517], Loss: 0.4161
  Epoch [38/200], Batch [423/517], Loss: 0.3086
  Epoch [38/200], Batch [470/517], Loss: 0.1368
  Epoch [38/200], Batch [517/517], Loss: 0.1443
--- Epoch [38/200] complete. Average Training Loss: 0.2555 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [39/200], Batch [47/517], Loss: 0.2444
  Epoch [39/200], Batch [94/517], Loss: 0.1508
  Epoch [39/200], Batch [141/517], Loss: 0.1694
  Epoch [39/200], Batch [188/517], Loss: 0.3516
  Epoch [39/200], Batch [235/517], Loss: 0.1478
  Epoch [39/200], Batch [282/517], Loss: 0.0940
  Epoch [39/200], Batch [329/517], Loss: 0.2459
  Epoch [39/200], Batch [376/517], Loss: 0.2037
  Epoch [39/200], Batch [423/517], Loss: 0.4595
  Epoch [39/200], Batch [470/517], Loss: 0.1824
  Epoch [39/200], Batch [517/517], Loss: 0.2157
--- Epoch [39/200] complete. Average Training Loss: 0.2573 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [40/200], Batch [47/517], Loss: 0.2175
  Epoch [40/200], Batch [94/517], Loss: 0.3764
  Epoch [40/200], Batch [141/517], Loss: 0.1445
  Epoch [40/200], Batch [188/517], Loss: 0.2057
  Epoch [40/200], Batch [235/517], Loss: 0.4073
  Epoch [40/200], Batch [282/517], Loss: 0.4078
  Epoch [40/200], Batch [329/517], Loss: 0.1367
  Epoch [40/200], Batch [376/517], Loss: 0.1546
  Epoch [40/200], Batch [423/517], Loss: 0.1600
  Epoch [40/200], Batch [470/517], Loss: 0.2474
  Epoch [40/200], Batch [517/517], Loss: 0.1893
--- Epoch [40/200] complete. Average Training Loss: 0.2546 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [41/200], Batch [47/517], Loss: 0.1813
  Epoch [41/200], Batch [94/517], Loss: 0.2887
  Epoch [41/200], Batch [141/517], Loss: 0.2594
  Epoch [41/200], Batch [188/517], Loss: 0.1668
  Epoch [41/200], Batch [235/517], Loss: 0.1929
  Epoch [41/200], Batch [282/517], Loss: 0.1755
  Epoch [41/200], Batch [329/517], Loss: 0.2904
  Epoch [41/200], Batch [376/517], Loss: 0.2696
  Epoch [41/200], Batch [423/517], Loss: 0.1599
  Epoch [41/200], Batch [470/517], Loss: 0.1343
  Epoch [41/200], Batch [517/517], Loss: 0.3176
--- Epoch [41/200] complete. Average Training Loss: 0.2527 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [42/200], Batch [47/517], Loss: 0.1324
  Epoch [42/200], Batch [94/517], Loss: 0.2669
  Epoch [42/200], Batch [141/517], Loss: 0.2056
  Epoch [42/200], Batch [188/517], Loss: 0.4129
  Epoch [42/200], Batch [235/517], Loss: 0.3891
  Epoch [42/200], Batch [282/517], Loss: 0.1781
  Epoch [42/200], Batch [329/517], Loss: 0.2988
  Epoch [42/200], Batch [376/517], Loss: 0.1187
  Epoch [42/200], Batch [423/517], Loss: 0.1640
  Epoch [42/200], Batch [470/517], Loss: 0.2180
  Epoch [42/200], Batch [517/517], Loss: 0.3720
--- Epoch [42/200] complete. Average Training Loss: 0.2506 ---
--- Time taken for epoch: 314.09 seconds ---
  Epoch [43/200], Batch [47/517], Loss: 0.1799
  Epoch [43/200], Batch [94/517], Loss: 0.1903
  Epoch [43/200], Batch [141/517], Loss: 0.4016
  Epoch [43/200], Batch [188/517], Loss: 0.1202
  Epoch [43/200], Batch [235/517], Loss: 0.1524
  Epoch [43/200], Batch [282/517], Loss: 0.2342
  Epoch [43/200], Batch [329/517], Loss: 0.2193
  Epoch [43/200], Batch [376/517], Loss: 0.1582
  Epoch [43/200], Batch [423/517], Loss: 0.1577
  Epoch [43/200], Batch [470/517], Loss: 0.2581
  Epoch [43/200], Batch [517/517], Loss: 0.1882
--- Epoch [43/200] complete. Average Training Loss: 0.2473 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [44/200], Batch [47/517], Loss: 0.2020
  Epoch [44/200], Batch [94/517], Loss: 0.0874
  Epoch [44/200], Batch [141/517], Loss: 0.1160
  Epoch [44/200], Batch [188/517], Loss: 0.3779
  Epoch [44/200], Batch [235/517], Loss: 0.4004
  Epoch [44/200], Batch [282/517], Loss: 0.2302
  Epoch [44/200], Batch [329/517], Loss: 0.4390
  Epoch [44/200], Batch [376/517], Loss: 0.2088
  Epoch [44/200], Batch [423/517], Loss: 0.2386
  Epoch [44/200], Batch [470/517], Loss: 0.2023
  Epoch [44/200], Batch [517/517], Loss: 0.1664
--- Epoch [44/200] complete. Average Training Loss: 0.2512 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [45/200], Batch [47/517], Loss: 0.1530
  Epoch [45/200], Batch [94/517], Loss: 0.1704
  Epoch [45/200], Batch [141/517], Loss: 0.3884
  Epoch [45/200], Batch [188/517], Loss: 0.2346
  Epoch [45/200], Batch [235/517], Loss: 0.2407
  Epoch [45/200], Batch [282/517], Loss: 0.2175
  Epoch [45/200], Batch [329/517], Loss: 0.2056
  Epoch [45/200], Batch [376/517], Loss: 0.3161
  Epoch [45/200], Batch [423/517], Loss: 0.1431
  Epoch [45/200], Batch [470/517], Loss: 0.2089
  Epoch [45/200], Batch [517/517], Loss: 0.1105
--- Epoch [45/200] complete. Average Training Loss: 0.2512 ---
--- Time taken for epoch: 314.05 seconds ---
  Epoch [46/200], Batch [47/517], Loss: 0.3747
  Epoch [46/200], Batch [94/517], Loss: 0.1867
  Epoch [46/200], Batch [141/517], Loss: 0.2021
  Epoch [46/200], Batch [188/517], Loss: 0.1327
  Epoch [46/200], Batch [235/517], Loss: 0.4034
  Epoch [46/200], Batch [282/517], Loss: 0.1400
  Epoch [46/200], Batch [329/517], Loss: 0.2889
  Epoch [46/200], Batch [376/517], Loss: 0.4521
  Epoch [46/200], Batch [423/517], Loss: 0.0975
  Epoch [46/200], Batch [470/517], Loss: 0.1840
  Epoch [46/200], Batch [517/517], Loss: 0.4014
--- Epoch [46/200] complete. Average Training Loss: 0.2460 ---
--- Time taken for epoch: 314.04 seconds ---
  Epoch [47/200], Batch [47/517], Loss: 0.3115
  Epoch [47/200], Batch [94/517], Loss: 0.2379
  Epoch [47/200], Batch [141/517], Loss: 0.2535
  Epoch [47/200], Batch [188/517], Loss: 0.1345
  Epoch [47/200], Batch [235/517], Loss: 0.1845
  Epoch [47/200], Batch [282/517], Loss: 0.1991
  Epoch [47/200], Batch [329/517], Loss: 0.3944
  Epoch [47/200], Batch [376/517], Loss: 0.4046
  Epoch [47/200], Batch [423/517], Loss: 0.1249
  Epoch [47/200], Batch [470/517], Loss: 0.2239
  Epoch [47/200], Batch [517/517], Loss: 0.1939
--- Epoch [47/200] complete. Average Training Loss: 0.2522 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [48/200], Batch [47/517], Loss: 0.4497
  Epoch [48/200], Batch [94/517], Loss: 0.2749
  Epoch [48/200], Batch [141/517], Loss: 0.3088
  Epoch [48/200], Batch [188/517], Loss: 0.4048
  Epoch [48/200], Batch [235/517], Loss: 0.1654
  Epoch [48/200], Batch [282/517], Loss: 0.1623
  Epoch [48/200], Batch [329/517], Loss: 0.2299
  Epoch [48/200], Batch [376/517], Loss: 0.3446
  Epoch [48/200], Batch [423/517], Loss: 0.1441
  Epoch [48/200], Batch [470/517], Loss: 0.3080
  Epoch [48/200], Batch [517/517], Loss: 0.4444
--- Epoch [48/200] complete. Average Training Loss: 0.2517 ---
--- Time taken for epoch: 314.07 seconds ---
  Epoch [49/200], Batch [47/517], Loss: 0.4146
  Epoch [49/200], Batch [94/517], Loss: 0.2108
  Epoch [49/200], Batch [141/517], Loss: 0.2175
  Epoch [49/200], Batch [188/517], Loss: 0.2042
  Epoch [49/200], Batch [235/517], Loss: 0.1539
  Epoch [49/200], Batch [282/517], Loss: 0.2370
  Epoch [49/200], Batch [329/517], Loss: 0.2937
  Epoch [49/200], Batch [376/517], Loss: 0.1367
  Epoch [49/200], Batch [423/517], Loss: 0.2278
  Epoch [49/200], Batch [470/517], Loss: 0.2072
  Epoch [49/200], Batch [517/517], Loss: 0.2286
--- Epoch [49/200] complete. Average Training Loss: 0.2473 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [50/200], Batch [47/517], Loss: 0.4131
  Epoch [50/200], Batch [94/517], Loss: 0.1949
  Epoch [50/200], Batch [141/517], Loss: 0.1501
  Epoch [50/200], Batch [188/517], Loss: 0.1775
  Epoch [50/200], Batch [235/517], Loss: 0.2186
  Epoch [50/200], Batch [282/517], Loss: 0.4240
  Epoch [50/200], Batch [329/517], Loss: 0.1730
  Epoch [50/200], Batch [376/517], Loss: 0.1665
  Epoch [50/200], Batch [423/517], Loss: 0.2283
  Epoch [50/200], Batch [470/517], Loss: 0.1880
  Epoch [50/200], Batch [517/517], Loss: 0.4028
--- Epoch [50/200] complete. Average Training Loss: 0.2382 ---
--- Time taken for epoch: 313.80 seconds ---
 -- Updated Checkpoint: Saved model at 50 epochs.
  Epoch [51/200], Batch [47/517], Loss: 0.1338
  Epoch [51/200], Batch [94/517], Loss: 0.1644
  Epoch [51/200], Batch [141/517], Loss: 0.2735
  Epoch [51/200], Batch [188/517], Loss: 0.1789
  Epoch [51/200], Batch [235/517], Loss: 0.2075
  Epoch [51/200], Batch [282/517], Loss: 0.1411
  Epoch [51/200], Batch [329/517], Loss: 0.2343
  Epoch [51/200], Batch [376/517], Loss: 0.1401
  Epoch [51/200], Batch [423/517], Loss: 0.0913
  Epoch [51/200], Batch [470/517], Loss: 0.1623
  Epoch [51/200], Batch [517/517], Loss: 0.3889
--- Epoch [51/200] complete. Average Training Loss: 0.2371 ---
--- Time taken for epoch: 313.76 seconds ---
  Epoch [52/200], Batch [47/517], Loss: 0.1294
  Epoch [52/200], Batch [94/517], Loss: 0.4020
  Epoch [52/200], Batch [141/517], Loss: 0.2421
  Epoch [52/200], Batch [188/517], Loss: 0.1499
  Epoch [52/200], Batch [235/517], Loss: 0.1983
  Epoch [52/200], Batch [282/517], Loss: 0.3035
  Epoch [52/200], Batch [329/517], Loss: 0.3404
  Epoch [52/200], Batch [376/517], Loss: 0.1666
  Epoch [52/200], Batch [423/517], Loss: 0.2784
  Epoch [52/200], Batch [470/517], Loss: 0.2136
  Epoch [52/200], Batch [517/517], Loss: 0.1568
--- Epoch [52/200] complete. Average Training Loss: 0.2409 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [53/200], Batch [47/517], Loss: 0.1518
  Epoch [53/200], Batch [94/517], Loss: 0.1883
  Epoch [53/200], Batch [141/517], Loss: 0.1819
  Epoch [53/200], Batch [188/517], Loss: 0.1297
  Epoch [53/200], Batch [235/517], Loss: 0.3451
  Epoch [53/200], Batch [282/517], Loss: 0.1786
  Epoch [53/200], Batch [329/517], Loss: 0.2546
  Epoch [53/200], Batch [376/517], Loss: 0.1334
  Epoch [53/200], Batch [423/517], Loss: 0.2704
  Epoch [53/200], Batch [470/517], Loss: 0.1272
  Epoch [53/200], Batch [517/517], Loss: 0.4386
--- Epoch [53/200] complete. Average Training Loss: 0.2491 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [54/200], Batch [47/517], Loss: 0.4290
  Epoch [54/200], Batch [94/517], Loss: 0.1636
  Epoch [54/200], Batch [141/517], Loss: 0.2067
  Epoch [54/200], Batch [188/517], Loss: 0.2960
  Epoch [54/200], Batch [235/517], Loss: 0.2848
  Epoch [54/200], Batch [282/517], Loss: 0.1417
  Epoch [54/200], Batch [329/517], Loss: 0.1953
  Epoch [54/200], Batch [376/517], Loss: 0.3943
  Epoch [54/200], Batch [423/517], Loss: 0.4048
  Epoch [54/200], Batch [470/517], Loss: 0.2594
  Epoch [54/200], Batch [517/517], Loss: 0.3873
--- Epoch [54/200] complete. Average Training Loss: 0.2364 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [55/200], Batch [47/517], Loss: 0.3966
  Epoch [55/200], Batch [94/517], Loss: 0.1491
  Epoch [55/200], Batch [141/517], Loss: 0.1540
  Epoch [55/200], Batch [188/517], Loss: 0.1517
  Epoch [55/200], Batch [235/517], Loss: 0.2712
  Epoch [55/200], Batch [282/517], Loss: 0.1000
  Epoch [55/200], Batch [329/517], Loss: 0.3962
  Epoch [55/200], Batch [376/517], Loss: 0.1692
  Epoch [55/200], Batch [423/517], Loss: 0.1814
  Epoch [55/200], Batch [470/517], Loss: 0.1782
  Epoch [55/200], Batch [517/517], Loss: 0.1341
--- Epoch [55/200] complete. Average Training Loss: 0.2361 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [56/200], Batch [47/517], Loss: 0.3898
  Epoch [56/200], Batch [94/517], Loss: 0.2222
  Epoch [56/200], Batch [141/517], Loss: 0.1383
  Epoch [56/200], Batch [188/517], Loss: 0.2876
  Epoch [56/200], Batch [235/517], Loss: 0.1040
  Epoch [56/200], Batch [282/517], Loss: 0.1651
  Epoch [56/200], Batch [329/517], Loss: 0.3264
  Epoch [56/200], Batch [376/517], Loss: 0.4350
  Epoch [56/200], Batch [423/517], Loss: 0.2732
  Epoch [56/200], Batch [470/517], Loss: 0.0994
  Epoch [56/200], Batch [517/517], Loss: 0.1469
--- Epoch [56/200] complete. Average Training Loss: 0.2317 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [57/200], Batch [47/517], Loss: 0.2070
  Epoch [57/200], Batch [94/517], Loss: 0.2158
  Epoch [57/200], Batch [141/517], Loss: 0.1492
  Epoch [57/200], Batch [188/517], Loss: 0.1245
  Epoch [57/200], Batch [235/517], Loss: 0.1872
  Epoch [57/200], Batch [282/517], Loss: 0.2229
  Epoch [57/200], Batch [329/517], Loss: 0.3632
  Epoch [57/200], Batch [376/517], Loss: 0.1192
  Epoch [57/200], Batch [423/517], Loss: 0.2593
  Epoch [57/200], Batch [470/517], Loss: 0.1801
  Epoch [57/200], Batch [517/517], Loss: 0.1124
--- Epoch [57/200] complete. Average Training Loss: 0.2332 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [58/200], Batch [47/517], Loss: 0.3857
  Epoch [58/200], Batch [94/517], Loss: 0.3937
  Epoch [58/200], Batch [141/517], Loss: 0.3888
  Epoch [58/200], Batch [188/517], Loss: 0.4192
  Epoch [58/200], Batch [235/517], Loss: 0.1064
  Epoch [58/200], Batch [282/517], Loss: 0.1723
  Epoch [58/200], Batch [329/517], Loss: 0.1925
  Epoch [58/200], Batch [376/517], Loss: 0.3831
  Epoch [58/200], Batch [423/517], Loss: 0.1627
  Epoch [58/200], Batch [470/517], Loss: 0.2694
  Epoch [58/200], Batch [517/517], Loss: 0.3874
--- Epoch [58/200] complete. Average Training Loss: 0.2339 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [59/200], Batch [47/517], Loss: 0.1835
  Epoch [59/200], Batch [94/517], Loss: 0.1652
  Epoch [59/200], Batch [141/517], Loss: 0.2528
  Epoch [59/200], Batch [188/517], Loss: 0.1233
  Epoch [59/200], Batch [235/517], Loss: 0.1734
  Epoch [59/200], Batch [282/517], Loss: 0.1790
  Epoch [59/200], Batch [329/517], Loss: 0.2307
  Epoch [59/200], Batch [376/517], Loss: 0.2901
  Epoch [59/200], Batch [423/517], Loss: 0.1147
  Epoch [59/200], Batch [470/517], Loss: 0.1083
  Epoch [59/200], Batch [517/517], Loss: 0.2277
--- Epoch [59/200] complete. Average Training Loss: 0.2435 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [60/200], Batch [47/517], Loss: 0.1907
  Epoch [60/200], Batch [94/517], Loss: 0.1320
  Epoch [60/200], Batch [141/517], Loss: 0.2152
  Epoch [60/200], Batch [188/517], Loss: 0.1598
  Epoch [60/200], Batch [235/517], Loss: 0.2225
  Epoch [60/200], Batch [282/517], Loss: 0.2764
  Epoch [60/200], Batch [329/517], Loss: 0.2525
  Epoch [60/200], Batch [376/517], Loss: 0.2279
  Epoch [60/200], Batch [423/517], Loss: 0.1297
  Epoch [60/200], Batch [470/517], Loss: 0.1242
  Epoch [60/200], Batch [517/517], Loss: 0.1235
--- Epoch [60/200] complete. Average Training Loss: 0.2390 ---
--- Time taken for epoch: 313.77 seconds ---
  Epoch [61/200], Batch [47/517], Loss: 0.1933
  Epoch [61/200], Batch [94/517], Loss: 0.4069
  Epoch [61/200], Batch [141/517], Loss: 0.2019
  Epoch [61/200], Batch [188/517], Loss: 0.1965
  Epoch [61/200], Batch [235/517], Loss: 0.1891
  Epoch [61/200], Batch [282/517], Loss: 0.1907
  Epoch [61/200], Batch [329/517], Loss: 0.1160
  Epoch [61/200], Batch [376/517], Loss: 0.0910
  Epoch [61/200], Batch [423/517], Loss: 0.0861
  Epoch [61/200], Batch [470/517], Loss: 0.0981
  Epoch [61/200], Batch [517/517], Loss: 0.2073
--- Epoch [61/200] complete. Average Training Loss: 0.2331 ---
--- Time taken for epoch: 313.73 seconds ---
  Epoch [62/200], Batch [47/517], Loss: 0.1878
  Epoch [62/200], Batch [94/517], Loss: 0.2900
  Epoch [62/200], Batch [141/517], Loss: 0.2514
  Epoch [62/200], Batch [188/517], Loss: 0.1508
  Epoch [62/200], Batch [235/517], Loss: 0.1726
  Epoch [62/200], Batch [282/517], Loss: 0.2142
  Epoch [62/200], Batch [329/517], Loss: 0.2428
  Epoch [62/200], Batch [376/517], Loss: 0.3959
  Epoch [62/200], Batch [423/517], Loss: 0.3336
  Epoch [62/200], Batch [470/517], Loss: 0.1851
  Epoch [62/200], Batch [517/517], Loss: 0.1269
--- Epoch [62/200] complete. Average Training Loss: 0.2302 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [63/200], Batch [47/517], Loss: 0.2133
  Epoch [63/200], Batch [94/517], Loss: 0.1628
  Epoch [63/200], Batch [141/517], Loss: 0.1543
  Epoch [63/200], Batch [188/517], Loss: 0.1728
  Epoch [63/200], Batch [235/517], Loss: 0.1612
  Epoch [63/200], Batch [282/517], Loss: 0.2291
  Epoch [63/200], Batch [329/517], Loss: 0.1536
  Epoch [63/200], Batch [376/517], Loss: 0.1103
  Epoch [63/200], Batch [423/517], Loss: 0.3711
  Epoch [63/200], Batch [470/517], Loss: 0.3724
  Epoch [63/200], Batch [517/517], Loss: 0.1591
--- Epoch [63/200] complete. Average Training Loss: 0.2316 ---
--- Time taken for epoch: 313.79 seconds ---
  Epoch [64/200], Batch [47/517], Loss: 0.4211
  Epoch [64/200], Batch [94/517], Loss: 0.1404
  Epoch [64/200], Batch [141/517], Loss: 0.1778
  Epoch [64/200], Batch [188/517], Loss: 0.3883
  Epoch [64/200], Batch [235/517], Loss: 0.2141
  Epoch [64/200], Batch [282/517], Loss: 0.1329
  Epoch [64/200], Batch [329/517], Loss: 0.2090
  Epoch [64/200], Batch [376/517], Loss: 0.4032
  Epoch [64/200], Batch [423/517], Loss: 0.1453
  Epoch [64/200], Batch [470/517], Loss: 0.1606
  Epoch [64/200], Batch [517/517], Loss: 0.4298
--- Epoch [64/200] complete. Average Training Loss: 0.2363 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [65/200], Batch [47/517], Loss: 0.2528
  Epoch [65/200], Batch [94/517], Loss: 0.1469
  Epoch [65/200], Batch [141/517], Loss: 0.1358
  Epoch [65/200], Batch [188/517], Loss: 0.2353
  Epoch [65/200], Batch [235/517], Loss: 0.2079
  Epoch [65/200], Batch [282/517], Loss: 0.3911
  Epoch [65/200], Batch [329/517], Loss: 0.1814
  Epoch [65/200], Batch [376/517], Loss: 0.1585
  Epoch [65/200], Batch [423/517], Loss: 0.1774
  Epoch [65/200], Batch [470/517], Loss: 0.1954
  Epoch [65/200], Batch [517/517], Loss: 0.3967
--- Epoch [65/200] complete. Average Training Loss: 0.2308 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [66/200], Batch [47/517], Loss: 0.1412
  Epoch [66/200], Batch [94/517], Loss: 0.2707
  Epoch [66/200], Batch [141/517], Loss: 0.1730
  Epoch [66/200], Batch [188/517], Loss: 0.1709
  Epoch [66/200], Batch [235/517], Loss: 0.1574
  Epoch [66/200], Batch [282/517], Loss: 0.1490
  Epoch [66/200], Batch [329/517], Loss: 0.1089
  Epoch [66/200], Batch [376/517], Loss: 0.1806
  Epoch [66/200], Batch [423/517], Loss: 0.2916
  Epoch [66/200], Batch [470/517], Loss: 0.2088
  Epoch [66/200], Batch [517/517], Loss: 0.1598
--- Epoch [66/200] complete. Average Training Loss: 0.2224 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [67/200], Batch [47/517], Loss: 0.2649
  Epoch [67/200], Batch [94/517], Loss: 0.4082
  Epoch [67/200], Batch [141/517], Loss: 0.2071
  Epoch [67/200], Batch [188/517], Loss: 0.2462
  Epoch [67/200], Batch [235/517], Loss: 0.0999
  Epoch [67/200], Batch [282/517], Loss: 0.3969
  Epoch [67/200], Batch [329/517], Loss: 0.3312
  Epoch [67/200], Batch [376/517], Loss: 0.1736
  Epoch [67/200], Batch [423/517], Loss: 0.2066
  Epoch [67/200], Batch [470/517], Loss: 0.4028
  Epoch [67/200], Batch [517/517], Loss: 0.1567
--- Epoch [67/200] complete. Average Training Loss: 0.2282 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [68/200], Batch [47/517], Loss: 0.3983
  Epoch [68/200], Batch [94/517], Loss: 0.1680
  Epoch [68/200], Batch [141/517], Loss: 0.3801
  Epoch [68/200], Batch [188/517], Loss: 0.3272
  Epoch [68/200], Batch [235/517], Loss: 0.4025
  Epoch [68/200], Batch [282/517], Loss: 0.1907
  Epoch [68/200], Batch [329/517], Loss: 0.1187
  Epoch [68/200], Batch [376/517], Loss: 0.1295
  Epoch [68/200], Batch [423/517], Loss: 0.2415
  Epoch [68/200], Batch [470/517], Loss: 0.3889
  Epoch [68/200], Batch [517/517], Loss: 0.4571
--- Epoch [68/200] complete. Average Training Loss: 0.2256 ---
--- Time taken for epoch: 313.75 seconds ---
  Epoch [69/200], Batch [47/517], Loss: 0.3979
  Epoch [69/200], Batch [94/517], Loss: 0.1930
  Epoch [69/200], Batch [141/517], Loss: 0.1932
  Epoch [69/200], Batch [188/517], Loss: 0.1800
  Epoch [69/200], Batch [235/517], Loss: 0.1296
  Epoch [69/200], Batch [282/517], Loss: 0.2673
  Epoch [69/200], Batch [329/517], Loss: 0.1869
  Epoch [69/200], Batch [376/517], Loss: 0.2692
  Epoch [69/200], Batch [423/517], Loss: 0.3325
  Epoch [69/200], Batch [470/517], Loss: 0.2384
  Epoch [69/200], Batch [517/517], Loss: 0.2073
--- Epoch [69/200] complete. Average Training Loss: 0.2361 ---
--- Time taken for epoch: 313.78 seconds ---
  Epoch [70/200], Batch [47/517], Loss: 0.2168
  Epoch [70/200], Batch [94/517], Loss: 0.3757
  Epoch [70/200], Batch [141/517], Loss: 0.2221
  Epoch [70/200], Batch [188/517], Loss: 0.3163
  Epoch [70/200], Batch [235/517], Loss: 0.1195
  Epoch [70/200], Batch [282/517], Loss: 0.1623
  Epoch [70/200], Batch [329/517], Loss: 0.2230
  Epoch [70/200], Batch [376/517], Loss: 0.2172
  Epoch [70/200], Batch [423/517], Loss: 0.1991
  Epoch [70/200], Batch [470/517], Loss: 0.2338
  Epoch [70/200], Batch [517/517], Loss: 0.3975
--- Epoch [70/200] complete. Average Training Loss: 0.2277 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [71/200], Batch [47/517], Loss: 0.1943
  Epoch [71/200], Batch [94/517], Loss: 0.3923
  Epoch [71/200], Batch [141/517], Loss: 0.1950
  Epoch [71/200], Batch [188/517], Loss: 0.1344
  Epoch [71/200], Batch [235/517], Loss: 0.3079
  Epoch [71/200], Batch [282/517], Loss: 0.0792
  Epoch [71/200], Batch [329/517], Loss: 0.1535
  Epoch [71/200], Batch [376/517], Loss: 0.1481
  Epoch [71/200], Batch [423/517], Loss: 0.3953
  Epoch [71/200], Batch [470/517], Loss: 0.1208
  Epoch [71/200], Batch [517/517], Loss: 0.1089
--- Epoch [71/200] complete. Average Training Loss: 0.2182 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [72/200], Batch [47/517], Loss: 0.1742
  Epoch [72/200], Batch [94/517], Loss: 0.2698
  Epoch [72/200], Batch [141/517], Loss: 0.4052
  Epoch [72/200], Batch [188/517], Loss: 0.1698
  Epoch [72/200], Batch [235/517], Loss: 0.1688
  Epoch [72/200], Batch [282/517], Loss: 0.1335
  Epoch [72/200], Batch [329/517], Loss: 0.1758
  Epoch [72/200], Batch [376/517], Loss: 0.1302
  Epoch [72/200], Batch [423/517], Loss: 0.1592
  Epoch [72/200], Batch [470/517], Loss: 0.3156
  Epoch [72/200], Batch [517/517], Loss: 0.4088
--- Epoch [72/200] complete. Average Training Loss: 0.2289 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [73/200], Batch [47/517], Loss: 0.3879
  Epoch [73/200], Batch [94/517], Loss: 0.1328
  Epoch [73/200], Batch [141/517], Loss: 0.1114
  Epoch [73/200], Batch [188/517], Loss: 0.1258
  Epoch [73/200], Batch [235/517], Loss: 0.2469
  Epoch [73/200], Batch [282/517], Loss: 0.1586
  Epoch [73/200], Batch [329/517], Loss: 0.1459
  Epoch [73/200], Batch [376/517], Loss: 0.4048
  Epoch [73/200], Batch [423/517], Loss: 0.1966
  Epoch [73/200], Batch [470/517], Loss: 0.4232
  Epoch [73/200], Batch [517/517], Loss: 0.3859
--- Epoch [73/200] complete. Average Training Loss: 0.2245 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [74/200], Batch [47/517], Loss: 0.2273
  Epoch [74/200], Batch [94/517], Loss: 0.1049
  Epoch [74/200], Batch [141/517], Loss: 0.2062
  Epoch [74/200], Batch [188/517], Loss: 0.2026
  Epoch [74/200], Batch [235/517], Loss: 0.1036
  Epoch [74/200], Batch [282/517], Loss: 0.1602
  Epoch [74/200], Batch [329/517], Loss: 0.1835
  Epoch [74/200], Batch [376/517], Loss: 0.2671
  Epoch [74/200], Batch [423/517], Loss: 0.2592
  Epoch [74/200], Batch [470/517], Loss: 0.3920
  Epoch [74/200], Batch [517/517], Loss: 0.3833
--- Epoch [74/200] complete. Average Training Loss: 0.2299 ---
--- Time taken for epoch: 313.79 seconds ---
  Epoch [75/200], Batch [47/517], Loss: 0.1246
  Epoch [75/200], Batch [94/517], Loss: 0.4079
  Epoch [75/200], Batch [141/517], Loss: 0.1241
  Epoch [75/200], Batch [188/517], Loss: 0.1137
  Epoch [75/200], Batch [235/517], Loss: 0.1541
  Epoch [75/200], Batch [282/517], Loss: 0.3840
  Epoch [75/200], Batch [329/517], Loss: 0.2107
  Epoch [75/200], Batch [376/517], Loss: 0.1758
  Epoch [75/200], Batch [423/517], Loss: 0.3981
  Epoch [75/200], Batch [470/517], Loss: 0.2247
  Epoch [75/200], Batch [517/517], Loss: 0.3885
--- Epoch [75/200] complete. Average Training Loss: 0.2232 ---
--- Time taken for epoch: 313.81 seconds ---
 -- Updated Checkpoint: Saved model at 75 epochs.
  Epoch [76/200], Batch [47/517], Loss: 0.1232
  Epoch [76/200], Batch [94/517], Loss: 0.1442
  Epoch [76/200], Batch [141/517], Loss: 0.1630
  Epoch [76/200], Batch [188/517], Loss: 0.2706
  Epoch [76/200], Batch [235/517], Loss: 0.1933
  Epoch [76/200], Batch [282/517], Loss: 0.0960
  Epoch [76/200], Batch [329/517], Loss: 0.1853
  Epoch [76/200], Batch [376/517], Loss: 0.1598
  Epoch [76/200], Batch [423/517], Loss: 0.4040
  Epoch [76/200], Batch [470/517], Loss: 0.1116
  Epoch [76/200], Batch [517/517], Loss: 0.0932
--- Epoch [76/200] complete. Average Training Loss: 0.2247 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [77/200], Batch [47/517], Loss: 0.2445
  Epoch [77/200], Batch [94/517], Loss: 0.3778
  Epoch [77/200], Batch [141/517], Loss: 0.1540
  Epoch [77/200], Batch [188/517], Loss: 0.2074
  Epoch [77/200], Batch [235/517], Loss: 0.1923
  Epoch [77/200], Batch [282/517], Loss: 0.2423
  Epoch [77/200], Batch [329/517], Loss: 0.0936
  Epoch [77/200], Batch [376/517], Loss: 0.1496
  Epoch [77/200], Batch [423/517], Loss: 0.3722
  Epoch [77/200], Batch [470/517], Loss: 0.1395
  Epoch [77/200], Batch [517/517], Loss: 0.1025
--- Epoch [77/200] complete. Average Training Loss: 0.2191 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [78/200], Batch [47/517], Loss: 0.3874
  Epoch [78/200], Batch [94/517], Loss: 0.3674
  Epoch [78/200], Batch [141/517], Loss: 0.3719
  Epoch [78/200], Batch [188/517], Loss: 0.3736
  Epoch [78/200], Batch [235/517], Loss: 0.4030
  Epoch [78/200], Batch [282/517], Loss: 0.1622
  Epoch [78/200], Batch [329/517], Loss: 0.1376
  Epoch [78/200], Batch [376/517], Loss: 0.3882
  Epoch [78/200], Batch [423/517], Loss: 0.1308
  Epoch [78/200], Batch [470/517], Loss: 0.4009
  Epoch [78/200], Batch [517/517], Loss: 0.3243
--- Epoch [78/200] complete. Average Training Loss: 0.2206 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [79/200], Batch [47/517], Loss: 0.1524
  Epoch [79/200], Batch [94/517], Loss: 0.1321
  Epoch [79/200], Batch [141/517], Loss: 0.1918
  Epoch [79/200], Batch [188/517], Loss: 0.3897
  Epoch [79/200], Batch [235/517], Loss: 0.3867
  Epoch [79/200], Batch [282/517], Loss: 0.1460
  Epoch [79/200], Batch [329/517], Loss: 0.1643
  Epoch [79/200], Batch [376/517], Loss: 0.1022
  Epoch [79/200], Batch [423/517], Loss: 0.1880
  Epoch [79/200], Batch [470/517], Loss: 0.1564
  Epoch [79/200], Batch [517/517], Loss: 0.2034
--- Epoch [79/200] complete. Average Training Loss: 0.2275 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [80/200], Batch [47/517], Loss: 0.2177
  Epoch [80/200], Batch [94/517], Loss: 0.2768
  Epoch [80/200], Batch [141/517], Loss: 0.3888
  Epoch [80/200], Batch [188/517], Loss: 0.1451
  Epoch [80/200], Batch [235/517], Loss: 0.3832
  Epoch [80/200], Batch [282/517], Loss: 0.3931
  Epoch [80/200], Batch [329/517], Loss: 0.3647
  Epoch [80/200], Batch [376/517], Loss: 0.1633
  Epoch [80/200], Batch [423/517], Loss: 0.1443
  Epoch [80/200], Batch [470/517], Loss: 0.1786
  Epoch [80/200], Batch [517/517], Loss: 0.1642
--- Epoch [80/200] complete. Average Training Loss: 0.2161 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [81/200], Batch [47/517], Loss: 0.1773
  Epoch [81/200], Batch [94/517], Loss: 0.1014
  Epoch [81/200], Batch [141/517], Loss: 0.1274
  Epoch [81/200], Batch [188/517], Loss: 0.2275
  Epoch [81/200], Batch [235/517], Loss: 0.0972
  Epoch [81/200], Batch [282/517], Loss: 0.1122
  Epoch [81/200], Batch [329/517], Loss: 0.2167
  Epoch [81/200], Batch [376/517], Loss: 0.1211
  Epoch [81/200], Batch [423/517], Loss: 0.1319
  Epoch [81/200], Batch [470/517], Loss: 0.1829
  Epoch [81/200], Batch [517/517], Loss: 0.1809
--- Epoch [81/200] complete. Average Training Loss: 0.2194 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [82/200], Batch [47/517], Loss: 0.1442
  Epoch [82/200], Batch [94/517], Loss: 0.1856
  Epoch [82/200], Batch [141/517], Loss: 0.2867
  Epoch [82/200], Batch [188/517], Loss: 0.2122
  Epoch [82/200], Batch [235/517], Loss: 0.1438
  Epoch [82/200], Batch [282/517], Loss: 0.1547
  Epoch [82/200], Batch [329/517], Loss: 0.3823
  Epoch [82/200], Batch [376/517], Loss: 0.4111
  Epoch [82/200], Batch [423/517], Loss: 0.0978
  Epoch [82/200], Batch [470/517], Loss: 0.1597
  Epoch [82/200], Batch [517/517], Loss: 0.1677
--- Epoch [82/200] complete. Average Training Loss: 0.2238 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [83/200], Batch [47/517], Loss: 0.4092
  Epoch [83/200], Batch [94/517], Loss: 0.1008
  Epoch [83/200], Batch [141/517], Loss: 0.1146
  Epoch [83/200], Batch [188/517], Loss: 0.3847
  Epoch [83/200], Batch [235/517], Loss: 0.2024
  Epoch [83/200], Batch [282/517], Loss: 0.1861
  Epoch [83/200], Batch [329/517], Loss: 0.4173
  Epoch [83/200], Batch [376/517], Loss: 0.1377
  Epoch [83/200], Batch [423/517], Loss: 0.3954
  Epoch [83/200], Batch [470/517], Loss: 0.1773
  Epoch [83/200], Batch [517/517], Loss: 0.2807
--- Epoch [83/200] complete. Average Training Loss: 0.2237 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [84/200], Batch [47/517], Loss: 0.2451
  Epoch [84/200], Batch [94/517], Loss: 0.0958
  Epoch [84/200], Batch [141/517], Loss: 0.1533
  Epoch [84/200], Batch [188/517], Loss: 0.4032
  Epoch [84/200], Batch [235/517], Loss: 0.1088
  Epoch [84/200], Batch [282/517], Loss: 0.1973
  Epoch [84/200], Batch [329/517], Loss: 0.1975
  Epoch [84/200], Batch [376/517], Loss: 0.4120
  Epoch [84/200], Batch [423/517], Loss: 0.1590
  Epoch [84/200], Batch [470/517], Loss: 0.3226
  Epoch [84/200], Batch [517/517], Loss: 0.3962
--- Epoch [84/200] complete. Average Training Loss: 0.2265 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [85/200], Batch [47/517], Loss: 0.3748
  Epoch [85/200], Batch [94/517], Loss: 0.1388
  Epoch [85/200], Batch [141/517], Loss: 0.1387
  Epoch [85/200], Batch [188/517], Loss: 0.1907
  Epoch [85/200], Batch [235/517], Loss: 0.1661
  Epoch [85/200], Batch [282/517], Loss: 0.2131
  Epoch [85/200], Batch [329/517], Loss: 0.2711
  Epoch [85/200], Batch [376/517], Loss: 0.3798
  Epoch [85/200], Batch [423/517], Loss: 0.1370
  Epoch [85/200], Batch [470/517], Loss: 0.1930
  Epoch [85/200], Batch [517/517], Loss: 0.1162
--- Epoch [85/200] complete. Average Training Loss: 0.2106 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [86/200], Batch [47/517], Loss: 0.1617
  Epoch [86/200], Batch [94/517], Loss: 0.4027
  Epoch [86/200], Batch [141/517], Loss: 0.1705
  Epoch [86/200], Batch [188/517], Loss: 0.3973
  Epoch [86/200], Batch [235/517], Loss: 0.1416
  Epoch [86/200], Batch [282/517], Loss: 0.1658
  Epoch [86/200], Batch [329/517], Loss: 0.1181
  Epoch [86/200], Batch [376/517], Loss: 0.2173
  Epoch [86/200], Batch [423/517], Loss: 0.1550
  Epoch [86/200], Batch [470/517], Loss: 0.2049
  Epoch [86/200], Batch [517/517], Loss: 0.2190
--- Epoch [86/200] complete. Average Training Loss: 0.2205 ---
--- Time taken for epoch: 313.77 seconds ---
  Epoch [87/200], Batch [47/517], Loss: 0.1654
  Epoch [87/200], Batch [94/517], Loss: 0.2978
  Epoch [87/200], Batch [141/517], Loss: 0.1209
  Epoch [87/200], Batch [188/517], Loss: 0.1443
  Epoch [87/200], Batch [235/517], Loss: 0.3955
  Epoch [87/200], Batch [282/517], Loss: 0.2000
  Epoch [87/200], Batch [329/517], Loss: 0.1294
  Epoch [87/200], Batch [376/517], Loss: 0.1265
  Epoch [87/200], Batch [423/517], Loss: 0.1805
  Epoch [87/200], Batch [470/517], Loss: 0.4071
  Epoch [87/200], Batch [517/517], Loss: 0.1589
--- Epoch [87/200] complete. Average Training Loss: 0.2077 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [88/200], Batch [47/517], Loss: 0.4145
  Epoch [88/200], Batch [94/517], Loss: 0.2640
  Epoch [88/200], Batch [141/517], Loss: 0.0926
  Epoch [88/200], Batch [188/517], Loss: 0.2012
  Epoch [88/200], Batch [235/517], Loss: 0.1875
  Epoch [88/200], Batch [282/517], Loss: 0.4226
  Epoch [88/200], Batch [329/517], Loss: 0.1392
  Epoch [88/200], Batch [376/517], Loss: 0.1767
  Epoch [88/200], Batch [423/517], Loss: 0.4179
  Epoch [88/200], Batch [470/517], Loss: 0.1534
  Epoch [88/200], Batch [517/517], Loss: 0.3977
--- Epoch [88/200] complete. Average Training Loss: 0.2181 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [89/200], Batch [47/517], Loss: 0.1489
  Epoch [89/200], Batch [94/517], Loss: 0.4152
  Epoch [89/200], Batch [141/517], Loss: 0.1487
  Epoch [89/200], Batch [188/517], Loss: 0.1733
  Epoch [89/200], Batch [235/517], Loss: 0.3851
  Epoch [89/200], Batch [282/517], Loss: 0.3627
  Epoch [89/200], Batch [329/517], Loss: 0.1586
  Epoch [89/200], Batch [376/517], Loss: 0.3981
  Epoch [89/200], Batch [423/517], Loss: 0.2160
  Epoch [89/200], Batch [470/517], Loss: 0.3495
  Epoch [89/200], Batch [517/517], Loss: 0.3969
--- Epoch [89/200] complete. Average Training Loss: 0.2182 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [90/200], Batch [47/517], Loss: 0.2233
  Epoch [90/200], Batch [94/517], Loss: 0.1768
  Epoch [90/200], Batch [141/517], Loss: 0.1049
  Epoch [90/200], Batch [188/517], Loss: 0.1302
  Epoch [90/200], Batch [235/517], Loss: 0.1206
  Epoch [90/200], Batch [282/517], Loss: 0.1926
  Epoch [90/200], Batch [329/517], Loss: 0.2632
  Epoch [90/200], Batch [376/517], Loss: 0.2432
  Epoch [90/200], Batch [423/517], Loss: 0.1967
  Epoch [90/200], Batch [470/517], Loss: 0.2002
  Epoch [90/200], Batch [517/517], Loss: 0.4090
--- Epoch [90/200] complete. Average Training Loss: 0.2109 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [91/200], Batch [47/517], Loss: 0.1590
  Epoch [91/200], Batch [94/517], Loss: 0.0919
  Epoch [91/200], Batch [141/517], Loss: 0.3745
  Epoch [91/200], Batch [188/517], Loss: 0.1272
  Epoch [91/200], Batch [235/517], Loss: 0.1695
  Epoch [91/200], Batch [282/517], Loss: 0.3735
  Epoch [91/200], Batch [329/517], Loss: 0.4103
  Epoch [91/200], Batch [376/517], Loss: 0.1195
  Epoch [91/200], Batch [423/517], Loss: 0.2665
  Epoch [91/200], Batch [470/517], Loss: 0.1393
  Epoch [91/200], Batch [517/517], Loss: 0.2071
--- Epoch [91/200] complete. Average Training Loss: 0.2132 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [92/200], Batch [47/517], Loss: 0.2230
  Epoch [92/200], Batch [94/517], Loss: 0.2720
  Epoch [92/200], Batch [141/517], Loss: 0.0919
  Epoch [92/200], Batch [188/517], Loss: 0.4079
  Epoch [92/200], Batch [235/517], Loss: 0.1984
  Epoch [92/200], Batch [282/517], Loss: 0.1677
  Epoch [92/200], Batch [329/517], Loss: 0.1327
  Epoch [92/200], Batch [376/517], Loss: 0.3816
  Epoch [92/200], Batch [423/517], Loss: 0.4097
  Epoch [92/200], Batch [470/517], Loss: 0.1014
  Epoch [92/200], Batch [517/517], Loss: 0.4117
--- Epoch [92/200] complete. Average Training Loss: 0.2170 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [93/200], Batch [47/517], Loss: 0.1651
  Epoch [93/200], Batch [94/517], Loss: 0.1647
  Epoch [93/200], Batch [141/517], Loss: 0.2365
  Epoch [93/200], Batch [188/517], Loss: 0.1425
  Epoch [93/200], Batch [235/517], Loss: 0.2932
  Epoch [93/200], Batch [282/517], Loss: 0.4022
  Epoch [93/200], Batch [329/517], Loss: 0.2449
  Epoch [93/200], Batch [376/517], Loss: 0.1772
  Epoch [93/200], Batch [423/517], Loss: 0.1444
  Epoch [93/200], Batch [470/517], Loss: 0.3932
  Epoch [93/200], Batch [517/517], Loss: 0.1092
--- Epoch [93/200] complete. Average Training Loss: 0.2147 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [94/200], Batch [47/517], Loss: 0.1461
  Epoch [94/200], Batch [94/517], Loss: 0.1126
  Epoch [94/200], Batch [141/517], Loss: 0.1818
  Epoch [94/200], Batch [188/517], Loss: 0.0847
  Epoch [94/200], Batch [235/517], Loss: 0.1241
  Epoch [94/200], Batch [282/517], Loss: 0.1408
  Epoch [94/200], Batch [329/517], Loss: 0.1760
  Epoch [94/200], Batch [376/517], Loss: 0.3904
  Epoch [94/200], Batch [423/517], Loss: 0.2411
  Epoch [94/200], Batch [470/517], Loss: 0.0899
  Epoch [94/200], Batch [517/517], Loss: 0.2756
--- Epoch [94/200] complete. Average Training Loss: 0.2068 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [95/200], Batch [47/517], Loss: 0.1319
  Epoch [95/200], Batch [94/517], Loss: 0.3958
  Epoch [95/200], Batch [141/517], Loss: 0.1300
  Epoch [95/200], Batch [188/517], Loss: 0.1126
  Epoch [95/200], Batch [235/517], Loss: 0.4026
  Epoch [95/200], Batch [282/517], Loss: 0.1801
  Epoch [95/200], Batch [329/517], Loss: 0.1837
  Epoch [95/200], Batch [376/517], Loss: 0.1294
  Epoch [95/200], Batch [423/517], Loss: 0.2069
  Epoch [95/200], Batch [470/517], Loss: 0.2974
  Epoch [95/200], Batch [517/517], Loss: 0.3759
--- Epoch [95/200] complete. Average Training Loss: 0.2098 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [96/200], Batch [47/517], Loss: 0.2110
  Epoch [96/200], Batch [94/517], Loss: 0.1592
  Epoch [96/200], Batch [141/517], Loss: 0.1522
  Epoch [96/200], Batch [188/517], Loss: 0.1753
  Epoch [96/200], Batch [235/517], Loss: 0.0986
  Epoch [96/200], Batch [282/517], Loss: 0.3800
  Epoch [96/200], Batch [329/517], Loss: 0.2074
  Epoch [96/200], Batch [376/517], Loss: 0.3941
  Epoch [96/200], Batch [423/517], Loss: 0.0999
  Epoch [96/200], Batch [470/517], Loss: 0.1609
  Epoch [96/200], Batch [517/517], Loss: 0.3860
--- Epoch [96/200] complete. Average Training Loss: 0.2066 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [97/200], Batch [47/517], Loss: 0.3964
  Epoch [97/200], Batch [94/517], Loss: 0.1755
  Epoch [97/200], Batch [141/517], Loss: 0.1882
  Epoch [97/200], Batch [188/517], Loss: 0.1881
  Epoch [97/200], Batch [235/517], Loss: 0.3993
  Epoch [97/200], Batch [282/517], Loss: 0.1026
  Epoch [97/200], Batch [329/517], Loss: 0.0926
  Epoch [97/200], Batch [376/517], Loss: 0.2382
  Epoch [97/200], Batch [423/517], Loss: 0.4031
  Epoch [97/200], Batch [470/517], Loss: 0.3365
  Epoch [97/200], Batch [517/517], Loss: 0.3379
--- Epoch [97/200] complete. Average Training Loss: 0.2227 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [98/200], Batch [47/517], Loss: 0.1448
  Epoch [98/200], Batch [94/517], Loss: 0.1797
  Epoch [98/200], Batch [141/517], Loss: 0.1779
  Epoch [98/200], Batch [188/517], Loss: 0.4299
  Epoch [98/200], Batch [235/517], Loss: 0.3889
  Epoch [98/200], Batch [282/517], Loss: 0.1689
  Epoch [98/200], Batch [329/517], Loss: 0.1824
  Epoch [98/200], Batch [376/517], Loss: 0.3565
  Epoch [98/200], Batch [423/517], Loss: 0.1655
  Epoch [98/200], Batch [470/517], Loss: 0.1400
  Epoch [98/200], Batch [517/517], Loss: 0.1160
--- Epoch [98/200] complete. Average Training Loss: 0.2260 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [99/200], Batch [47/517], Loss: 0.2255
  Epoch [99/200], Batch [94/517], Loss: 0.1596
  Epoch [99/200], Batch [141/517], Loss: 0.3903
  Epoch [99/200], Batch [188/517], Loss: 0.2286
  Epoch [99/200], Batch [235/517], Loss: 0.0772
  Epoch [99/200], Batch [282/517], Loss: 0.4153
  Epoch [99/200], Batch [329/517], Loss: 0.3832
  Epoch [99/200], Batch [376/517], Loss: 0.1412
  Epoch [99/200], Batch [423/517], Loss: 0.3848
  Epoch [99/200], Batch [470/517], Loss: 0.2404
  Epoch [99/200], Batch [517/517], Loss: 0.1475
--- Epoch [99/200] complete. Average Training Loss: 0.2097 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [100/200], Batch [47/517], Loss: 0.1782
  Epoch [100/200], Batch [94/517], Loss: 0.1508
  Epoch [100/200], Batch [141/517], Loss: 0.3886
  Epoch [100/200], Batch [188/517], Loss: 0.1062
  Epoch [100/200], Batch [235/517], Loss: 0.3859
  Epoch [100/200], Batch [282/517], Loss: 0.1170
  Epoch [100/200], Batch [329/517], Loss: 0.1341
  Epoch [100/200], Batch [376/517], Loss: 0.1099
  Epoch [100/200], Batch [423/517], Loss: 0.3885
  Epoch [100/200], Batch [470/517], Loss: 0.3791
  Epoch [100/200], Batch [517/517], Loss: 0.1507
--- Epoch [100/200] complete. Average Training Loss: 0.2072 ---
--- Time taken for epoch: 313.89 seconds ---
 -- Updated Checkpoint: Saved model at 100 epochs.
  Epoch [101/200], Batch [47/517], Loss: 0.1160
  Epoch [101/200], Batch [94/517], Loss: 0.0780
  Epoch [101/200], Batch [141/517], Loss: 0.1984
  Epoch [101/200], Batch [188/517], Loss: 0.2059
  Epoch [101/200], Batch [235/517], Loss: 0.1290
  Epoch [101/200], Batch [282/517], Loss: 0.1203
  Epoch [101/200], Batch [329/517], Loss: 0.0839
  Epoch [101/200], Batch [376/517], Loss: 0.3903
  Epoch [101/200], Batch [423/517], Loss: 0.1618
  Epoch [101/200], Batch [470/517], Loss: 0.1681
  Epoch [101/200], Batch [517/517], Loss: 0.3715
--- Epoch [101/200] complete. Average Training Loss: 0.1969 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [102/200], Batch [47/517], Loss: 0.3397
  Epoch [102/200], Batch [94/517], Loss: 0.0923
  Epoch [102/200], Batch [141/517], Loss: 0.1197
  Epoch [102/200], Batch [188/517], Loss: 0.4032
  Epoch [102/200], Batch [235/517], Loss: 0.0866
  Epoch [102/200], Batch [282/517], Loss: 0.1017
  Epoch [102/200], Batch [329/517], Loss: 0.1655
  Epoch [102/200], Batch [376/517], Loss: 0.1314
  Epoch [102/200], Batch [423/517], Loss: 0.4123
  Epoch [102/200], Batch [470/517], Loss: 0.1146
  Epoch [102/200], Batch [517/517], Loss: 0.1423
--- Epoch [102/200] complete. Average Training Loss: 0.2025 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [103/200], Batch [47/517], Loss: 0.1131
  Epoch [103/200], Batch [94/517], Loss: 0.1620
  Epoch [103/200], Batch [141/517], Loss: 0.2030
  Epoch [103/200], Batch [188/517], Loss: 0.2616
  Epoch [103/200], Batch [235/517], Loss: 0.1668
  Epoch [103/200], Batch [282/517], Loss: 0.4109
  Epoch [103/200], Batch [329/517], Loss: 0.1426
  Epoch [103/200], Batch [376/517], Loss: 0.1513
  Epoch [103/200], Batch [423/517], Loss: 0.1268
  Epoch [103/200], Batch [470/517], Loss: 0.1246
  Epoch [103/200], Batch [517/517], Loss: 0.3881
--- Epoch [103/200] complete. Average Training Loss: 0.2047 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [104/200], Batch [47/517], Loss: 0.3783
  Epoch [104/200], Batch [94/517], Loss: 0.4064
  Epoch [104/200], Batch [141/517], Loss: 0.1183
  Epoch [104/200], Batch [188/517], Loss: 0.0887
  Epoch [104/200], Batch [235/517], Loss: 0.3948
  Epoch [104/200], Batch [282/517], Loss: 0.0926
  Epoch [104/200], Batch [329/517], Loss: 0.1061
  Epoch [104/200], Batch [376/517], Loss: 0.1417
  Epoch [104/200], Batch [423/517], Loss: 0.1354
  Epoch [104/200], Batch [470/517], Loss: 0.3063
  Epoch [104/200], Batch [517/517], Loss: 0.4299
--- Epoch [104/200] complete. Average Training Loss: 0.2092 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [105/200], Batch [47/517], Loss: 0.4056
  Epoch [105/200], Batch [94/517], Loss: 0.0947
  Epoch [105/200], Batch [141/517], Loss: 0.1648
  Epoch [105/200], Batch [188/517], Loss: 0.1761
  Epoch [105/200], Batch [235/517], Loss: 0.3903
  Epoch [105/200], Batch [282/517], Loss: 0.1942
  Epoch [105/200], Batch [329/517], Loss: 0.3200
  Epoch [105/200], Batch [376/517], Loss: 0.1306
  Epoch [105/200], Batch [423/517], Loss: 0.1600
  Epoch [105/200], Batch [470/517], Loss: 0.1277
  Epoch [105/200], Batch [517/517], Loss: 0.4146
--- Epoch [105/200] complete. Average Training Loss: 0.2113 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [106/200], Batch [47/517], Loss: 0.1405
  Epoch [106/200], Batch [94/517], Loss: 0.0827
  Epoch [106/200], Batch [141/517], Loss: 0.4195
  Epoch [106/200], Batch [188/517], Loss: 0.1307
  Epoch [106/200], Batch [235/517], Loss: 0.2085
  Epoch [106/200], Batch [282/517], Loss: 0.2557
  Epoch [106/200], Batch [329/517], Loss: 0.1872
  Epoch [106/200], Batch [376/517], Loss: 0.1773
  Epoch [106/200], Batch [423/517], Loss: 0.1535
  Epoch [106/200], Batch [470/517], Loss: 0.1146
  Epoch [106/200], Batch [517/517], Loss: 0.3138
--- Epoch [106/200] complete. Average Training Loss: 0.2080 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [107/200], Batch [47/517], Loss: 0.4079
  Epoch [107/200], Batch [94/517], Loss: 0.1294
  Epoch [107/200], Batch [141/517], Loss: 0.0902
  Epoch [107/200], Batch [188/517], Loss: 0.3757
  Epoch [107/200], Batch [235/517], Loss: 0.1299
  Epoch [107/200], Batch [282/517], Loss: 0.1816
  Epoch [107/200], Batch [329/517], Loss: 0.1501
  Epoch [107/200], Batch [376/517], Loss: 0.3800
  Epoch [107/200], Batch [423/517], Loss: 0.1176
  Epoch [107/200], Batch [470/517], Loss: 0.3871
  Epoch [107/200], Batch [517/517], Loss: 0.0759
--- Epoch [107/200] complete. Average Training Loss: 0.2022 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [108/200], Batch [47/517], Loss: 0.2282
  Epoch [108/200], Batch [94/517], Loss: 0.1686
  Epoch [108/200], Batch [141/517], Loss: 0.1533
  Epoch [108/200], Batch [188/517], Loss: 0.1253
  Epoch [108/200], Batch [235/517], Loss: 0.4149
  Epoch [108/200], Batch [282/517], Loss: 0.2348
  Epoch [108/200], Batch [329/517], Loss: 0.2258
  Epoch [108/200], Batch [376/517], Loss: 0.1413
  Epoch [108/200], Batch [423/517], Loss: 0.1502
  Epoch [108/200], Batch [470/517], Loss: 0.2536
  Epoch [108/200], Batch [517/517], Loss: 0.1313
--- Epoch [108/200] complete. Average Training Loss: 0.2078 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [109/200], Batch [47/517], Loss: 0.1766
  Epoch [109/200], Batch [94/517], Loss: 0.1081
  Epoch [109/200], Batch [141/517], Loss: 0.1001
  Epoch [109/200], Batch [188/517], Loss: 0.1421
  Epoch [109/200], Batch [235/517], Loss: 0.1310
  Epoch [109/200], Batch [282/517], Loss: 0.1949
  Epoch [109/200], Batch [329/517], Loss: 0.1557
  Epoch [109/200], Batch [376/517], Loss: 0.1603
  Epoch [109/200], Batch [423/517], Loss: 0.1326
  Epoch [109/200], Batch [470/517], Loss: 0.2832
  Epoch [109/200], Batch [517/517], Loss: 0.4582
--- Epoch [109/200] complete. Average Training Loss: 0.2122 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [110/200], Batch [47/517], Loss: 0.1641
  Epoch [110/200], Batch [94/517], Loss: 0.1620
  Epoch [110/200], Batch [141/517], Loss: 0.3852
  Epoch [110/200], Batch [188/517], Loss: 0.1738
  Epoch [110/200], Batch [235/517], Loss: 0.2337
  Epoch [110/200], Batch [282/517], Loss: 0.1821
  Epoch [110/200], Batch [329/517], Loss: 0.1263
  Epoch [110/200], Batch [376/517], Loss: 0.1995
  Epoch [110/200], Batch [423/517], Loss: 0.1479
  Epoch [110/200], Batch [470/517], Loss: 0.4035
  Epoch [110/200], Batch [517/517], Loss: 0.4331
--- Epoch [110/200] complete. Average Training Loss: 0.2101 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [111/200], Batch [47/517], Loss: 0.1288
  Epoch [111/200], Batch [94/517], Loss: 0.4000
  Epoch [111/200], Batch [141/517], Loss: 0.1908
  Epoch [111/200], Batch [188/517], Loss: 0.1025
  Epoch [111/200], Batch [235/517], Loss: 0.1207
  Epoch [111/200], Batch [282/517], Loss: 0.3091
  Epoch [111/200], Batch [329/517], Loss: 0.2519
  Epoch [111/200], Batch [376/517], Loss: 0.1687
  Epoch [111/200], Batch [423/517], Loss: 0.1484
  Epoch [111/200], Batch [470/517], Loss: 0.2614
  Epoch [111/200], Batch [517/517], Loss: 0.3911
--- Epoch [111/200] complete. Average Training Loss: 0.2125 ---
--- Time taken for epoch: 313.95 seconds ---
  Epoch [112/200], Batch [47/517], Loss: 0.0922
  Epoch [112/200], Batch [94/517], Loss: 0.1396
  Epoch [112/200], Batch [141/517], Loss: 0.0770
  Epoch [112/200], Batch [188/517], Loss: 0.1553
  Epoch [112/200], Batch [235/517], Loss: 0.1488
  Epoch [112/200], Batch [282/517], Loss: 0.1227
  Epoch [112/200], Batch [329/517], Loss: 0.1548
  Epoch [112/200], Batch [376/517], Loss: 0.1758
  Epoch [112/200], Batch [423/517], Loss: 0.3204
  Epoch [112/200], Batch [470/517], Loss: 0.1582
  Epoch [112/200], Batch [517/517], Loss: 0.3946
--- Epoch [112/200] complete. Average Training Loss: 0.2123 ---
--- Time taken for epoch: 313.78 seconds ---
  Epoch [113/200], Batch [47/517], Loss: 0.3997
  Epoch [113/200], Batch [94/517], Loss: 0.2302
  Epoch [113/200], Batch [141/517], Loss: 0.1494
  Epoch [113/200], Batch [188/517], Loss: 0.2899
  Epoch [113/200], Batch [235/517], Loss: 0.1259
  Epoch [113/200], Batch [282/517], Loss: 0.3707
  Epoch [113/200], Batch [329/517], Loss: 0.4495
  Epoch [113/200], Batch [376/517], Loss: 0.0999
  Epoch [113/200], Batch [423/517], Loss: 0.1743
  Epoch [113/200], Batch [470/517], Loss: 0.4072
  Epoch [113/200], Batch [517/517], Loss: 0.3194
--- Epoch [113/200] complete. Average Training Loss: 0.2053 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [114/200], Batch [47/517], Loss: 0.4071
  Epoch [114/200], Batch [94/517], Loss: 0.1661
  Epoch [114/200], Batch [141/517], Loss: 0.2613
  Epoch [114/200], Batch [188/517], Loss: 0.1477
  Epoch [114/200], Batch [235/517], Loss: 0.2241
  Epoch [114/200], Batch [282/517], Loss: 0.1345
  Epoch [114/200], Batch [329/517], Loss: 0.3140
  Epoch [114/200], Batch [376/517], Loss: 0.1482
  Epoch [114/200], Batch [423/517], Loss: 0.1535
  Epoch [114/200], Batch [470/517], Loss: 0.4072
  Epoch [114/200], Batch [517/517], Loss: 0.2045
--- Epoch [114/200] complete. Average Training Loss: 0.2189 ---
--- Time taken for epoch: 314.11 seconds ---
  Epoch [115/200], Batch [47/517], Loss: 0.1434
  Epoch [115/200], Batch [94/517], Loss: 0.1430
  Epoch [115/200], Batch [141/517], Loss: 0.2653
  Epoch [115/200], Batch [188/517], Loss: 0.3384
  Epoch [115/200], Batch [235/517], Loss: 0.4053
  Epoch [115/200], Batch [282/517], Loss: 0.2709
  Epoch [115/200], Batch [329/517], Loss: 0.2780
  Epoch [115/200], Batch [376/517], Loss: 0.2335
  Epoch [115/200], Batch [423/517], Loss: 0.4187
  Epoch [115/200], Batch [470/517], Loss: 0.2908
  Epoch [115/200], Batch [517/517], Loss: 0.2994
--- Epoch [115/200] complete. Average Training Loss: 0.2944 ---
--- Time taken for epoch: 314.41 seconds ---
  Epoch [116/200], Batch [47/517], Loss: 0.1588
  Epoch [116/200], Batch [94/517], Loss: 0.2877
  Epoch [116/200], Batch [141/517], Loss: 0.2599
  Epoch [116/200], Batch [188/517], Loss: 0.2385
  Epoch [116/200], Batch [235/517], Loss: 0.1416
  Epoch [116/200], Batch [282/517], Loss: 0.2341
  Epoch [116/200], Batch [329/517], Loss: 0.2587
  Epoch [116/200], Batch [376/517], Loss: 0.2479
  Epoch [116/200], Batch [423/517], Loss: 0.4123
  Epoch [116/200], Batch [470/517], Loss: 0.1893
  Epoch [116/200], Batch [517/517], Loss: 0.2908
--- Epoch [116/200] complete. Average Training Loss: 0.2578 ---
--- Time taken for epoch: 314.03 seconds ---
  Epoch [117/200], Batch [47/517], Loss: 0.2146
  Epoch [117/200], Batch [94/517], Loss: 0.1666
  Epoch [117/200], Batch [141/517], Loss: 0.2441
  Epoch [117/200], Batch [188/517], Loss: 0.3898
  Epoch [117/200], Batch [235/517], Loss: 0.3833
  Epoch [117/200], Batch [282/517], Loss: 0.1228
  Epoch [117/200], Batch [329/517], Loss: 0.4249
  Epoch [117/200], Batch [376/517], Loss: 0.4442
  Epoch [117/200], Batch [423/517], Loss: 0.4430
  Epoch [117/200], Batch [470/517], Loss: 0.4398
  Epoch [117/200], Batch [517/517], Loss: 0.3935
--- Epoch [117/200] complete. Average Training Loss: 0.3382 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [118/200], Batch [47/517], Loss: 0.0919
  Epoch [118/200], Batch [94/517], Loss: 0.0512
  Epoch [118/200], Batch [141/517], Loss: 0.4019
  Epoch [118/200], Batch [188/517], Loss: 0.4199
  Epoch [118/200], Batch [235/517], Loss: 0.0637
  Epoch [118/200], Batch [282/517], Loss: 0.3966
  Epoch [118/200], Batch [329/517], Loss: 0.4088
  Epoch [118/200], Batch [376/517], Loss: 0.4133
  Epoch [118/200], Batch [423/517], Loss: 0.4232
  Epoch [118/200], Batch [470/517], Loss: 0.4048
  Epoch [118/200], Batch [517/517], Loss: 0.4832
--- Epoch [118/200] complete. Average Training Loss: 0.3841 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [119/200], Batch [47/517], Loss: 0.4518
  Epoch [119/200], Batch [94/517], Loss: 0.4081
  Epoch [119/200], Batch [141/517], Loss: 0.4551
  Epoch [119/200], Batch [188/517], Loss: 0.4323
  Epoch [119/200], Batch [235/517], Loss: 0.0559
  Epoch [119/200], Batch [282/517], Loss: 0.4421
  Epoch [119/200], Batch [329/517], Loss: 0.4277
  Epoch [119/200], Batch [376/517], Loss: 0.4508
  Epoch [119/200], Batch [423/517], Loss: 0.4273
  Epoch [119/200], Batch [470/517], Loss: 0.0791
  Epoch [119/200], Batch [517/517], Loss: 0.4145
--- Epoch [119/200] complete. Average Training Loss: 0.3808 ---
--- Time taken for epoch: 314.08 seconds ---
  Epoch [120/200], Batch [47/517], Loss: 0.4033
  Epoch [120/200], Batch [94/517], Loss: 0.4373
  Epoch [120/200], Batch [141/517], Loss: 0.4082
  Epoch [120/200], Batch [188/517], Loss: 0.4259
  Epoch [120/200], Batch [235/517], Loss: 0.3756
  Epoch [120/200], Batch [282/517], Loss: 0.3112
  Epoch [120/200], Batch [329/517], Loss: 0.3431
  Epoch [120/200], Batch [376/517], Loss: 0.3639
  Epoch [120/200], Batch [423/517], Loss: 0.4166
  Epoch [120/200], Batch [470/517], Loss: 0.4023
  Epoch [120/200], Batch [517/517], Loss: 0.4351
--- Epoch [120/200] complete. Average Training Loss: 0.3775 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [121/200], Batch [47/517], Loss: 0.4164
  Epoch [121/200], Batch [94/517], Loss: 0.2404
  Epoch [121/200], Batch [141/517], Loss: 0.3845
  Epoch [121/200], Batch [188/517], Loss: 0.3948
  Epoch [121/200], Batch [235/517], Loss: 0.2773
  Epoch [121/200], Batch [282/517], Loss: 0.2520
  Epoch [121/200], Batch [329/517], Loss: 0.2231
  Epoch [121/200], Batch [376/517], Loss: 0.2122
  Epoch [121/200], Batch [423/517], Loss: 0.2868
  Epoch [121/200], Batch [470/517], Loss: 0.3945
  Epoch [121/200], Batch [517/517], Loss: 0.4095
--- Epoch [121/200] complete. Average Training Loss: 0.3138 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [122/200], Batch [47/517], Loss: 0.3922
  Epoch [122/200], Batch [94/517], Loss: 0.3821
  Epoch [122/200], Batch [141/517], Loss: 0.3310
  Epoch [122/200], Batch [188/517], Loss: 0.1296
  Epoch [122/200], Batch [235/517], Loss: 0.4069
  Epoch [122/200], Batch [282/517], Loss: 0.1776
  Epoch [122/200], Batch [329/517], Loss: 0.4323
  Epoch [122/200], Batch [376/517], Loss: 0.2271
  Epoch [122/200], Batch [423/517], Loss: 0.2723
  Epoch [122/200], Batch [470/517], Loss: 0.1710
  Epoch [122/200], Batch [517/517], Loss: 0.1565
--- Epoch [122/200] complete. Average Training Loss: 0.2406 ---
--- Time taken for epoch: 313.79 seconds ---
  Epoch [123/200], Batch [47/517], Loss: 0.2418
  Epoch [123/200], Batch [94/517], Loss: 0.4068
  Epoch [123/200], Batch [141/517], Loss: 0.1764
  Epoch [123/200], Batch [188/517], Loss: 0.3868
  Epoch [123/200], Batch [235/517], Loss: 0.3185
  Epoch [123/200], Batch [282/517], Loss: 0.2951
  Epoch [123/200], Batch [329/517], Loss: 0.4081
  Epoch [123/200], Batch [376/517], Loss: 0.4100
  Epoch [123/200], Batch [423/517], Loss: 0.3537
  Epoch [123/200], Batch [470/517], Loss: 0.2605
  Epoch [123/200], Batch [517/517], Loss: 0.2518
--- Epoch [123/200] complete. Average Training Loss: 0.2864 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [124/200], Batch [47/517], Loss: 0.3254
  Epoch [124/200], Batch [94/517], Loss: 0.2121
  Epoch [124/200], Batch [141/517], Loss: 0.3989
  Epoch [124/200], Batch [188/517], Loss: 0.2208
  Epoch [124/200], Batch [235/517], Loss: 0.4221
  Epoch [124/200], Batch [282/517], Loss: 0.2988
  Epoch [124/200], Batch [329/517], Loss: 0.2152
  Epoch [124/200], Batch [376/517], Loss: 0.1819
  Epoch [124/200], Batch [423/517], Loss: 0.2859
  Epoch [124/200], Batch [470/517], Loss: 0.1902
  Epoch [124/200], Batch [517/517], Loss: 0.4001
--- Epoch [124/200] complete. Average Training Loss: 0.3203 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [125/200], Batch [47/517], Loss: 0.2427
  Epoch [125/200], Batch [94/517], Loss: 0.3918
  Epoch [125/200], Batch [141/517], Loss: 0.2756
  Epoch [125/200], Batch [188/517], Loss: 0.2016
  Epoch [125/200], Batch [235/517], Loss: 0.2364
  Epoch [125/200], Batch [282/517], Loss: 0.3938
  Epoch [125/200], Batch [329/517], Loss: 0.4102
  Epoch [125/200], Batch [376/517], Loss: 0.4030
  Epoch [125/200], Batch [423/517], Loss: 0.4260
  Epoch [125/200], Batch [470/517], Loss: 0.4189
  Epoch [125/200], Batch [517/517], Loss: 0.4037
--- Epoch [125/200] complete. Average Training Loss: 0.3177 ---
--- Time taken for epoch: 313.87 seconds ---
 -- Updated Checkpoint: Saved model at 125 epochs.
  Epoch [126/200], Batch [47/517], Loss: 0.3958
  Epoch [126/200], Batch [94/517], Loss: 0.3816
  Epoch [126/200], Batch [141/517], Loss: 0.4089
  Epoch [126/200], Batch [188/517], Loss: 0.4074
  Epoch [126/200], Batch [235/517], Loss: 0.4167
  Epoch [126/200], Batch [282/517], Loss: 0.4126
  Epoch [126/200], Batch [329/517], Loss: 0.3825
  Epoch [126/200], Batch [376/517], Loss: 0.2112
  Epoch [126/200], Batch [423/517], Loss: 0.2777
  Epoch [126/200], Batch [470/517], Loss: 0.3973
  Epoch [126/200], Batch [517/517], Loss: 0.4224
--- Epoch [126/200] complete. Average Training Loss: 0.3428 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [127/200], Batch [47/517], Loss: 0.4093
  Epoch [127/200], Batch [94/517], Loss: 0.3815
  Epoch [127/200], Batch [141/517], Loss: 0.2175
  Epoch [127/200], Batch [188/517], Loss: 0.4048
  Epoch [127/200], Batch [235/517], Loss: 0.2424
  Epoch [127/200], Batch [282/517], Loss: 0.2191
  Epoch [127/200], Batch [329/517], Loss: 0.3833
  Epoch [127/200], Batch [376/517], Loss: 0.2362
  Epoch [127/200], Batch [423/517], Loss: 0.1829
  Epoch [127/200], Batch [470/517], Loss: 0.3998
  Epoch [127/200], Batch [517/517], Loss: 0.2818
--- Epoch [127/200] complete. Average Training Loss: 0.2840 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [128/200], Batch [47/517], Loss: 0.1658
  Epoch [128/200], Batch [94/517], Loss: 0.2067
  Epoch [128/200], Batch [141/517], Loss: 0.3906
  Epoch [128/200], Batch [188/517], Loss: 0.3363
  Epoch [128/200], Batch [235/517], Loss: 0.4079
  Epoch [128/200], Batch [282/517], Loss: 0.1462
  Epoch [128/200], Batch [329/517], Loss: 0.4057
  Epoch [128/200], Batch [376/517], Loss: 0.4411
  Epoch [128/200], Batch [423/517], Loss: 0.2016
  Epoch [128/200], Batch [470/517], Loss: 0.3079
  Epoch [128/200], Batch [517/517], Loss: 0.3522
--- Epoch [128/200] complete. Average Training Loss: 0.2701 ---
--- Time taken for epoch: 313.94 seconds ---
  Epoch [129/200], Batch [47/517], Loss: 0.3332
  Epoch [129/200], Batch [94/517], Loss: 0.2632
  Epoch [129/200], Batch [141/517], Loss: 0.2600
  Epoch [129/200], Batch [188/517], Loss: 0.4220
  Epoch [129/200], Batch [235/517], Loss: 0.2858
  Epoch [129/200], Batch [282/517], Loss: 0.4278
  Epoch [129/200], Batch [329/517], Loss: 0.3073
  Epoch [129/200], Batch [376/517], Loss: 0.2962
  Epoch [129/200], Batch [423/517], Loss: 0.2894
  Epoch [129/200], Batch [470/517], Loss: 0.4293
  Epoch [129/200], Batch [517/517], Loss: 0.1404
--- Epoch [129/200] complete. Average Training Loss: 0.3188 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [130/200], Batch [47/517], Loss: 0.3917
  Epoch [130/200], Batch [94/517], Loss: 0.2435
  Epoch [130/200], Batch [141/517], Loss: 0.2644
  Epoch [130/200], Batch [188/517], Loss: 0.2743
  Epoch [130/200], Batch [235/517], Loss: 0.2975
  Epoch [130/200], Batch [282/517], Loss: 0.3494
  Epoch [130/200], Batch [329/517], Loss: 0.1332
  Epoch [130/200], Batch [376/517], Loss: 0.1994
  Epoch [130/200], Batch [423/517], Loss: 0.2380
  Epoch [130/200], Batch [470/517], Loss: 0.1968
  Epoch [130/200], Batch [517/517], Loss: 0.4224
--- Epoch [130/200] complete. Average Training Loss: 0.2752 ---
--- Time taken for epoch: 313.71 seconds ---
  Epoch [131/200], Batch [47/517], Loss: 0.2446
  Epoch [131/200], Batch [94/517], Loss: 0.1281
  Epoch [131/200], Batch [141/517], Loss: 0.1868
  Epoch [131/200], Batch [188/517], Loss: 0.4081
  Epoch [131/200], Batch [235/517], Loss: 0.1908
  Epoch [131/200], Batch [282/517], Loss: 0.2127
  Epoch [131/200], Batch [329/517], Loss: 0.3783
  Epoch [131/200], Batch [376/517], Loss: 0.2161
  Epoch [131/200], Batch [423/517], Loss: 0.1138
  Epoch [131/200], Batch [470/517], Loss: 0.1500
  Epoch [131/200], Batch [517/517], Loss: 0.1693
--- Epoch [131/200] complete. Average Training Loss: 0.2518 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [132/200], Batch [47/517], Loss: 0.1806
  Epoch [132/200], Batch [94/517], Loss: 0.1329
  Epoch [132/200], Batch [141/517], Loss: 0.2214
  Epoch [132/200], Batch [188/517], Loss: 0.2316
  Epoch [132/200], Batch [235/517], Loss: 0.1730
  Epoch [132/200], Batch [282/517], Loss: 0.2692
  Epoch [132/200], Batch [329/517], Loss: 0.1532
  Epoch [132/200], Batch [376/517], Loss: 0.1415
  Epoch [132/200], Batch [423/517], Loss: 0.3460
  Epoch [132/200], Batch [470/517], Loss: 0.4128
  Epoch [132/200], Batch [517/517], Loss: 0.4262
--- Epoch [132/200] complete. Average Training Loss: 0.2507 ---
--- Time taken for epoch: 313.90 seconds ---
  Epoch [133/200], Batch [47/517], Loss: 0.3278
  Epoch [133/200], Batch [94/517], Loss: 0.4004
  Epoch [133/200], Batch [141/517], Loss: 0.1728
  Epoch [133/200], Batch [188/517], Loss: 0.1890
  Epoch [133/200], Batch [235/517], Loss: 0.2038
  Epoch [133/200], Batch [282/517], Loss: 0.2622
  Epoch [133/200], Batch [329/517], Loss: 0.4141
  Epoch [133/200], Batch [376/517], Loss: 0.4245
  Epoch [133/200], Batch [423/517], Loss: 0.1516
  Epoch [133/200], Batch [470/517], Loss: 0.1542
  Epoch [133/200], Batch [517/517], Loss: 0.3860
--- Epoch [133/200] complete. Average Training Loss: 0.2567 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [134/200], Batch [47/517], Loss: 0.1638
  Epoch [134/200], Batch [94/517], Loss: 0.1283
  Epoch [134/200], Batch [141/517], Loss: 0.1732
  Epoch [134/200], Batch [188/517], Loss: 0.3974
  Epoch [134/200], Batch [235/517], Loss: 0.1543
  Epoch [134/200], Batch [282/517], Loss: 0.2448
  Epoch [134/200], Batch [329/517], Loss: 0.1623
  Epoch [134/200], Batch [376/517], Loss: 0.2220
  Epoch [134/200], Batch [423/517], Loss: 0.3041
  Epoch [134/200], Batch [470/517], Loss: 0.3377
  Epoch [134/200], Batch [517/517], Loss: 0.2549
--- Epoch [134/200] complete. Average Training Loss: 0.2451 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [135/200], Batch [47/517], Loss: 0.2094
  Epoch [135/200], Batch [94/517], Loss: 0.1960
  Epoch [135/200], Batch [141/517], Loss: 0.2387
  Epoch [135/200], Batch [188/517], Loss: 0.2649
  Epoch [135/200], Batch [235/517], Loss: 0.2941
  Epoch [135/200], Batch [282/517], Loss: 0.3256
  Epoch [135/200], Batch [329/517], Loss: 0.1546
  Epoch [135/200], Batch [376/517], Loss: 0.2046
  Epoch [135/200], Batch [423/517], Loss: 0.2887
  Epoch [135/200], Batch [470/517], Loss: 0.1875
  Epoch [135/200], Batch [517/517], Loss: 0.4043
--- Epoch [135/200] complete. Average Training Loss: 0.2604 ---
--- Time taken for epoch: 313.91 seconds ---
  Epoch [136/200], Batch [47/517], Loss: 0.1612
  Epoch [136/200], Batch [94/517], Loss: 0.2208
  Epoch [136/200], Batch [141/517], Loss: 0.4060
  Epoch [136/200], Batch [188/517], Loss: 0.3791
  Epoch [136/200], Batch [235/517], Loss: 0.4242
  Epoch [136/200], Batch [282/517], Loss: 0.4051
  Epoch [136/200], Batch [329/517], Loss: 0.1341
  Epoch [136/200], Batch [376/517], Loss: 0.2346
  Epoch [136/200], Batch [423/517], Loss: 0.3938
  Epoch [136/200], Batch [470/517], Loss: 0.1261
  Epoch [136/200], Batch [517/517], Loss: 0.3878
--- Epoch [136/200] complete. Average Training Loss: 0.2418 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [137/200], Batch [47/517], Loss: 0.3967
  Epoch [137/200], Batch [94/517], Loss: 0.2467
  Epoch [137/200], Batch [141/517], Loss: 0.3891
  Epoch [137/200], Batch [188/517], Loss: 0.4001
  Epoch [137/200], Batch [235/517], Loss: 0.1410
  Epoch [137/200], Batch [282/517], Loss: 0.2582
  Epoch [137/200], Batch [329/517], Loss: 0.2909
  Epoch [137/200], Batch [376/517], Loss: 0.2077
  Epoch [137/200], Batch [423/517], Loss: 0.2919
  Epoch [137/200], Batch [470/517], Loss: 0.4061
  Epoch [137/200], Batch [517/517], Loss: 0.2677
--- Epoch [137/200] complete. Average Training Loss: 0.2506 ---
--- Time taken for epoch: 313.96 seconds ---
  Epoch [138/200], Batch [47/517], Loss: 0.2246
  Epoch [138/200], Batch [94/517], Loss: 0.4114
  Epoch [138/200], Batch [141/517], Loss: 0.2277
  Epoch [138/200], Batch [188/517], Loss: 0.1538
  Epoch [138/200], Batch [235/517], Loss: 0.1778
  Epoch [138/200], Batch [282/517], Loss: 0.3920
  Epoch [138/200], Batch [329/517], Loss: 0.1707
  Epoch [138/200], Batch [376/517], Loss: 0.1948
  Epoch [138/200], Batch [423/517], Loss: 0.3989
  Epoch [138/200], Batch [470/517], Loss: 0.1754
  Epoch [138/200], Batch [517/517], Loss: 0.3477
--- Epoch [138/200] complete. Average Training Loss: 0.2462 ---
--- Time taken for epoch: 314.02 seconds ---
  Epoch [139/200], Batch [47/517], Loss: 0.2513
  Epoch [139/200], Batch [94/517], Loss: 0.2531
  Epoch [139/200], Batch [141/517], Loss: 0.4161
  Epoch [139/200], Batch [188/517], Loss: 0.2553
  Epoch [139/200], Batch [235/517], Loss: 0.1607
  Epoch [139/200], Batch [282/517], Loss: 0.2344
  Epoch [139/200], Batch [329/517], Loss: 0.3897
  Epoch [139/200], Batch [376/517], Loss: 0.1657
  Epoch [139/200], Batch [423/517], Loss: 0.3961
  Epoch [139/200], Batch [470/517], Loss: 0.2935
  Epoch [139/200], Batch [517/517], Loss: 0.1906
--- Epoch [139/200] complete. Average Training Loss: 0.2589 ---
--- Time taken for epoch: 313.98 seconds ---
  Epoch [140/200], Batch [47/517], Loss: 0.1793
  Epoch [140/200], Batch [94/517], Loss: 0.1934
  Epoch [140/200], Batch [141/517], Loss: 0.3943
  Epoch [140/200], Batch [188/517], Loss: 0.1131
  Epoch [140/200], Batch [235/517], Loss: 0.1482
  Epoch [140/200], Batch [282/517], Loss: 0.1379
  Epoch [140/200], Batch [329/517], Loss: 0.1542
  Epoch [140/200], Batch [376/517], Loss: 0.1715
  Epoch [140/200], Batch [423/517], Loss: 0.3090
  Epoch [140/200], Batch [470/517], Loss: 0.3427
  Epoch [140/200], Batch [517/517], Loss: 0.3553
--- Epoch [140/200] complete. Average Training Loss: 0.2411 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [141/200], Batch [47/517], Loss: 0.1840
  Epoch [141/200], Batch [94/517], Loss: 0.1888
  Epoch [141/200], Batch [141/517], Loss: 0.1982
  Epoch [141/200], Batch [188/517], Loss: 0.2838
  Epoch [141/200], Batch [235/517], Loss: 0.2092
  Epoch [141/200], Batch [282/517], Loss: 0.1194
  Epoch [141/200], Batch [329/517], Loss: 0.2037
  Epoch [141/200], Batch [376/517], Loss: 0.4001
  Epoch [141/200], Batch [423/517], Loss: 0.4067
  Epoch [141/200], Batch [470/517], Loss: 0.1526
  Epoch [141/200], Batch [517/517], Loss: 0.4379
--- Epoch [141/200] complete. Average Training Loss: 0.2782 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [142/200], Batch [47/517], Loss: 0.1629
  Epoch [142/200], Batch [94/517], Loss: 0.4058
  Epoch [142/200], Batch [141/517], Loss: 0.2671
  Epoch [142/200], Batch [188/517], Loss: 0.4250
  Epoch [142/200], Batch [235/517], Loss: 0.1958
  Epoch [142/200], Batch [282/517], Loss: 0.2091
  Epoch [142/200], Batch [329/517], Loss: 0.1116
  Epoch [142/200], Batch [376/517], Loss: 0.2626
  Epoch [142/200], Batch [423/517], Loss: 0.1777
  Epoch [142/200], Batch [470/517], Loss: 0.2127
  Epoch [142/200], Batch [517/517], Loss: 0.3790
--- Epoch [142/200] complete. Average Training Loss: 0.2670 ---
--- Time taken for epoch: 314.00 seconds ---
  Epoch [143/200], Batch [47/517], Loss: 0.1105
  Epoch [143/200], Batch [94/517], Loss: 0.1544
  Epoch [143/200], Batch [141/517], Loss: 0.3898
  Epoch [143/200], Batch [188/517], Loss: 0.4317
  Epoch [143/200], Batch [235/517], Loss: 0.2931
  Epoch [143/200], Batch [282/517], Loss: 0.3079
  Epoch [143/200], Batch [329/517], Loss: 0.4358
  Epoch [143/200], Batch [376/517], Loss: 0.1879
  Epoch [143/200], Batch [423/517], Loss: 0.3186
  Epoch [143/200], Batch [470/517], Loss: 0.2374
  Epoch [143/200], Batch [517/517], Loss: 0.2445
--- Epoch [143/200] complete. Average Training Loss: 0.2902 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [144/200], Batch [47/517], Loss: 0.3512
  Epoch [144/200], Batch [94/517], Loss: 0.3001
  Epoch [144/200], Batch [141/517], Loss: 0.1350
  Epoch [144/200], Batch [188/517], Loss: 0.3935
  Epoch [144/200], Batch [235/517], Loss: 0.3760
  Epoch [144/200], Batch [282/517], Loss: 0.2599
  Epoch [144/200], Batch [329/517], Loss: 0.2011
  Epoch [144/200], Batch [376/517], Loss: 0.4103
  Epoch [144/200], Batch [423/517], Loss: 0.3025
  Epoch [144/200], Batch [470/517], Loss: 0.1936
  Epoch [144/200], Batch [517/517], Loss: 0.1780
--- Epoch [144/200] complete. Average Training Loss: 0.2650 ---
--- Time taken for epoch: 313.80 seconds ---
  Epoch [145/200], Batch [47/517], Loss: 0.2929
  Epoch [145/200], Batch [94/517], Loss: 0.3489
  Epoch [145/200], Batch [141/517], Loss: 0.4179
  Epoch [145/200], Batch [188/517], Loss: 0.2381
  Epoch [145/200], Batch [235/517], Loss: 0.4534
  Epoch [145/200], Batch [282/517], Loss: 0.2844
  Epoch [145/200], Batch [329/517], Loss: 0.3361
  Epoch [145/200], Batch [376/517], Loss: 0.3938
  Epoch [145/200], Batch [423/517], Loss: 0.3050
  Epoch [145/200], Batch [470/517], Loss: 0.3632
  Epoch [145/200], Batch [517/517], Loss: 0.2511
--- Epoch [145/200] complete. Average Training Loss: 0.3211 ---
--- Time taken for epoch: 313.93 seconds ---
  Epoch [146/200], Batch [47/517], Loss: 0.1241
  Epoch [146/200], Batch [94/517], Loss: 0.2373
  Epoch [146/200], Batch [141/517], Loss: 0.2814
  Epoch [146/200], Batch [188/517], Loss: 0.2776
  Epoch [146/200], Batch [235/517], Loss: 0.4379
  Epoch [146/200], Batch [282/517], Loss: 0.1912
  Epoch [146/200], Batch [329/517], Loss: 0.1534
  Epoch [146/200], Batch [376/517], Loss: 0.3090
  Epoch [146/200], Batch [423/517], Loss: 0.2053
  Epoch [146/200], Batch [470/517], Loss: 0.2658
  Epoch [146/200], Batch [517/517], Loss: 0.1467
--- Epoch [146/200] complete. Average Training Loss: 0.2716 ---
--- Time taken for epoch: 313.85 seconds ---
  Epoch [147/200], Batch [47/517], Loss: 0.2961
  Epoch [147/200], Batch [94/517], Loss: 0.3986
  Epoch [147/200], Batch [141/517], Loss: 0.3248
  Epoch [147/200], Batch [188/517], Loss: 0.1763
  Epoch [147/200], Batch [235/517], Loss: 0.3428
  Epoch [147/200], Batch [282/517], Loss: 0.2266
  Epoch [147/200], Batch [329/517], Loss: 0.3242
  Epoch [147/200], Batch [376/517], Loss: 0.1455
  Epoch [147/200], Batch [423/517], Loss: 0.2575
  Epoch [147/200], Batch [470/517], Loss: 0.4144
  Epoch [147/200], Batch [517/517], Loss: 0.2097
--- Epoch [147/200] complete. Average Training Loss: 0.2432 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [148/200], Batch [47/517], Loss: 0.1519
  Epoch [148/200], Batch [94/517], Loss: 0.1613
  Epoch [148/200], Batch [141/517], Loss: 0.2811
  Epoch [148/200], Batch [188/517], Loss: 0.3820
  Epoch [148/200], Batch [235/517], Loss: 0.1588
  Epoch [148/200], Batch [282/517], Loss: 0.2530
  Epoch [148/200], Batch [329/517], Loss: 0.1651
  Epoch [148/200], Batch [376/517], Loss: 0.4094
  Epoch [148/200], Batch [423/517], Loss: 0.1785
  Epoch [148/200], Batch [470/517], Loss: 0.1292
  Epoch [148/200], Batch [517/517], Loss: 0.2614
--- Epoch [148/200] complete. Average Training Loss: 0.2385 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [149/200], Batch [47/517], Loss: 0.4379
  Epoch [149/200], Batch [94/517], Loss: 0.3993
  Epoch [149/200], Batch [141/517], Loss: 0.1662
  Epoch [149/200], Batch [188/517], Loss: 0.1525
  Epoch [149/200], Batch [235/517], Loss: 0.1862
  Epoch [149/200], Batch [282/517], Loss: 0.2248
  Epoch [149/200], Batch [329/517], Loss: 0.3966
  Epoch [149/200], Batch [376/517], Loss: 0.2189
  Epoch [149/200], Batch [423/517], Loss: 0.1585
  Epoch [149/200], Batch [470/517], Loss: 0.2513
  Epoch [149/200], Batch [517/517], Loss: 0.2159
--- Epoch [149/200] complete. Average Training Loss: 0.2274 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [150/200], Batch [47/517], Loss: 0.4621
  Epoch [150/200], Batch [94/517], Loss: 0.4169
  Epoch [150/200], Batch [141/517], Loss: 0.4479
  Epoch [150/200], Batch [188/517], Loss: 0.4111
  Epoch [150/200], Batch [235/517], Loss: 0.4260
  Epoch [150/200], Batch [282/517], Loss: 0.4265
  Epoch [150/200], Batch [329/517], Loss: 0.3972
  Epoch [150/200], Batch [376/517], Loss: 0.4260
  Epoch [150/200], Batch [423/517], Loss: 0.4686
  Epoch [150/200], Batch [470/517], Loss: 0.4343
  Epoch [150/200], Batch [517/517], Loss: 0.4097
--- Epoch [150/200] complete. Average Training Loss: 0.3746 ---
--- Time taken for epoch: 313.73 seconds ---
 -- Updated Checkpoint: Saved model at 150 epochs.
  Epoch [151/200], Batch [47/517], Loss: 0.3934
  Epoch [151/200], Batch [94/517], Loss: 0.4667
  Epoch [151/200], Batch [141/517], Loss: 0.4120
  Epoch [151/200], Batch [188/517], Loss: 0.0650
  Epoch [151/200], Batch [235/517], Loss: 0.4246
  Epoch [151/200], Batch [282/517], Loss: 0.4311
  Epoch [151/200], Batch [329/517], Loss: 0.4072
  Epoch [151/200], Batch [376/517], Loss: 0.1407
  Epoch [151/200], Batch [423/517], Loss: 0.4361
  Epoch [151/200], Batch [470/517], Loss: 0.4538
  Epoch [151/200], Batch [517/517], Loss: 0.4159
--- Epoch [151/200] complete. Average Training Loss: 0.3704 ---
--- Time taken for epoch: 313.84 seconds ---
  Epoch [152/200], Batch [47/517], Loss: 0.4104
  Epoch [152/200], Batch [94/517], Loss: 0.4329
  Epoch [152/200], Batch [141/517], Loss: 0.3947
  Epoch [152/200], Batch [188/517], Loss: 0.4013
  Epoch [152/200], Batch [235/517], Loss: 0.4256
  Epoch [152/200], Batch [282/517], Loss: 0.4116
  Epoch [152/200], Batch [329/517], Loss: 0.4147
  Epoch [152/200], Batch [376/517], Loss: 0.4200
  Epoch [152/200], Batch [423/517], Loss: 0.4087
  Epoch [152/200], Batch [470/517], Loss: 0.4249
  Epoch [152/200], Batch [517/517], Loss: 0.4039
--- Epoch [152/200] complete. Average Training Loss: 0.3719 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [153/200], Batch [47/517], Loss: 0.4190
  Epoch [153/200], Batch [94/517], Loss: 0.4050
  Epoch [153/200], Batch [141/517], Loss: 0.0579
  Epoch [153/200], Batch [188/517], Loss: 0.4134
  Epoch [153/200], Batch [235/517], Loss: 0.4141
  Epoch [153/200], Batch [282/517], Loss: 0.4058
  Epoch [153/200], Batch [329/517], Loss: 0.3997
  Epoch [153/200], Batch [376/517], Loss: 0.3876
  Epoch [153/200], Batch [423/517], Loss: 0.4164
  Epoch [153/200], Batch [470/517], Loss: 0.4000
  Epoch [153/200], Batch [517/517], Loss: 0.3681
--- Epoch [153/200] complete. Average Training Loss: 0.3717 ---
--- Time taken for epoch: 313.75 seconds ---
  Epoch [154/200], Batch [47/517], Loss: 0.4323
  Epoch [154/200], Batch [94/517], Loss: 0.4046
  Epoch [154/200], Batch [141/517], Loss: 0.4062
  Epoch [154/200], Batch [188/517], Loss: 0.4245
  Epoch [154/200], Batch [235/517], Loss: 0.3960
  Epoch [154/200], Batch [282/517], Loss: 0.4115
  Epoch [154/200], Batch [329/517], Loss: 0.3978
  Epoch [154/200], Batch [376/517], Loss: 0.3998
  Epoch [154/200], Batch [423/517], Loss: 0.4244
  Epoch [154/200], Batch [470/517], Loss: 0.4011
  Epoch [154/200], Batch [517/517], Loss: 0.4114
--- Epoch [154/200] complete. Average Training Loss: 0.3722 ---
--- Time taken for epoch: 313.97 seconds ---
  Epoch [155/200], Batch [47/517], Loss: 0.4139
  Epoch [155/200], Batch [94/517], Loss: 0.3943
  Epoch [155/200], Batch [141/517], Loss: 0.4075
  Epoch [155/200], Batch [188/517], Loss: 0.4113
  Epoch [155/200], Batch [235/517], Loss: 0.4312
  Epoch [155/200], Batch [282/517], Loss: 0.0704
  Epoch [155/200], Batch [329/517], Loss: 0.3962
  Epoch [155/200], Batch [376/517], Loss: 0.3990
  Epoch [155/200], Batch [423/517], Loss: 0.4000
  Epoch [155/200], Batch [470/517], Loss: 0.0478
  Epoch [155/200], Batch [517/517], Loss: 0.4070
--- Epoch [155/200] complete. Average Training Loss: 0.3732 ---
--- Time taken for epoch: 313.84 seconds ---
  Epoch [156/200], Batch [47/517], Loss: 0.3953
  Epoch [156/200], Batch [94/517], Loss: 0.4058
  Epoch [156/200], Batch [141/517], Loss: 0.4189
  Epoch [156/200], Batch [188/517], Loss: 0.4055
  Epoch [156/200], Batch [235/517], Loss: 0.3950
  Epoch [156/200], Batch [282/517], Loss: 0.3946
  Epoch [156/200], Batch [329/517], Loss: 0.3760
  Epoch [156/200], Batch [376/517], Loss: 0.4142
  Epoch [156/200], Batch [423/517], Loss: 0.3856
  Epoch [156/200], Batch [470/517], Loss: 0.0594
  Epoch [156/200], Batch [517/517], Loss: 0.4029
--- Epoch [156/200] complete. Average Training Loss: 0.3690 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [157/200], Batch [47/517], Loss: 0.0644
  Epoch [157/200], Batch [94/517], Loss: 0.4294
  Epoch [157/200], Batch [141/517], Loss: 0.0752
  Epoch [157/200], Batch [188/517], Loss: 0.3901
  Epoch [157/200], Batch [235/517], Loss: 0.4709
  Epoch [157/200], Batch [282/517], Loss: 0.3956
  Epoch [157/200], Batch [329/517], Loss: 0.4544
  Epoch [157/200], Batch [376/517], Loss: 0.3828
  Epoch [157/200], Batch [423/517], Loss: 0.4373
  Epoch [157/200], Batch [470/517], Loss: 0.3900
  Epoch [157/200], Batch [517/517], Loss: 0.3920
--- Epoch [157/200] complete. Average Training Loss: 0.3626 ---
--- Time taken for epoch: 314.13 seconds ---
  Epoch [158/200], Batch [47/517], Loss: 0.4340
  Epoch [158/200], Batch [94/517], Loss: 0.0545
  Epoch [158/200], Batch [141/517], Loss: 0.4159
  Epoch [158/200], Batch [188/517], Loss: 0.4064
  Epoch [158/200], Batch [235/517], Loss: 0.4179
  Epoch [158/200], Batch [282/517], Loss: 0.3960
  Epoch [158/200], Batch [329/517], Loss: 0.3710
  Epoch [158/200], Batch [376/517], Loss: 0.4044
  Epoch [158/200], Batch [423/517], Loss: 0.4114
  Epoch [158/200], Batch [470/517], Loss: 0.3865
  Epoch [158/200], Batch [517/517], Loss: 0.4136
--- Epoch [158/200] complete. Average Training Loss: 0.3581 ---
--- Time taken for epoch: 314.01 seconds ---
  Epoch [159/200], Batch [47/517], Loss: 0.3976
  Epoch [159/200], Batch [94/517], Loss: 0.4339
  Epoch [159/200], Batch [141/517], Loss: 0.3841
  Epoch [159/200], Batch [188/517], Loss: 0.3849
  Epoch [159/200], Batch [235/517], Loss: 0.1090
  Epoch [159/200], Batch [282/517], Loss: 0.4305
  Epoch [159/200], Batch [329/517], Loss: 0.4080
  Epoch [159/200], Batch [376/517], Loss: 0.4067
  Epoch [159/200], Batch [423/517], Loss: 0.3859
  Epoch [159/200], Batch [470/517], Loss: 0.0757
  Epoch [159/200], Batch [517/517], Loss: 0.3907
--- Epoch [159/200] complete. Average Training Loss: 0.3590 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [160/200], Batch [47/517], Loss: 0.4285
  Epoch [160/200], Batch [94/517], Loss: 0.4350
  Epoch [160/200], Batch [141/517], Loss: 0.3947
  Epoch [160/200], Batch [188/517], Loss: 0.4239
  Epoch [160/200], Batch [235/517], Loss: 0.3868
  Epoch [160/200], Batch [282/517], Loss: 0.3915
  Epoch [160/200], Batch [329/517], Loss: 0.0677
  Epoch [160/200], Batch [376/517], Loss: 0.3789
  Epoch [160/200], Batch [423/517], Loss: 0.0717
  Epoch [160/200], Batch [470/517], Loss: 0.4018
  Epoch [160/200], Batch [517/517], Loss: 0.4363
--- Epoch [160/200] complete. Average Training Loss: 0.3645 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [161/200], Batch [47/517], Loss: 0.3957
  Epoch [161/200], Batch [94/517], Loss: 0.4014
  Epoch [161/200], Batch [141/517], Loss: 0.4091
  Epoch [161/200], Batch [188/517], Loss: 0.1328
  Epoch [161/200], Batch [235/517], Loss: 0.3793
  Epoch [161/200], Batch [282/517], Loss: 0.3975
  Epoch [161/200], Batch [329/517], Loss: 0.4558
  Epoch [161/200], Batch [376/517], Loss: 0.4195
  Epoch [161/200], Batch [423/517], Loss: 0.4073
  Epoch [161/200], Batch [470/517], Loss: 0.0927
  Epoch [161/200], Batch [517/517], Loss: 0.3839
--- Epoch [161/200] complete. Average Training Loss: 0.3625 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [162/200], Batch [47/517], Loss: 0.3936
  Epoch [162/200], Batch [94/517], Loss: 0.3842
  Epoch [162/200], Batch [141/517], Loss: 0.4028
  Epoch [162/200], Batch [188/517], Loss: 0.4246
  Epoch [162/200], Batch [235/517], Loss: 0.4141
  Epoch [162/200], Batch [282/517], Loss: 0.4046
  Epoch [162/200], Batch [329/517], Loss: 0.4024
  Epoch [162/200], Batch [376/517], Loss: 0.3948
  Epoch [162/200], Batch [423/517], Loss: 0.4003
  Epoch [162/200], Batch [470/517], Loss: 0.4130
  Epoch [162/200], Batch [517/517], Loss: 0.3960
--- Epoch [162/200] complete. Average Training Loss: 0.3560 ---
--- Time taken for epoch: 313.78 seconds ---
  Epoch [163/200], Batch [47/517], Loss: 0.4113
  Epoch [163/200], Batch [94/517], Loss: 0.3925
  Epoch [163/200], Batch [141/517], Loss: 0.0575
  Epoch [163/200], Batch [188/517], Loss: 0.4114
  Epoch [163/200], Batch [235/517], Loss: 0.3830
  Epoch [163/200], Batch [282/517], Loss: 0.0574
  Epoch [163/200], Batch [329/517], Loss: 0.4141
  Epoch [163/200], Batch [376/517], Loss: 0.4060
  Epoch [163/200], Batch [423/517], Loss: 0.3973
  Epoch [163/200], Batch [470/517], Loss: 0.4741
  Epoch [163/200], Batch [517/517], Loss: 0.0706
--- Epoch [163/200] complete. Average Training Loss: 0.3591 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [164/200], Batch [47/517], Loss: 0.4083
  Epoch [164/200], Batch [94/517], Loss: 0.3925
  Epoch [164/200], Batch [141/517], Loss: 0.4139
  Epoch [164/200], Batch [188/517], Loss: 0.3722
  Epoch [164/200], Batch [235/517], Loss: 0.0557
  Epoch [164/200], Batch [282/517], Loss: 0.4086
  Epoch [164/200], Batch [329/517], Loss: 0.3806
  Epoch [164/200], Batch [376/517], Loss: 0.4056
  Epoch [164/200], Batch [423/517], Loss: 0.3864
  Epoch [164/200], Batch [470/517], Loss: 0.3962
  Epoch [164/200], Batch [517/517], Loss: 0.3889
--- Epoch [164/200] complete. Average Training Loss: 0.3537 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [165/200], Batch [47/517], Loss: 0.0447
  Epoch [165/200], Batch [94/517], Loss: 0.3945
  Epoch [165/200], Batch [141/517], Loss: 0.4060
  Epoch [165/200], Batch [188/517], Loss: 0.2261
  Epoch [165/200], Batch [235/517], Loss: 0.4010
  Epoch [165/200], Batch [282/517], Loss: 0.4068
  Epoch [165/200], Batch [329/517], Loss: 0.1098
  Epoch [165/200], Batch [376/517], Loss: 0.3037
  Epoch [165/200], Batch [423/517], Loss: 0.1230
  Epoch [165/200], Batch [470/517], Loss: 0.2359
  Epoch [165/200], Batch [517/517], Loss: 0.1720
--- Epoch [165/200] complete. Average Training Loss: 0.2767 ---
--- Time taken for epoch: 313.88 seconds ---
  Epoch [166/200], Batch [47/517], Loss: 0.1488
  Epoch [166/200], Batch [94/517], Loss: 0.0812
  Epoch [166/200], Batch [141/517], Loss: 0.3891
  Epoch [166/200], Batch [188/517], Loss: 0.2458
  Epoch [166/200], Batch [235/517], Loss: 0.2450
  Epoch [166/200], Batch [282/517], Loss: 0.3334
  Epoch [166/200], Batch [329/517], Loss: 0.2611
  Epoch [166/200], Batch [376/517], Loss: 0.4142
  Epoch [166/200], Batch [423/517], Loss: 0.2523
  Epoch [166/200], Batch [470/517], Loss: 0.4005
  Epoch [166/200], Batch [517/517], Loss: 0.3865
--- Epoch [166/200] complete. Average Training Loss: 0.2520 ---
--- Time taken for epoch: 313.75 seconds ---
  Epoch [167/200], Batch [47/517], Loss: 0.4309
  Epoch [167/200], Batch [94/517], Loss: 0.1838
  Epoch [167/200], Batch [141/517], Loss: 0.1366
  Epoch [167/200], Batch [188/517], Loss: 0.2552
  Epoch [167/200], Batch [235/517], Loss: 0.3321
  Epoch [167/200], Batch [282/517], Loss: 0.1689
  Epoch [167/200], Batch [329/517], Loss: 0.1430
  Epoch [167/200], Batch [376/517], Loss: 0.2618
  Epoch [167/200], Batch [423/517], Loss: 0.4132
  Epoch [167/200], Batch [470/517], Loss: 0.2929
  Epoch [167/200], Batch [517/517], Loss: 0.4030
--- Epoch [167/200] complete. Average Training Loss: 0.2554 ---
--- Time taken for epoch: 313.75 seconds ---
  Epoch [168/200], Batch [47/517], Loss: 0.1997
  Epoch [168/200], Batch [94/517], Loss: 0.1610
  Epoch [168/200], Batch [141/517], Loss: 0.2172
  Epoch [168/200], Batch [188/517], Loss: 0.4151
  Epoch [168/200], Batch [235/517], Loss: 0.4618
  Epoch [168/200], Batch [282/517], Loss: 0.2141
  Epoch [168/200], Batch [329/517], Loss: 0.0953
  Epoch [168/200], Batch [376/517], Loss: 0.2249
  Epoch [168/200], Batch [423/517], Loss: 0.3305
  Epoch [168/200], Batch [470/517], Loss: 0.0839
  Epoch [168/200], Batch [517/517], Loss: 0.2402
--- Epoch [168/200] complete. Average Training Loss: 0.2345 ---
--- Time taken for epoch: 313.87 seconds ---
  Epoch [169/200], Batch [47/517], Loss: 0.1329
  Epoch [169/200], Batch [94/517], Loss: 0.3876
  Epoch [169/200], Batch [141/517], Loss: 0.2048
  Epoch [169/200], Batch [188/517], Loss: 0.2730
  Epoch [169/200], Batch [235/517], Loss: 0.1821
  Epoch [169/200], Batch [282/517], Loss: 0.1781
  Epoch [169/200], Batch [329/517], Loss: 0.2182
  Epoch [169/200], Batch [376/517], Loss: 0.2401
  Epoch [169/200], Batch [423/517], Loss: 0.2666
  Epoch [169/200], Batch [470/517], Loss: 0.2354
  Epoch [169/200], Batch [517/517], Loss: 0.1634
--- Epoch [169/200] complete. Average Training Loss: 0.2523 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [170/200], Batch [47/517], Loss: 0.1464
  Epoch [170/200], Batch [94/517], Loss: 0.1106
  Epoch [170/200], Batch [141/517], Loss: 0.2324
  Epoch [170/200], Batch [188/517], Loss: 0.4360
  Epoch [170/200], Batch [235/517], Loss: 0.1483
  Epoch [170/200], Batch [282/517], Loss: 0.1608
  Epoch [170/200], Batch [329/517], Loss: 0.1781
  Epoch [170/200], Batch [376/517], Loss: 0.4166
  Epoch [170/200], Batch [423/517], Loss: 0.2532
  Epoch [170/200], Batch [470/517], Loss: 0.3143
  Epoch [170/200], Batch [517/517], Loss: 0.1937
--- Epoch [170/200] complete. Average Training Loss: 0.2538 ---
--- Time taken for epoch: 313.84 seconds ---
  Epoch [171/200], Batch [47/517], Loss: 0.4068
  Epoch [171/200], Batch [94/517], Loss: 0.3094
  Epoch [171/200], Batch [141/517], Loss: 0.2246
  Epoch [171/200], Batch [188/517], Loss: 0.1529
  Epoch [171/200], Batch [235/517], Loss: 0.3428
  Epoch [171/200], Batch [282/517], Loss: 0.2378
  Epoch [171/200], Batch [329/517], Loss: 0.2485
  Epoch [171/200], Batch [376/517], Loss: 0.1592
  Epoch [171/200], Batch [423/517], Loss: 0.2188
  Epoch [171/200], Batch [470/517], Loss: 0.0512
  Epoch [171/200], Batch [517/517], Loss: 0.3667
--- Epoch [171/200] complete. Average Training Loss: 0.2695 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [172/200], Batch [47/517], Loss: 0.3069
  Epoch [172/200], Batch [94/517], Loss: 0.3946
  Epoch [172/200], Batch [141/517], Loss: 0.1509
  Epoch [172/200], Batch [188/517], Loss: 0.2571
  Epoch [172/200], Batch [235/517], Loss: 0.4289
  Epoch [172/200], Batch [282/517], Loss: 0.1965
  Epoch [172/200], Batch [329/517], Loss: 0.2466
  Epoch [172/200], Batch [376/517], Loss: 0.1705
  Epoch [172/200], Batch [423/517], Loss: 0.1878
  Epoch [172/200], Batch [470/517], Loss: 0.4027
  Epoch [172/200], Batch [517/517], Loss: 0.3907
--- Epoch [172/200] complete. Average Training Loss: 0.2658 ---
--- Time taken for epoch: 313.86 seconds ---
  Epoch [173/200], Batch [47/517], Loss: 0.3603
  Epoch [173/200], Batch [94/517], Loss: 0.4052
  Epoch [173/200], Batch [141/517], Loss: 0.1485
  Epoch [173/200], Batch [188/517], Loss: 0.4360
  Epoch [173/200], Batch [235/517], Loss: 0.4429
  Epoch [173/200], Batch [282/517], Loss: 0.4313
  Epoch [173/200], Batch [329/517], Loss: 0.2734
  Epoch [173/200], Batch [376/517], Loss: 0.2608
  Epoch [173/200], Batch [423/517], Loss: 0.2871
  Epoch [173/200], Batch [470/517], Loss: 0.1916
  Epoch [173/200], Batch [517/517], Loss: 0.3828
--- Epoch [173/200] complete. Average Training Loss: 0.2770 ---
--- Time taken for epoch: 313.83 seconds ---
  Epoch [174/200], Batch [47/517], Loss: 0.2015
  Epoch [174/200], Batch [94/517], Loss: 0.1771
  Epoch [174/200], Batch [141/517], Loss: 0.2877
  Epoch [174/200], Batch [188/517], Loss: 0.2233
  Epoch [174/200], Batch [235/517], Loss: 0.3859
  Epoch [174/200], Batch [282/517], Loss: 0.3863
  Epoch [174/200], Batch [329/517], Loss: 0.1779
  Epoch [174/200], Batch [376/517], Loss: 0.4630
  Epoch [174/200], Batch [423/517], Loss: 0.0773
  Epoch [174/200], Batch [470/517], Loss: 0.3935
  Epoch [174/200], Batch [517/517], Loss: 0.0655
--- Epoch [174/200] complete. Average Training Loss: 0.3023 ---
--- Time taken for epoch: 313.92 seconds ---
  Epoch [175/200], Batch [47/517], Loss: 0.4195
  Epoch [175/200], Batch [94/517], Loss: 0.4209
  Epoch [175/200], Batch [141/517], Loss: 0.4267
  Epoch [175/200], Batch [188/517], Loss: 0.0705
  Epoch [175/200], Batch [235/517], Loss: 0.3954
  Epoch [175/200], Batch [282/517], Loss: 0.4282
  Epoch [175/200], Batch [329/517], Loss: 0.0925
  Epoch [175/200], Batch [376/517], Loss: 0.4007
  Epoch [175/200], Batch [423/517], Loss: 0.4379
  Epoch [175/200], Batch [470/517], Loss: 0.3949
  Epoch [175/200], Batch [517/517], Loss: 0.4103
--- Epoch [175/200] complete. Average Training Loss: 0.3797 ---
--- Time taken for epoch: 313.82 seconds ---
 -- Updated Checkpoint: Saved model at 175 epochs.
  Epoch [176/200], Batch [47/517], Loss: 0.4031
  Epoch [176/200], Batch [94/517], Loss: 0.4127
  Epoch [176/200], Batch [141/517], Loss: 0.4252
  Epoch [176/200], Batch [188/517], Loss: 0.4094
  Epoch [176/200], Batch [235/517], Loss: 0.3883
  Epoch [176/200], Batch [282/517], Loss: 0.4517
  Epoch [176/200], Batch [329/517], Loss: 0.3997
  Epoch [176/200], Batch [376/517], Loss: 0.4396
  Epoch [176/200], Batch [423/517], Loss: 0.4171
  Epoch [176/200], Batch [470/517], Loss: 0.4235
  Epoch [176/200], Batch [517/517], Loss: 0.3841
--- Epoch [176/200] complete. Average Training Loss: 0.3757 ---
--- Time taken for epoch: 313.79 seconds ---
  Epoch [177/200], Batch [47/517], Loss: 0.4171
  Epoch [177/200], Batch [94/517], Loss: 0.3942
  Epoch [177/200], Batch [141/517], Loss: 0.4200
  Epoch [177/200], Batch [188/517], Loss: 0.4366
  Epoch [177/200], Batch [235/517], Loss: 0.3979
  Epoch [177/200], Batch [282/517], Loss: 0.4326
  Epoch [177/200], Batch [329/517], Loss: 0.0634
  Epoch [177/200], Batch [376/517], Loss: 0.0617
  Epoch [177/200], Batch [423/517], Loss: 0.0477
  Epoch [177/200], Batch [470/517], Loss: 0.4010
  Epoch [177/200], Batch [517/517], Loss: 0.3917
--- Epoch [177/200] complete. Average Training Loss: 0.3715 ---
--- Time taken for epoch: 313.70 seconds ---
  Epoch [178/200], Batch [47/517], Loss: 0.4140
  Epoch [178/200], Batch [94/517], Loss: 0.4151
  Epoch [178/200], Batch [141/517], Loss: 0.4031
  Epoch [178/200], Batch [188/517], Loss: 0.4105
  Epoch [178/200], Batch [235/517], Loss: 0.4177
  Epoch [178/200], Batch [282/517], Loss: 0.4360
  Epoch [178/200], Batch [329/517], Loss: 0.3917
  Epoch [178/200], Batch [376/517], Loss: 0.0679
  Epoch [178/200], Batch [423/517], Loss: 0.4425
  Epoch [178/200], Batch [470/517], Loss: 0.4439
  Epoch [178/200], Batch [517/517], Loss: 0.4122
--- Epoch [178/200] complete. Average Training Loss: 0.3673 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [179/200], Batch [47/517], Loss: 0.3978
  Epoch [179/200], Batch [94/517], Loss: 0.0771
  Epoch [179/200], Batch [141/517], Loss: 0.4032
  Epoch [179/200], Batch [188/517], Loss: 0.4129
  Epoch [179/200], Batch [235/517], Loss: 0.3943
  Epoch [179/200], Batch [282/517], Loss: 0.4098
  Epoch [179/200], Batch [329/517], Loss: 0.3819
  Epoch [179/200], Batch [376/517], Loss: 0.3806
  Epoch [179/200], Batch [423/517], Loss: 0.3984
  Epoch [179/200], Batch [470/517], Loss: 0.3903
  Epoch [179/200], Batch [517/517], Loss: 0.3803
--- Epoch [179/200] complete. Average Training Loss: 0.3626 ---
--- Time taken for epoch: 313.80 seconds ---
  Epoch [180/200], Batch [47/517], Loss: 0.0703
  Epoch [180/200], Batch [94/517], Loss: 0.3893
  Epoch [180/200], Batch [141/517], Loss: 0.4123
  Epoch [180/200], Batch [188/517], Loss: 0.4307
  Epoch [180/200], Batch [235/517], Loss: 0.0991
  Epoch [180/200], Batch [282/517], Loss: 0.3947
  Epoch [180/200], Batch [329/517], Loss: 0.3927
  Epoch [180/200], Batch [376/517], Loss: 0.4041
  Epoch [180/200], Batch [423/517], Loss: 0.4009
  Epoch [180/200], Batch [470/517], Loss: 0.4248
  Epoch [180/200], Batch [517/517], Loss: 0.3775
--- Epoch [180/200] complete. Average Training Loss: 0.3720 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [181/200], Batch [47/517], Loss: 0.3898
  Epoch [181/200], Batch [94/517], Loss: 0.4040
  Epoch [181/200], Batch [141/517], Loss: 0.4212
  Epoch [181/200], Batch [188/517], Loss: 0.3943
  Epoch [181/200], Batch [235/517], Loss: 0.4274
  Epoch [181/200], Batch [282/517], Loss: 0.3965
  Epoch [181/200], Batch [329/517], Loss: 0.4228
  Epoch [181/200], Batch [376/517], Loss: 0.4015
  Epoch [181/200], Batch [423/517], Loss: 0.0574
  Epoch [181/200], Batch [470/517], Loss: 0.0589
  Epoch [181/200], Batch [517/517], Loss: 0.4063
--- Epoch [181/200] complete. Average Training Loss: 0.3597 ---
--- Time taken for epoch: 313.82 seconds ---
  Epoch [182/200], Batch [47/517], Loss: 0.4268
  Epoch [182/200], Batch [94/517], Loss: 0.4227
  Epoch [182/200], Batch [141/517], Loss: 0.3973
  Epoch [182/200], Batch [188/517], Loss: 0.1151
  Epoch [182/200], Batch [235/517], Loss: 0.4288
  Epoch [182/200], Batch [282/517], Loss: 0.4277
  Epoch [182/200], Batch [329/517], Loss: 0.0566
  Epoch [182/200], Batch [376/517], Loss: 0.4131
  Epoch [182/200], Batch [423/517], Loss: 0.4175
  Epoch [182/200], Batch [470/517], Loss: 0.4045
  Epoch [182/200], Batch [517/517], Loss: 0.3771
--- Epoch [182/200] complete. Average Training Loss: 0.3708 ---
--- Time taken for epoch: 313.89 seconds ---
  Epoch [183/200], Batch [47/517], Loss: 0.3797
  Epoch [183/200], Batch [94/517], Loss: 0.4147
  Epoch [183/200], Batch [141/517], Loss: 0.4038
  Epoch [183/200], Batch [188/517], Loss: 0.3983
  Epoch [183/200], Batch [235/517], Loss: 0.3864
  Epoch [183/200], Batch [282/517], Loss: 0.3865
  Epoch [183/200], Batch [329/517], Loss: 0.3937
  Epoch [183/200], Batch [376/517], Loss: 0.0679
  Epoch [183/200], Batch [423/517], Loss: 0.4307
  Epoch [183/200], Batch [470/517], Loss: 0.4122
  Epoch [183/200], Batch [517/517], Loss: 0.3819
--- Epoch [183/200] complete. Average Training Loss: 0.3657 ---
--- Time taken for epoch: 313.81 seconds ---
  Epoch [184/200], Batch [47/517], Loss: 0.3985
  Epoch [184/200], Batch [94/517], Loss: 0.3900
  Epoch [184/200], Batch [141/517], Loss: 0.3984
  Epoch [184/200], Batch [188/517], Loss: 0.0772
  Epoch [184/200], Batch [235/517], Loss: 0.4022
  Epoch [184/200], Batch [282/517], Loss: 0.3935
  Epoch [184/200], Batch [329/517], Loss: 0.3845
  Epoch [184/200], Batch [376/517], Loss: 0.0752
  Epoch [184/200], Batch [423/517], Loss: 0.4232
  Epoch [184/200], Batch [470/517], Loss: 0.4171
  Epoch [184/200], Batch [517/517], Loss: 0.0481
--- Epoch [184/200] complete. Average Training Loss: 0.3651 ---
--- Time taken for epoch: 313.78 seconds ---
  Epoch [185/200], Batch [47/517], Loss: 0.4058
  Epoch [185/200], Batch [94/517], Loss: 0.3840
  Epoch [185/200], Batch [141/517], Loss: 0.3995
  Epoch [185/200], Batch [188/517], Loss: 0.4185
  Epoch [185/200], Batch [235/517], Loss: 0.0474
  Epoch [185/200], Batch [282/517], Loss: 0.4306
  Epoch [185/200], Batch [329/517], Loss: 0.0863
  Epoch [185/200], Batch [376/517], Loss: 0.4348
  Epoch [185/200], Batch [423/517], Loss: 0.3987
  Epoch [185/200], Batch [470/517], Loss: 0.0618
  Epoch [185/200], Batch [517/517], Loss: 0.4245
--- Epoch [185/200] complete. Average Training Loss: 0.3609 ---
--- Time taken for epoch: 314.10 seconds ---
  Epoch [186/200], Batch [47/517], Loss: 0.3859
  Epoch [186/200], Batch [94/517], Loss: 0.4206
  Epoch [186/200], Batch [141/517], Loss: 0.3805
  Epoch [186/200], Batch [188/517], Loss: 0.4386
  Epoch [186/200], Batch [235/517], Loss: 0.3986
  Epoch [186/200], Batch [282/517], Loss: 0.4066
  Epoch [186/200], Batch [329/517], Loss: 0.4199
  Epoch [186/200], Batch [376/517], Loss: 0.4049
  Epoch [186/200], Batch [423/517], Loss: 0.0695
  Epoch [186/200], Batch [470/517], Loss: 0.4773
  Epoch [186/200], Batch [517/517], Loss: 0.0695
--- Epoch [186/200] complete. Average Training Loss: 0.3595 ---
--- Time taken for epoch: 314.30 seconds ---
  Epoch [187/200], Batch [47/517], Loss: 0.3924
  Epoch [187/200], Batch [94/517], Loss: 0.3874
  Epoch [187/200], Batch [141/517], Loss: 0.3750
  Epoch [187/200], Batch [188/517], Loss: 0.4010
  Epoch [187/200], Batch [235/517], Loss: 0.4230
  Epoch [187/200], Batch [282/517], Loss: 0.3201
  Epoch [187/200], Batch [329/517], Loss: 0.4016
  Epoch [187/200], Batch [376/517], Loss: 0.1698
  Epoch [187/200], Batch [423/517], Loss: 0.1597
  Epoch [187/200], Batch [470/517], Loss: 0.1752
  Epoch [187/200], Batch [517/517], Loss: 0.2567
--- Epoch [187/200] complete. Average Training Loss: 0.2961 ---
--- Time taken for epoch: 314.26 seconds ---
  Epoch [188/200], Batch [47/517], Loss: 0.2522
  Epoch [188/200], Batch [94/517], Loss: 0.2320
  Epoch [188/200], Batch [141/517], Loss: 0.2079
  Epoch [188/200], Batch [188/517], Loss: 0.2248
  Epoch [188/200], Batch [235/517], Loss: 0.2058
  Epoch [188/200], Batch [282/517], Loss: 0.4061
  Epoch [188/200], Batch [329/517], Loss: 0.3008
  Epoch [188/200], Batch [376/517], Loss: 0.1928
  Epoch [188/200], Batch [423/517], Loss: 0.3920
  Epoch [188/200], Batch [470/517], Loss: 0.2197
  Epoch [188/200], Batch [517/517], Loss: 0.4095
--- Epoch [188/200] complete. Average Training Loss: 0.2781 ---
--- Time taken for epoch: 314.31 seconds ---
  Epoch [189/200], Batch [47/517], Loss: 0.4313
  Epoch [189/200], Batch [94/517], Loss: 0.2385
  Epoch [189/200], Batch [141/517], Loss: 0.3052
  Epoch [189/200], Batch [188/517], Loss: 0.2165
  Epoch [189/200], Batch [235/517], Loss: 0.2350
  Epoch [189/200], Batch [282/517], Loss: 0.2151
  Epoch [189/200], Batch [329/517], Loss: 0.2910
  Epoch [189/200], Batch [376/517], Loss: 0.4077
  Epoch [189/200], Batch [423/517], Loss: 0.3838
  Epoch [189/200], Batch [470/517], Loss: 0.1209
  Epoch [189/200], Batch [517/517], Loss: 0.2249
--- Epoch [189/200] complete. Average Training Loss: 0.2493 ---
--- Time taken for epoch: 314.51 seconds ---
  Epoch [190/200], Batch [47/517], Loss: 0.2265
  Epoch [190/200], Batch [94/517], Loss: 0.2006
  Epoch [190/200], Batch [141/517], Loss: 0.2762
  Epoch [190/200], Batch [188/517], Loss: 0.2165
  Epoch [190/200], Batch [235/517], Loss: 0.2595
  Epoch [190/200], Batch [282/517], Loss: 0.2027
  Epoch [190/200], Batch [329/517], Loss: 0.2028
  Epoch [190/200], Batch [376/517], Loss: 0.3127
  Epoch [190/200], Batch [423/517], Loss: 0.1088
  Epoch [190/200], Batch [470/517], Loss: 0.1565
  Epoch [190/200], Batch [517/517], Loss: 0.4042
--- Epoch [190/200] complete. Average Training Loss: 0.2425 ---
--- Time taken for epoch: 314.55 seconds ---
  Epoch [191/200], Batch [47/517], Loss: 0.1629
  Epoch [191/200], Batch [94/517], Loss: 0.2018
  Epoch [191/200], Batch [141/517], Loss: 0.3833
  Epoch [191/200], Batch [188/517], Loss: 0.4184
  Epoch [191/200], Batch [235/517], Loss: 0.1924
  Epoch [191/200], Batch [282/517], Loss: 0.1248
  Epoch [191/200], Batch [329/517], Loss: 0.2358
  Epoch [191/200], Batch [376/517], Loss: 0.1996
  Epoch [191/200], Batch [423/517], Loss: 0.3964
  Epoch [191/200], Batch [470/517], Loss: 0.0429
  Epoch [191/200], Batch [517/517], Loss: 0.1678
--- Epoch [191/200] complete. Average Training Loss: 0.2230 ---
--- Time taken for epoch: 314.40 seconds ---
  Epoch [192/200], Batch [47/517], Loss: 0.1221
  Epoch [192/200], Batch [94/517], Loss: 0.2452
  Epoch [192/200], Batch [141/517], Loss: 0.1486
  Epoch [192/200], Batch [188/517], Loss: 0.1462
  Epoch [192/200], Batch [235/517], Loss: 0.3667
  Epoch [192/200], Batch [282/517], Loss: 0.3155
  Epoch [192/200], Batch [329/517], Loss: 0.1618
  Epoch [192/200], Batch [376/517], Loss: 0.2335
  Epoch [192/200], Batch [423/517], Loss: 0.1948
  Epoch [192/200], Batch [470/517], Loss: 0.2417
  Epoch [192/200], Batch [517/517], Loss: 0.1066
--- Epoch [192/200] complete. Average Training Loss: 0.2253 ---
--- Time taken for epoch: 314.57 seconds ---
  Epoch [193/200], Batch [47/517], Loss: 0.4113
  Epoch [193/200], Batch [94/517], Loss: 0.2033
  Epoch [193/200], Batch [141/517], Loss: 0.2725
  Epoch [193/200], Batch [188/517], Loss: 0.0637
  Epoch [193/200], Batch [235/517], Loss: 0.2200
  Epoch [193/200], Batch [282/517], Loss: 0.1671
  Epoch [193/200], Batch [329/517], Loss: 0.2379
  Epoch [193/200], Batch [376/517], Loss: 0.1175
  Epoch [193/200], Batch [423/517], Loss: 0.4306
  Epoch [193/200], Batch [470/517], Loss: 0.1723
  Epoch [193/200], Batch [517/517], Loss: 0.2247
--- Epoch [193/200] complete. Average Training Loss: 0.2561 ---
--- Time taken for epoch: 314.12 seconds ---
  Epoch [194/200], Batch [47/517], Loss: 0.4037
  Epoch [194/200], Batch [94/517], Loss: 0.2195
  Epoch [194/200], Batch [141/517], Loss: 0.1235
  Epoch [194/200], Batch [188/517], Loss: 0.3445
  Epoch [194/200], Batch [235/517], Loss: 0.3237
  Epoch [194/200], Batch [282/517], Loss: 0.3443
  Epoch [194/200], Batch [329/517], Loss: 0.2572
  Epoch [194/200], Batch [376/517], Loss: 0.4038
  Epoch [194/200], Batch [423/517], Loss: 0.2491
  Epoch [194/200], Batch [470/517], Loss: 0.2357
  Epoch [194/200], Batch [517/517], Loss: 0.1245
--- Epoch [194/200] complete. Average Training Loss: 0.2708 ---
--- Time taken for epoch: 314.28 seconds ---
  Epoch [195/200], Batch [47/517], Loss: 0.2459
  Epoch [195/200], Batch [94/517], Loss: 0.4158
  Epoch [195/200], Batch [141/517], Loss: 0.2424
  Epoch [195/200], Batch [188/517], Loss: 0.1796
  Epoch [195/200], Batch [235/517], Loss: 0.3925
  Epoch [195/200], Batch [282/517], Loss: 0.3856
  Epoch [195/200], Batch [329/517], Loss: 0.4154
  Epoch [195/200], Batch [376/517], Loss: 0.2515
  Epoch [195/200], Batch [423/517], Loss: 0.4228
  Epoch [195/200], Batch [470/517], Loss: 0.3860
  Epoch [195/200], Batch [517/517], Loss: 0.1792
--- Epoch [195/200] complete. Average Training Loss: 0.3143 ---
--- Time taken for epoch: 314.25 seconds ---
  Epoch [196/200], Batch [47/517], Loss: 0.2801
  Epoch [196/200], Batch [94/517], Loss: 0.2856
  Epoch [196/200], Batch [141/517], Loss: 0.4110
  Epoch [196/200], Batch [188/517], Loss: 0.1672
  Epoch [196/200], Batch [235/517], Loss: 0.2671
  Epoch [196/200], Batch [282/517], Loss: 0.1715
  Epoch [196/200], Batch [329/517], Loss: 0.1846
  Epoch [196/200], Batch [376/517], Loss: 0.2345
  Epoch [196/200], Batch [423/517], Loss: 0.4075
  Epoch [196/200], Batch [470/517], Loss: 0.3990
  Epoch [196/200], Batch [517/517], Loss: 0.2561
--- Epoch [196/200] complete. Average Training Loss: 0.2949 ---
--- Time taken for epoch: 314.31 seconds ---
  Epoch [197/200], Batch [47/517], Loss: 0.4075
  Epoch [197/200], Batch [94/517], Loss: 0.3807
  Epoch [197/200], Batch [141/517], Loss: 0.3489
  Epoch [197/200], Batch [188/517], Loss: 0.1125
  Epoch [197/200], Batch [235/517], Loss: 0.1470
  Epoch [197/200], Batch [282/517], Loss: 0.3960
  Epoch [197/200], Batch [329/517], Loss: 0.1795
  Epoch [197/200], Batch [376/517], Loss: 0.3931
  Epoch [197/200], Batch [423/517], Loss: 0.1769
  Epoch [197/200], Batch [470/517], Loss: 0.1038
  Epoch [197/200], Batch [517/517], Loss: 0.4013
--- Epoch [197/200] complete. Average Training Loss: 0.2579 ---
--- Time taken for epoch: 314.35 seconds ---
  Epoch [198/200], Batch [47/517], Loss: 0.2423
  Epoch [198/200], Batch [94/517], Loss: 0.1262
  Epoch [198/200], Batch [141/517], Loss: 0.3396
  Epoch [198/200], Batch [188/517], Loss: 0.3542
  Epoch [198/200], Batch [235/517], Loss: 0.3600
  Epoch [198/200], Batch [282/517], Loss: 0.4190
  Epoch [198/200], Batch [329/517], Loss: 0.1810
  Epoch [198/200], Batch [376/517], Loss: 0.4048
  Epoch [198/200], Batch [423/517], Loss: 0.4273
  Epoch [198/200], Batch [470/517], Loss: 0.4041
  Epoch [198/200], Batch [517/517], Loss: 0.2329
--- Epoch [198/200] complete. Average Training Loss: 0.2794 ---
--- Time taken for epoch: 314.32 seconds ---
  Epoch [199/200], Batch [47/517], Loss: 0.1684
  Epoch [199/200], Batch [94/517], Loss: 0.4487
  Epoch [199/200], Batch [141/517], Loss: 0.1673
  Epoch [199/200], Batch [188/517], Loss: 0.1824
  Epoch [199/200], Batch [235/517], Loss: 0.2183
  Epoch [199/200], Batch [282/517], Loss: 0.2227
  Epoch [199/200], Batch [329/517], Loss: 0.1748
  Epoch [199/200], Batch [376/517], Loss: 0.1963
  Epoch [199/200], Batch [423/517], Loss: 0.2926
  Epoch [199/200], Batch [470/517], Loss: 0.1851
  Epoch [199/200], Batch [517/517], Loss: 0.2294
--- Epoch [199/200] complete. Average Training Loss: 0.2547 ---
--- Time taken for epoch: 314.30 seconds ---
  Epoch [200/200], Batch [47/517], Loss: 0.1978
  Epoch [200/200], Batch [94/517], Loss: 0.3585
  Epoch [200/200], Batch [141/517], Loss: 0.3997
  Epoch [200/200], Batch [188/517], Loss: 0.3116
  Epoch [200/200], Batch [235/517], Loss: 0.3231
  Epoch [200/200], Batch [282/517], Loss: 0.4040
  Epoch [200/200], Batch [329/517], Loss: 0.2091
  Epoch [200/200], Batch [376/517], Loss: 0.2786
  Epoch [200/200], Batch [423/517], Loss: 0.3604
  Epoch [200/200], Batch [470/517], Loss: 0.1739
  Epoch [200/200], Batch [517/517], Loss: 0.2187
--- Epoch [200/200] complete. Average Training Loss: 0.3102 ---
--- Time taken for epoch: 314.26 seconds ---
 -- Updated Checkpoint: Saved model at 200 epochs.
UNet training finished.
UNet training process completed.

Starting UNet Testing...
